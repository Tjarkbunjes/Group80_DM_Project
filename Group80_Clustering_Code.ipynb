{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6641c99f",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"img/Group80_Background.png\" alt=\"Group80 Background\" style=\"width:80%;\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9246d",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='0'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>0. Introduction and Review of EDA</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a222ef23",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "Amazing International Airlines Inc. (AIAI) faces the challenge of designing personalized services and marketing strategies for a diverse customer base. In today’s highly competitive airline industry, leveraging data-driven approaches to understand customer segments is crucial for improving satisfaction, increasing retention, and maximizing revenue potential.\n",
    "\n",
    "In this project, we act as consultants for AIAI and analyze loyalty membership data and flight activity collected over a three-year period to develop a data-driven segmentation strategy.\n",
    "\n",
    "We analyze loyalty membership data and flight activity collected over a three-year period (2019-2021) to develop a data-driven customer segmentation strategy through clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316eed50",
   "metadata": {},
   "source": [
    "## **Metadata**\n",
    "\n",
    "### **Customer Database** \n",
    "\n",
    "| Variable | Description |\n",
    "| --- | --- |\n",
    "| **Loyalty#** | Unique customer identifier for loyalty program members |\n",
    "| **Country** | Customer's country of residence |\n",
    "| **Province or State** | Customer's province or state |\n",
    "| **City** | Customer's city of residence |\n",
    "| **Postal code** | Customer's postal/ZIP code |\n",
    "| **Gender** | Customer's gender |\n",
    "| **Education** | Customer's highest education level (e.g., Bachelor, College) |\n",
    "| **Location Code** | Urban/Suburban/Rural classification of customer residence |\n",
    "| **Income** | Customer's annual income |\n",
    "| **Marital Status** | Customer's marital status (Married, Single, Divorced) |\n",
    "| **LoyaltyStatus** | Current tier status in loyalty program (Star > Nova > Aurora) |\n",
    "| **EnrollmentDateOpening** | Date when customer joined the loyalty program |\n",
    "| **CancellationDate** | Date when customer left the program |\n",
    "| **Customer Lifetime Value** | Total calculated monetary value of customer relationship |\n",
    "| **EnrollmentType** | Method of joining loyalty program |\n",
    "\n",
    "### **Flight Activity Database**\n",
    "\n",
    "| Variable | Description |\n",
    "| --- | --- |\n",
    "| **Loyalty#** | Unique customer identifier linking to CustomerDB |\n",
    "| **Year** | Year of flight activity record |\n",
    "| **Month** | Month of flight activity record (1-12) |\n",
    "| **YearMonthDate** | First day of the month for the activity period |\n",
    "| **NumFlights** | Total number of flights taken by customer in the month |\n",
    "| **NumFlightsWithCompanions** | Number of flights where customer traveled with companions |\n",
    "| **DistanceKM** | Total distance traveled in kilometers for the month |\n",
    "| **PointsAccumulated** | Loyalty points earned by customer during the month |\n",
    "| **PointsRedeemed** | Loyalty points spent/redeemed during the month |\n",
    "| **DollarCostPointsRedeemed** | Dollar value of points redeemed during the month |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144768d4",
   "metadata": {},
   "source": [
    "## **Review of EDA (Open to do)**\n",
    "\n",
    "### hier auch sagen dass wir die outlier nochmal tief angehen und wir ne startegy dafür haben\n",
    "\n",
    "Key findings from the Exploratory Data Analysis:\n",
    "\n",
    "- **Missing Values:** Income, CancellationDate, and Customer Lifetime Value have missing values; Income and CLTV are perfectly correlated\n",
    "- **Duplicates:** Found duplicate Loyalty# entries in CustomerDB (will be removed)\n",
    "- **Incoherences:** \n",
    "  - Distance/km with zero flights: Months exist where DistanceKM > 0 despite NumFlights == 0\n",
    "  - Physically impossible leg lengths: Some customers show DistanceKM beyond realistic bounds (~14,000 km per flight)\n",
    "  - Fractional counts in flight metrics for 2019 data\n",
    "  - CLTV-positive customers with zero recorded flights\n",
    "- **Outliers:** Zero-income customers with CLTV > 0 (mostly \"College\" education)\n",
    "- **Feature Engineering:** Created recency features (is_active_6m, is_active_12m), rejoined status, and aggregated flight metrics\n",
    "\n",
    "\n",
    "ergänzen:\n",
    "\n",
    "neue features die wir hinzufügen blablabla"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fcfd06",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a693e32",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='1'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #6BCF5D); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>1. Import Libraries and Load Data</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00096b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# For visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib import cm, colorbar\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.colors as mpl_colors\n",
    "from matplotlib.scale import FuncScale\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from scipy.stats import mstats\n",
    "\n",
    "\n",
    "# For clustering\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, MeanShift, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
    "from minisom import MiniSom\n",
    "from matplotlib.patches import RegularPolygon\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='seaborn')\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "# For better resolution plots\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Setting seaborn style\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c8560a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom color palette for Group 80\n",
    "CUSTOM_HEX = [\n",
    "    \"#00411E\", \"#00622D\", \"#00823C\", \"#45AF28\", \"#6BCF5D\", \"#D5E6D0\",\"#212121\", \"#313131\", \"#595959\", \"#909090\"\n",
    "]\n",
    "\n",
    "# Apply globally (Seaborn + Matplotlib)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette(CUSTOM_HEX)\n",
    "mpl.rcParams[\"axes.prop_cycle\"] = mpl.cycler(color=CUSTOM_HEX)\n",
    "\n",
    "# Continuous colormap (for numeric hues, heatmaps, etc.)\n",
    "GROUP80_palette_continuous = LinearSegmentedColormap.from_list(\n",
    "    \"green_white_gray_black\",\n",
    "    [\"#00411E\", \"#00823C\", \"#82BA72\", \"#D5E6D0\", \"#FFFFFF\", \"#909090\", \"#595959\", \"#313131\", \"#212121\"]\n",
    ")\n",
    "\n",
    "# Colors for specific uses\n",
    "colors = [\"#00411E\", \"#00622D\", \"#00823C\", \"#45AF28\", \"#6BCF5D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9592df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_Customer = pd.read_csv('data/input_data/DM_AIAI_CustomerDB.csv')\n",
    "df_Flights = pd.read_csv('data/input_data/DM_AIAI_FlightsDB.csv')\n",
    "\n",
    "print(df_Customer.head())\n",
    "print(df_Flights.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ee3491",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67508af4",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='2'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;   max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>2. Data Preparation</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca72d7d",
   "metadata": {},
   "source": [
    "## **2.1 Data Type Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date columns in CustomerDB\n",
    "df_Customer['EnrollmentDateOpening'] = pd.to_datetime(\n",
    "    df_Customer['EnrollmentDateOpening'],\n",
    "    format='%m/%d/%Y',\n",
    "    errors='coerce'\n",
    ")\n",
    "df_Customer['CancellationDate'] = pd.to_datetime(\n",
    "    df_Customer['CancellationDate'],\n",
    "    format='%m/%d/%Y',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Convert suitable object columns into pandas Categorical dtype\n",
    "categorical_cols = [\n",
    "    'Loyalty#', 'Country', 'Province or State', 'City', 'Postal code',\n",
    "    'Gender', 'Education', 'Location Code', 'Marital Status',\n",
    "    'LoyaltyStatus', 'EnrollmentType'\n",
    "]\n",
    "df_Customer[categorical_cols] = df_Customer[categorical_cols].apply(lambda s: s.astype('category'))\n",
    "\n",
    "# Convert date column in FlightsDB\n",
    "df_Flights[\"YearMonthDate\"] = (\n",
    "    pd.to_datetime(df_Flights[\"YearMonthDate\"], format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "      .dt.to_period(\"M\")\n",
    "      .astype(str)\n",
    ")\n",
    "df_Flights['Loyalty#'] = df_Flights['Loyalty#'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb7d067",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Summary of Actions Taken</h3>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Date conversions:</strong> Converted EnrollmentDateOpening, CancellationDate, and YearMonthDate to datetime format</li>\n",
    "        <li><strong>Categorical conversions:</strong> Converted 11 columns (Loyalty#, demographic variables, LoyaltyStatus, EnrollmentType) to categorical dtype for memory efficiency</li>\n",
    "        <li><strong>Period format:</strong> Converted YearMonthDate to monthly period string format for time-series analysis</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b995ad",
   "metadata": {},
   "source": [
    "## **2.2 Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate Loyalty# in CustomerDB\n",
    "duplicate_loyalty_count = df_Customer['Loyalty#'].duplicated().sum()\n",
    "print(f'Duplicate Loyalty# in CustomerDB: {duplicate_loyalty_count}')\n",
    "\n",
    "# Identify and remove duplicate Loyalty# from both datasets\n",
    "duplicate_loyalty_numbers = df_Customer[df_Customer.duplicated(subset='Loyalty#', keep=False)]['Loyalty#'].unique()\n",
    "print(f'Number of Loyalty# with duplicates: {len(duplicate_loyalty_numbers)}')\n",
    "\n",
    "# Remove duplicates from both datasets\n",
    "df_Customer = df_Customer[~df_Customer['Loyalty#'].isin(duplicate_loyalty_numbers)]\n",
    "df_Flights = df_Flights[~df_Flights['Loyalty#'].isin(duplicate_loyalty_numbers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f832881d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Summary of Actions Taken</h3>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Removed all duplicate Loyalty# entries</strong> from both CustomerDB and FlightsDB (because we cannot determine which duplicate is correct)</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac3d3c",
   "metadata": {},
   "source": [
    "## **2.3 Incoherences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788158b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDs in FlightsDB but not in CustomerDB\n",
    "missing_flights = df_Flights.loc[~df_Flights['Loyalty#'].isin(df_Customer['Loyalty#']), 'Loyalty#']\n",
    "\n",
    "# IDs in CustomerDB but not in FlightsDB\n",
    "missing_customer = df_Customer.loc[~df_Customer['Loyalty#'].isin(df_Flights['Loyalty#']), 'Loyalty#']\n",
    "\n",
    "print(f\"FlightsDB IDs missing in CustomerDB: {missing_flights.nunique()}\")\n",
    "print(f\"CustomerDB IDs missing in FlightsDB: {missing_customer.nunique()}\")\n",
    "\n",
    "# Remove those rows\n",
    "df_Flights = df_Flights[~df_Flights['Loyalty#'].isin(missing_flights)]\n",
    "df_Customer = df_Customer[~df_Customer['Loyalty#'].isin(missing_customer)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37947569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: Set DistanceKM, PointsAccumulated and NumFlightsWithCompanions to 0 where NumFlights == 0\n",
    "incoherent_rows = (df_Flights['NumFlights'] == 0) & (\n",
    "    (df_Flights['DistanceKM'] > 0) | \n",
    "    (df_Flights['PointsAccumulated'] > 0) | \n",
    "    (df_Flights['NumFlightsWithCompanions'] > 0)\n",
    ")\n",
    "print(f'Rows with NumFlights=0 but other metrics>0: {incoherent_rows.sum()}')\n",
    "\n",
    "# Set those values to 0 where NumFlights == 0 but other metrics > 0\n",
    "df_Flights.loc[df_Flights['NumFlights'] == 0, ['DistanceKM', 'PointsAccumulated', 'NumFlightsWithCompanions']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598810b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: Check for illogical flight distances (> 14,000 km per flight)\n",
    "max_flight_distance = 14000\n",
    "avg_distance = df_Flights['DistanceKM'] / df_Flights['NumFlights'].replace(0, np.nan)\n",
    "illogical_mask = avg_distance > max_flight_distance\n",
    "\n",
    "print(f'Rows with illogical distances (>{max_flight_distance}km per flight): {illogical_mask.sum()}')\n",
    "\n",
    "# Set to 0 for illogical distances\n",
    "df_Flights.loc[illogical_mask, ['NumFlights', 'NumFlightsWithCompanions', 'DistanceKM', 'PointsAccumulated']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c30cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers with no flights in 2019-2021\n",
    "# Customers without any flight activity cannot be meaningfully segmented\n",
    "# They have zero values for all flight-related metrics (distance, companions, points, etc.)\n",
    "# This filtering ensures we only cluster customers with actual travel behavior to analyze\n",
    "print(f\"Customers before removing non-flyers: {len(df_Customer):,}\")\n",
    "\n",
    "# Sum total flights per customer across all 36 months\n",
    "total_flights_per_customer = df_Flights.groupby('Loyalty#')['NumFlights'].sum()\n",
    "customers_with_flights = total_flights_per_customer[total_flights_per_customer > 0].index\n",
    "\n",
    "df_Customer = df_Customer[df_Customer['Loyalty#'].isin(customers_with_flights)].copy()\n",
    "df_Flights = df_Flights[df_Flights['Loyalty#'].isin(customers_with_flights)].copy()\n",
    "\n",
    "print(f\"Customers after removing non-flyers: {len(df_Customer):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68240d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers with inconsistent enrollment data: enrolled before 2021 but with '2021 Promotion' type\n",
    "print(f\"Customers before removing inconsistent enrollments: {len(df_Customer):,}\")\n",
    "\n",
    "inconsistent_enrollment = (\n",
    "    (df_Customer['EnrollmentDateOpening'] < pd.to_datetime('2021-01-01')) & \n",
    "    (df_Customer['EnrollmentType'] == '2021 Promotion')\n",
    ")\n",
    "inconsistent_ids = df_Customer[inconsistent_enrollment]['Loyalty#']\n",
    "\n",
    "df_Customer = df_Customer[~inconsistent_enrollment].copy()\n",
    "df_Flights = df_Flights[~df_Flights['Loyalty#'].isin(inconsistent_ids)].copy()\n",
    "\n",
    "print(f\"Customers after removing inconsistent enrollments: {len(df_Customer):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a283b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers with impossible redemption_rate: enrolled after 2019 but rate > 1\n",
    "# These cannot have accumulated points before our data window, so rate > 1 indicates data errors\n",
    "print(f\"Customers before removing impossible redemption rates: {len(df_Customer):,}\")\n",
    "\n",
    "# Temporarily calculate redemption_rate\n",
    "points_temp = df_Flights.groupby('Loyalty#').agg({\n",
    "    'PointsAccumulated': 'sum',\n",
    "    'PointsRedeemed': 'sum'\n",
    "})\n",
    "points_temp['redemption_rate_temp'] = np.where(\n",
    "    points_temp['PointsAccumulated'] > 0,\n",
    "    points_temp['PointsRedeemed'] / points_temp['PointsAccumulated'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Temporary merge\n",
    "df_Customer = df_Customer.merge(points_temp[['redemption_rate_temp']], on='Loyalty#', how='left')\n",
    "df_Customer['redemption_rate_temp'] = df_Customer['redemption_rate_temp'].fillna(0)\n",
    "\n",
    "impossible_redemption = (\n",
    "    (df_Customer['redemption_rate_temp'] > 1) & \n",
    "    (df_Customer['EnrollmentDateOpening'] > pd.to_datetime('2019-01-01'))\n",
    ")\n",
    "impossible_ids = df_Customer[impossible_redemption]['Loyalty#']\n",
    "\n",
    "df_Customer = df_Customer[~impossible_redemption].copy()\n",
    "df_Flights = df_Flights[~df_Flights['Loyalty#'].isin(impossible_ids)].copy()\n",
    "\n",
    "# Drop temporary column\n",
    "df_Customer = df_Customer.drop(columns=['redemption_rate_temp'])\n",
    "\n",
    "print(f\"Customers after removing impossible redemption rates: {len(df_Customer):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dfbeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Flights[df_Flights['Loyalty#'] == 748810]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf50cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers with CancellationDate before EnrollmentDateOpening (data inconsistency)\n",
    "print(f\"Customers before removing invalid cancellation dates: {len(df_Customer):,}\")\n",
    "\n",
    "invalid_cancellation = (\n",
    "    df_Customer['CancellationDate'].notna() & \n",
    "    df_Customer['EnrollmentDateOpening'].notna() &\n",
    "    (df_Customer['CancellationDate'] < df_Customer['EnrollmentDateOpening'])\n",
    ")\n",
    "invalid_ids = df_Customer[invalid_cancellation]['Loyalty#']\n",
    "\n",
    "df_Customer = df_Customer[~invalid_cancellation].copy()\n",
    "df_Flights = df_Flights[~df_Flights['Loyalty#'].isin(invalid_ids)].copy()\n",
    "\n",
    "print(f\"Customers after removing invalid cancellation dates: {len(df_Customer):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7740f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show customers who redeemed points before their enrollment date\n",
    "first_redemption = df_Flights[df_Flights['PointsRedeemed'] > 0].groupby('Loyalty#')['YearMonthDate'].min().rename('first_redemption_date')\n",
    "\n",
    "redemption_check = df_Customer[['Loyalty#', 'EnrollmentDateOpening', 'EnrollmentType']].merge(first_redemption, on='Loyalty#', how='inner')\n",
    "\n",
    "invalid_redemptions = redemption_check[\n",
    "    (redemption_check['first_redemption_date'] < redemption_check['EnrollmentDateOpening'])]\n",
    "\n",
    "print(f\"Customers with redemption before enrollment: {len(invalid_redemptions)}\")\n",
    "display(invalid_redemptions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706dc4f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fff9e6ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #8B8000, #B8A000, #D4C000, #E8D800, #F0E68C) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #8B8000; font-weight: bold;\">Key Insight for Strategy: Loyalty Program Design Flaw</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Critical Finding:</strong> 1,967 customers were able to accumulate and redeem points <strong>before their official enrollment date</strong>. This represents a significant program design issue.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #8B8000; font-weight: bold;\">Why This Matters:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li><strong>Reduces enrollment incentive:</strong> If customers can earn and use points without formally joining the loyalty program, there is no motivation to enroll. The enrollment barrier becomes meaningless.</li>\n",
    "        <li><strong>Lost engagement opportunity:</strong> Non-enrolled customers cannot be targeted with personalized communications, tier-based promotions, or retention campaigns.</li>\n",
    "        <li><strong>Data quality impact:</strong> Creates inconsistent customer records where flight activity precedes membership, complicating customer journey analysis.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #8B8000; font-weight: bold;\">Our Approach:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Data retained:</strong> We keep these 1,967 customers in the dataset. Removing them would significantly reduce our sample size and these customers still exhibit valid flight behavior. We assume the system allowed this and treat their data as valid for clustering purposes.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #8B8000; font-weight: bold;\">Recommended Action:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Fix the system:</strong> Points accumulation and redemption should be strictly gated behind enrollment. Customers flying without enrollment should see \"You could have earned X points - enroll now!\" messaging to drive program signup. This converts passive flyers into engaged loyalty members.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7df881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers who redeemed points before accumulating any points\n",
    "# Exception: customers enrolled before 2019 (they could have accumulated points before our data window)\n",
    "print(f\"Customers before removing invalid redemption sequence: {len(df_Customer):,}\")\n",
    "\n",
    "# Find first redemption date and first accumulation date per customer\n",
    "first_redemption = df_Flights[df_Flights['PointsRedeemed'] > 0].groupby('Loyalty#')['YearMonthDate'].min().rename('first_redemption_date')\n",
    "first_accumulation = df_Flights[df_Flights['PointsAccumulated'] > 0].groupby('Loyalty#')['YearMonthDate'].min().rename('first_accumulation_date')\n",
    "\n",
    "# Merge with enrollment date\n",
    "sequence_check = df_Customer[['Loyalty#', 'EnrollmentDateOpening']].merge(first_redemption, on='Loyalty#', how='inner')\n",
    "sequence_check = sequence_check.merge(first_accumulation, on='Loyalty#', how='left')\n",
    "\n",
    "# Find customers who redeemed before accumulating and enrolled after 2019\n",
    "# (enrolled before 2019 = could have accumulated points before our data window)\n",
    "invalid_sequence_ids = sequence_check[\n",
    "    (sequence_check['first_redemption_date'] < sequence_check['first_accumulation_date']) &\n",
    "    (sequence_check['EnrollmentDateOpening'] >= pd.to_datetime('2019-01-01'))\n",
    "]['Loyalty#']\n",
    "\n",
    "print(f\"Customers with invalid redemption sequence (enrolled after 2019): {len(invalid_sequence_ids)}\")\n",
    "\n",
    "df_Customer = df_Customer[~df_Customer['Loyalty#'].isin(invalid_sequence_ids)].copy()\n",
    "df_Flights = df_Flights[~df_Flights['Loyalty#'].isin(invalid_sequence_ids)].copy()\n",
    "\n",
    "print(f\"Customers after removing invalid redemption sequence: {len(df_Customer):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21bc5b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Summary of Actions Taken</h3>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Removed mismatched IDs:</strong> Deleted records with Loyalty# present in one database but missing in the other (20 customers)</li>\n",
    "        <li><strong>Fixed zero-flight distances:</strong> Set DistanceKM, PointsAccumulated, and NumFlightsWithCompanions to 0 where NumFlights == 0</li>\n",
    "        <li><strong>Removed impossible distances:</strong> Set all flight metrics to 0 for records showing >14,000 km per flight (physically impossible)</li>\n",
    "        <li><strong>Removed inconsistent enrollments:</strong> Deleted 168 customers enrolled before 2021 but with '2021 Promotion' enrollment type (logically inconsistent)</li>\n",
    "        <li><strong>Removed impossible redemption rates:</strong> Deleted customers enrolled after 2019 with redemption_rate > 1 (cannot have accumulated points before data window, indicates data error)</li>\n",
    "        <li><strong>Removed invalid cancellation dates:</strong> Deleted customers with CancellationDate before EnrollmentDateOpening (impossible timeline)</li>\n",
    "        <li><strong>Removed invalid redemption sequences:</strong> Deleted customers who redeemed points before accumulating any (enrolled after 2019, so no prior accumulation possible)</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a5114a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fce8e8ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #8B0000, #A52A2A, #CD5C5C, #F08080) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #8B0000; font-weight: bold;\">Critical: Data Quality Cleaning</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>2,328 customers (14.0%) were removed</strong> from the dataset through sequential cleaning steps to ensure data quality and logical consistency.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Why This Matters:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li><strong>Behavioral clustering requires behavior:</strong> Customers with no flights have zero values for all behavioral features (avg_distance, distance_variability, companion_ratio, points, etc.), making them impossible to segment meaningfully.</li>\n",
    "        <li><strong>Data integrity:</strong> Customers with impossible timelines (cancellation before enrollment, redemption before accumulation) indicate data errors that would introduce noise.</li>\n",
    "        <li><strong>Business relevance:</strong> Non-flyers and data anomalies represent different strategic challenges outside the scope of behavioral clustering.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Dataset Impact:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Original:</strong> 16,594 | <strong>After ID mismatch:</strong> 16,574 (-20) | <strong>After non-flyers:</strong> 15,080 (-1,494) | <strong>After inconsistent enrollment:</strong> 14,912 (-168) | <strong>After impossible redemption rate:</strong> 14,450 (-462) | <strong>After invalid cancellation:</strong> 14,295 (-155) | <strong>After invalid redemption sequence:</strong> 14,266 (-29) | <strong>Final:</strong> 14,266 (86.0% retained)\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02263923",
   "metadata": {},
   "source": [
    "## **2.4 Missingness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92218ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in flightsDB\n",
    "df_Flights.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acec92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in CustomerDB\n",
    "missing_customer = pd.DataFrame({\n",
    "    'n_missing': df_Customer.isnull().sum(),\n",
    "    '%_missing': round(df_Customer.isnull().mean() * 100, 2)\n",
    "})\n",
    "missing_customer = missing_customer[missing_customer['n_missing'] > 0]\n",
    "print('Missing values in CustomerDB:')\n",
    "print(missing_customer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b79731",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Summary of Actions Taken</h3>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>No imputation for CancellationDate</strong> - missing values are logical</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816ab95",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Key Actionable</h3>\n",
    "    <p style=\"margin: 0; color: #000; margin-right: 40px; margin-bottom: 10px;\">\n",
    "        As those missing values in the Column <strong>CancellationDate</strong> are <strong>logical</strong> (customers not cancelled yet), we will <strong>not impute</strong> them, but will create a <strong>new feature</strong> later indicating loyal vs not loyalty customers, which is called <strong>\"is_loyal\"</strong>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a6f69",
   "metadata": {},
   "source": [
    "## **2.5 Outlier Handling Strategy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484dc72a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Ändern ausstehended Outlier and Scaling Strategy</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">The Challenge:</h4>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "       Outlier detection in customer segmentation requires distinguishing between three fundamentally different cases. Section 2.3 already removed obvious data errors (negative income, impossible distances), but statistical outliers present a more nuanced challenge. Extreme values may represent either remaining data quality issues or legitimate high-value customers that clustering algorithms need to handle carefully.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Two-Perspective Strategy:</h4>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        <strong>Demographic Features:</strong> Primarily discrete and ordinal variables (Income bins 0-4, Education levels 0-2, Location codes 0-2) plus frequency-encoded geography (Province, City, FSA). Feature engineering through binning and collapsing already addressed univariate outliers by converting continuous extremes into discrete categories.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        <strong>Behavioral Features:</strong> Entirely continuous variables (flights 0-200, distance 0-300k km, points 0-3M) that retain full numeric ranges and require explicit outlier detection and handling.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Three-Stage Approach:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 10px;\">\n",
    "            <strong>Stage 1: Data Errors (Section 2.3, Completed)</strong>\n",
    "            <ul style=\"margin-top: 6px; padding-left: 20px;\">\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Scope:</em> Both demographic and behavioral features</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Examples:</em> Income < $0, Age > 120, DistanceKM > 50,000 per flight, logically inconsistent combinations</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Method:</em> Domain knowledge and business rules</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Action:</em> Delete (removed X customers, Y% of original dataset)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-bottom: 10px;\">\n",
    "            <strong>Stage 2: Feature Classification and Differential Scaling (Section 2.5, Section 5)</strong>\n",
    "            <ul style=\"margin-top: 6px; padding-left: 20px;\">\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Scope:</em> All features after data cleaning, analyzed individually</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Method:</em> Distribution analysis (skewness, IQR outlier detection, boxplots) to classify each feature into categories</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Categories:</em>\n",
    "                    <ul style=\"padding-left: 20px; margin-top: 4px;\">\n",
    "                        <li><strong>Category 1 (Normal/Moderate):</strong> |Skewness| < 1.0, outliers < 5% → StandardScaler</li>\n",
    "                        <li><strong>Category 2 (Heavily Skewed):</strong> |Skewness| ≥ 1.0, outliers ≥ 5% → RobustScaler</li>\n",
    "                        <li><strong>Category 3 (Binned/Discrete):</strong> Some Features will be binned and this will be addressed in the feature engineering section 3 → StandardScaler</li>\n",
    "                    </ul>\n",
    "                </li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Action:</em> Assign appropriate scaler per feature, apply differential scaling in Section 5</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Rationale:</em> Features with clean distributions use StandardScaler (interpretable Z-scores), while heavily skewed features use RobustScaler (outlier-resistant median/IQR)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li style=\"margin-bottom: 10px;\">\n",
    "            <strong>Stage 3: Multivariate Outlier Detection (Section 8.0, Behavioral Only)</strong>\n",
    "            <ul style=\"margin-top: 6px; padding-left: 20px;\">\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Scope:</em> Behavioral features only (continuous multidimensional space)</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Examples:</em> High flights + low distance (short-haul commuter), low flights + extreme distance (single long trip), high points accumulated + zero redemption (hoarder)</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Method:</em> DBSCAN density-based clustering identifies low-density regions in behavioral feature space</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Action:</em> Separate into df_behavioral_outliers for dedicated profiling (typically 2-3%, ~300-450 customers)</li>\n",
    "                <li style=\"margin-bottom: 4px;\"><em>Rationale:</em> Unusual behavioral combinations pull K-Means centroids, create spurious hierarchical merges, and distort SOM topology. Separation ensures core algorithms work on typical patterns while preserving outliers for niche analysis</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Why No DBSCAN for Demographics:</h4>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        Demographic features are predominantly discrete (Income bins, Education levels, Location codes) with approximately 270 theoretical combinations across 14,527 customers (average 54 customers per combination). Unusual combinations like \"High Income + Rural + Low Education\" represent potentially valuable niche segments (entrepreneurs, remote workers, farmers) rather than statistical noise. Feature engineering through binning already converted continuous extremes into discrete categories, addressing univariate outliers. DBSCAN on discrete feature space would fragment meaningful segments rather than isolate distortive outliers. Therefore, demographic clustering (Sections 7.1-7.5) proceeds directly on all customers without multivariate outlier separation.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Why DBSCAN for Behavioral:</h4>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        Behavioral features are entirely continuous (flights 0-200, distance 0-300k km, points 0-3M), creating a true multidimensional continuous space where multivariate outliers represent unusual behavioral patterns. Unlike demographics where unusual combinations might be valid market segments, behavioral outliers like \"2 flights with 100k km total\" or \"200 flights with 5k km total\" represent statistically rare patterns that severely distort distance-based clustering. DBSCAN separation ensures core behavioral clustering captures typical travel patterns while preserving unusual behaviors for dedicated strategic analysis.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Outcome:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Demographic clustering (Sections 7.1-7.5):</strong> Operates on all active customers with differential scaling based on feature characteristics, no multivariate outlier removal</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Behavioral clustering (Sections 8.1-8.x):</strong> Operates on df_behavioral_core (~97% of customers) after DBSCAN separation, with multivariate outliers profiled separately in Section 9</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Transparency:</strong> Full documentation of sample sizes at each stage, scaler assignments per feature, and outlier characteristics</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>No deletion of legitimate customers:</strong> Only data errors removed, all statistical outliers retained either in core clustering or separate outlier segments</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5046675",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32491490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numeric_distribution(\n",
    "    df,\n",
    "    column,\n",
    "    color=None,\n",
    "    show_kde=True,\n",
    "    y_scale=\"linear\",\n",
    "    bins=None,\n",
    "    integer_ticks=False,\n",
    "    show_boxplot=True,\n",
    "    show_pct_labels=True,\n",
    "):\n",
    "    series = df[column].dropna()\n",
    "    total_count = len(series)\n",
    "    sns.set_style(\"white\")\n",
    "    if color is None:\n",
    "        color = CUSTOM_HEX[1]\n",
    "\n",
    "    ncols = 2 if show_boxplot else 1\n",
    "    fig, axes = plt.subplots(1, ncols, figsize=(16, 4) if show_boxplot else (8, 4))\n",
    "    if show_boxplot:\n",
    "        hist_ax, box_ax = axes\n",
    "    else:\n",
    "        hist_ax = axes\n",
    "\n",
    "    hist_kwargs = dict(\n",
    "        x=series,\n",
    "        ax=hist_ax,\n",
    "        kde=show_kde,\n",
    "        color=color,\n",
    "        edgecolor=color,\n",
    "        linewidth=1,\n",
    "        stat=\"count\",\n",
    "    )\n",
    "    if bins is not None:\n",
    "        hist_kwargs[\"bins\"] = bins\n",
    "\n",
    "    sns.histplot(**hist_kwargs)\n",
    "\n",
    "    # Add percentage labels on top of bars\n",
    "    if show_pct_labels:\n",
    "        for patch in hist_ax.patches:\n",
    "            height = patch.get_height()\n",
    "            if height > 0:\n",
    "                pct = height / total_count * 100\n",
    "                hist_ax.annotate(\n",
    "                    f'{pct:.1f}%',\n",
    "                    xy=(patch.get_x() + patch.get_width() / 2, height),\n",
    "                    ha='center',\n",
    "                    va='bottom',\n",
    "                    fontsize=12,\n",
    "                    fontweight='bold'\n",
    "                )\n",
    "\n",
    "    if integer_ticks:\n",
    "        hist_ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        if show_boxplot:\n",
    "            box_ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    if y_scale == \"log\":\n",
    "        hist_ax.set_yscale(\"log\")\n",
    "        y_label = \"Count (log scale)\"\n",
    "    elif y_scale == \"sqrt\":\n",
    "        hist_ax.set_yscale(FuncScale(hist_ax, functions=(np.sqrt, lambda y: y**2)))\n",
    "        y_label = \"Count (sqrt scale)\"\n",
    "    else:\n",
    "        y_label = \"Count\"\n",
    "\n",
    "    hist_ax.set_title(f\"{column} Distribution\")\n",
    "    hist_ax.set_xlabel(column)\n",
    "    hist_ax.set_ylabel(y_label)\n",
    "    hist_ax.grid(False)\n",
    "\n",
    "    if show_boxplot:\n",
    "        sns.boxplot(\n",
    "            x=series,\n",
    "            ax=box_ax,\n",
    "            color=color,\n",
    "            saturation=1,\n",
    "            linewidth=1,\n",
    "            flierprops=dict(marker=\"o\", markersize=4, markerfacecolor=\"white\", markeredgecolor=\"black\"),\n",
    "            boxprops=dict(alpha=0.9),\n",
    "            whiskerprops=dict(color=\"0.3\", linewidth=1),\n",
    "            capprops=dict(color=\"0.3\", linewidth=1),\n",
    "            medianprops=dict(color=\"0.2\", linewidth=1.5),\n",
    "        )\n",
    "        box_ax.set_title(f\"{column} Boxplot\")\n",
    "        box_ax.set_xlabel(column)\n",
    "        box_ax.set_yticks([])\n",
    "        box_ax.grid(False)\n",
    "        box_ax.spines[\"left\"].set_visible(False)\n",
    "        sns.despine(ax=box_ax, left=True)\n",
    "\n",
    "    sns.despine(ax=hist_ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_categorical_distribution(df, column, top_n=None):\n",
    "    \"\"\"\n",
    "    Plot category counts as a horizontal bar chart.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Source data.\n",
    "    column : str\n",
    "        Categorical column to summarise.\n",
    "    top_n : int, optional\n",
    "        Keep only the N most frequent categories.\n",
    "    \"\"\"\n",
    "    series = df[column].dropna().astype(str)\n",
    "    counts = series.value_counts()\n",
    "    total = counts.sum()\n",
    "\n",
    "    # Sort by frequency (descending)\n",
    "    counts = counts.sort_values(ascending=False)\n",
    "    \n",
    "    # Keep only top N if specified\n",
    "    if top_n is not None:\n",
    "        counts = counts.iloc[:top_n]\n",
    "\n",
    "    sns.set_style(\"white\")\n",
    "    color = CUSTOM_HEX[1]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    sns.barplot(x=counts.values, y=counts.index, color=color, edgecolor=\"0.3\", ax=ax)\n",
    "    ax.set_title(f\"{column} Distribution\")\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    ax.set_ylabel(column)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    # Always annotate with percentages\n",
    "    if len(counts) > 0:\n",
    "        offset = counts.values.max() * 0.02\n",
    "        ax.set_xlim(0, counts.values.max() * 1.1)\n",
    "        for val, idx in zip(counts.values, range(len(counts))):\n",
    "            percentage = (val / total) * 100\n",
    "            ax.text(val + offset, idx, f\"{percentage:.1f}%\", va=\"center\", ha=\"left\", fontsize=9)\n",
    "\n",
    "    sns.despine(ax=ax, left=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Systematic identification of statistical outliers using IQR method\n",
    "\n",
    "IQR_THRESHOLD = 1.5\n",
    "\n",
    "def detect_univariate_outliers(series, feature_name, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using Interquartile Range (IQR) method.\n",
    "    \"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - threshold * IQR\n",
    "    upper_bound = Q3 + threshold * IQR\n",
    "    \n",
    "    outliers_mask = (series < lower_bound) | (series > upper_bound)\n",
    "    n_outliers = outliers_mask.sum()\n",
    "    pct_outliers = (n_outliers / len(series)) * 100\n",
    "    \n",
    "    lower_outliers = (series < lower_bound).sum()\n",
    "    upper_outliers = (series > upper_bound).sum()\n",
    "    \n",
    "    return {\n",
    "        'Feature': feature_name,\n",
    "        'N_Analyzed': len(series),\n",
    "        'Q1': Q1,\n",
    "        'Q3': Q3,\n",
    "        'IQR': IQR,\n",
    "        'Lower_Bound': lower_bound,\n",
    "        'Upper_Bound': upper_bound,\n",
    "        'Total_Outliers': n_outliers,\n",
    "        'Outlier_Pct': pct_outliers,\n",
    "        'Lower_Outliers': lower_outliers,\n",
    "        'Upper_Outliers': upper_outliers,\n",
    "        'Min_Value': series.min(),\n",
    "        'Max_Value': series.max()\n",
    "    }\n",
    "\n",
    "def analyze_outliers(df, features, caption=None, exclude_zeros=False):\n",
    "    \"\"\"\n",
    "    Analyze outliers for multiple features and return a summary table.\n",
    "    \"\"\"\n",
    "    outlier_results = []\n",
    "    \n",
    "    for feature in features:\n",
    "        if exclude_zeros:\n",
    "            series = df[df[feature] > 0][feature]\n",
    "        else:\n",
    "            series = df[feature].dropna()\n",
    "        \n",
    "        if len(series) > 0:\n",
    "            result = detect_univariate_outliers(series, feature, threshold=IQR_THRESHOLD)\n",
    "            if exclude_zeros:\n",
    "                result['Zero_Count'] = (df[feature] == 0).sum()\n",
    "                result['Zero_Pct'] = (df[feature] == 0).sum() / len(df) * 100\n",
    "            outlier_results.append(result)\n",
    "\n",
    "    columns = ['Feature', 'N_Analyzed', 'Total_Outliers', 'Outlier_Pct', 'Upper_Outliers', 'Upper_Bound', 'Lower_Outliers', 'Lower_Bound', 'Min_Value', 'Max_Value']\n",
    "    if exclude_zeros:\n",
    "        columns.insert(2, 'Zero_Pct')\n",
    "    \n",
    "    summary = pd.DataFrame(outlier_results)[columns]\n",
    "    \n",
    "    if caption:\n",
    "        print(caption)\n",
    "    display(summary)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def analyze_skewness(df, features, skew_threshold=1.0):\n",
    "    \"\"\"\n",
    "    Analyze skewness of features to determine if log transformation is needed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing the features\n",
    "    features : list\n",
    "        List of column names to analyze\n",
    "    skew_threshold : float, default=1.0\n",
    "        Threshold above which a feature is considered highly skewed\n",
    "        |skewness| > threshold → recommend log transformation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame : Summary with skewness values and transformation recommendations\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for feature in features:\n",
    "        series = df[feature].dropna()\n",
    "        skewness = series.skew()\n",
    "        \n",
    "        # Determine recommendation\n",
    "        if abs(skewness) > skew_threshold:\n",
    "            recommendation = \"Log Transform\"\n",
    "        else:\n",
    "            recommendation = \"No Transform\"\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': feature,\n",
    "            'N_Analyzed': len(series),\n",
    "            'Skewness': skewness,\n",
    "            'Recommendation': recommendation\n",
    "        })\n",
    "    \n",
    "    summary = pd.DataFrame(results)\n",
    "    \n",
    "    # Print features that need transformation\n",
    "    skewed_features = summary[summary['Recommendation'] == \"Log Transform\"]['Feature'].tolist()\n",
    "    if skewed_features:\n",
    "        print(f\"Features requiring log transformation (|skewness| > {skew_threshold}):\")\n",
    "        for f in skewed_features:\n",
    "            skew_val = summary[summary['Feature'] == f]['Skewness'].values[0]\n",
    "            print(f\"  - {f}: skewness = {skew_val:.2f}\")\n",
    "    else:\n",
    "        print(f\"No features exceed skewness threshold of {skew_threshold}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb1b9b9",
   "metadata": {},
   "source": [
    "### **Part 0: Business Related Exclusions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fd3c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove customers with first flight after July 2021\n",
    "# Customers with only 1-2 months of flight data become artificial outliers in behavioral features like distance_variability or companion_flight_ratio, and seasonal_concentration etc. (calculated on monthly aggregates). Requiring first flight by July 2021 ensures at least 6 months of data for meaningful patterns.\n",
    "\n",
    "print(f\"Customers before removing late-starters: {len(df_Customer):,}\")\n",
    "\n",
    "# Calculate first flight date per customer\n",
    "first_flight = (\n",
    "    df_Flights[df_Flights['NumFlights'] > 0]\n",
    "    .groupby('Loyalty#')['YearMonthDate']\n",
    "    .min()\n",
    ")\n",
    "\n",
    "# Convert to datetime for proper comparison\n",
    "first_flight_dt = pd.to_datetime(first_flight + \"-01\")\n",
    "cutoff_date = pd.to_datetime(\"2021-07-01\")\n",
    "\n",
    "customers_with_enough_history = first_flight_dt[first_flight_dt <= cutoff_date].index\n",
    "\n",
    "df_Customer = df_Customer[df_Customer['Loyalty#'].isin(customers_with_enough_history)].copy()\n",
    "df_Flights = df_Flights[df_Flights['Loyalty#'].isin(customers_with_enough_history)].copy()\n",
    "\n",
    "print(f\"Customers after removing late-starters: {len(df_Customer):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e33e21",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fce8e8ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #8B0000, #A52A2A, #CD5C5C, #F08080) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #8B0000; font-weight: bold;\">Critical: Late-Starting Customers Removed</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>731 customers were removed</strong> because their first flight occurred after July 2021.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Why This Matters:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li><strong>Insufficient data for behavioral features:</strong> Customers with less than 6 months of flight history produce unreliable coefficients of variation (distance_variability), ratios (companion_flight_ratio), and seasonal patterns since these metrics require multiple observations across time.</li>\n",
    "        <li><strong>Seasonal concentration requires multiple seasons:</strong> The seasonal_concentration feature (Gini coefficient across Winter/Spring/Summer/Fall) is meaningless for customers who only have data in 1-2 seasons, artificially inflating their concentration score.</li>\n",
    "        <li><strong>Minimum threshold:</strong> Requiring first flight by July 2021 ensures at least 6 months of potential flight activity (Jul-Dec), covering at least 3 seasons for meaningful seasonal pattern detection. Because Winter is starting on December in our definition.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Dataset Impact:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Before:</strong> 14,266 customers | <strong>After:</strong> 13,535 customers\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856edfe0",
   "metadata": {},
   "source": [
    "### **Part 1: df_Customer Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0734fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_features = ['Income', 'Customer Lifetime Value']\n",
    "\n",
    "# Plots\n",
    "for feature in customer_features:\n",
    "    plot_numeric_distribution(\n",
    "        df_Customer, \n",
    "        feature,\n",
    "        show_pct_labels=False\n",
    "    )\n",
    "\n",
    "\n",
    "# Outlier Summary\n",
    "customer_summary = analyze_outliers(\n",
    "    df_Customer, \n",
    "    features=customer_features,\n",
    "    caption='df_Customer Features (Income, Customer Lifetime Value) Outlier Summary (IQR Method)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afb3cd4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Decision</h3>\n",
    "    <p style=\"margin: 0 40px 10px 0; color: #000;\">\n",
    "        <strong>Note:</strong> Skewness analysis is skipped for these features. Income will be binned in Section 3 (rendering skewness irrelevant), and Customer Lifetime Value is not used in clustering. For behavioral features, we apply formal skewness checks to determine log transformation needs.\n",
    "    </p>\n",
    "    <ul style=\"margin: 0; color: #000; margin-right: 40px; padding-left: 20px;\">\n",
    "        <li style=\"margin-bottom: 8px;\">\n",
    "            <strong>Income:</strong> Right skewed distribution visible in histogram with high concentration at low values. Equal width binning would create imbalanced categories. Instead, we apply <strong>custom bins based on domain knowledge</strong> in Section 3 to create meaningful income segments. The binned feature (Income_Bin_Num) will use <strong>StandardScaler</strong>.\n",
    "        </li>\n",
    "        <li style=\"margin-bottom: 8px;\">\n",
    "            <strong>Customer Lifetime Value:</strong> Heavily right skewed with significant outlier percentage visible in boxplot. However, this feature is <strong>not used in clustering</strong> as value-based segmentation is derived from FM features (Frequency, Monetary) instead. No scaling or transformation required.\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079c5e96",
   "metadata": {},
   "source": [
    "### **Part 2: df_Flights Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00985719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As mentioned in the EDA Part: \"Fractional counts in flight metrics: The KPIs (NumFlights, NumFlightsWithCompanions, DistanceKM, PointsAccumulated, PointsRedeemed, DollarCostPointsRedeemed) contain decimal values in 2019. These likely stem from multi-year aggregation\"\n",
    "\n",
    "# Summary table: Float analysis per column and year (excluding zeros)\n",
    "summary = []\n",
    "for col in df_Flights.select_dtypes(\"float\").columns:\n",
    "    row = {'Column': col}\n",
    "    for year in sorted(df_Flights['Year'].unique()):\n",
    "        non_zero = df_Flights[(df_Flights['Year'] == year) & (df_Flights[col] != 0)]\n",
    "        floats = (non_zero[col] % 1 != 0).sum()\n",
    "        total = len(non_zero)\n",
    "        row[f'{year} (n)'] = floats\n",
    "        row[f'{year} (%)'] = round(floats / total * 100, 2) if total > 0 else 0\n",
    "    summary.append(row)\n",
    "\n",
    "display(pd.DataFrame(summary).set_index('Column'))\n",
    "\n",
    "# Remove 2019 data due to quality issues (90-99% fractional values)\n",
    "df_Flights = df_Flights[df_Flights['Year'] != 2019].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fe6b00",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fce8e8ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #8B0000, #A52A2A, #CD5C5C, #F08080) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #8B0000; font-weight: bold;\">Critical: 2019 Data excluded due to Quality Issues</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>All 2019 flight records are excluded</strong> from behavioral feature engineering due to pervasive fractional values indicating data quality problems.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Why This Matters:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li><strong>Fractional values in count metrics:</strong> 90% to 99% of non-zero values in 2019 are floats for NumFlights, NumFlightsWithCompanions, PointsAccumulated, PointsRedeemed, and DollarCostPointsRedeemed. These metrics should be integers (e.g., 3 flights, not 2.67 flights).</li>\n",
    "        <li><strong>Likely cause:</strong> Multi-year aggregation or data migration artifacts from legacy systems created fractional counts that do not represent actual customer behavior.</li>\n",
    "        <li><strong>2020 and 2021 are clean:</strong> Float ratios drop to 0% in 2020 and 2021, confirming data quality is resolved for these years.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Analysis uses only 2020 and 2021 data.</strong> While this reduces the observation window for seasonal features, it ensures reliable behavioral metrics. Since we focus on customers active in 2021 who could also fly in 2020, two full years provide sufficient data for meaningful segmentation.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c766f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: df_Flights Features (non zero values only, as most records are zero)\n",
    "flight_features = [\n",
    "    'NumFlights',\n",
    "    'NumFlightsWithCompanions', \n",
    "    'DistanceKM',\n",
    "    'PointsAccumulated',\n",
    "    'PointsRedeemed'\n",
    "]\n",
    "\n",
    "for feature in flight_features:\n",
    "    plot_numeric_distribution(\n",
    "        df_Flights[df_Flights[feature] > 0], \n",
    "        feature,\n",
    "        show_pct_labels=False\n",
    "    )\n",
    "\n",
    "# Outlier Summary\n",
    "flight_summary = analyze_outliers(\n",
    "    df_Flights, \n",
    "    features=flight_features,\n",
    "    caption='df_Flights Features (non zero values only, as most records are zero) Outlier Summary (IQR Method)',\n",
    "    exclude_zeros=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a23661",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Decision</h3>\n",
    "    <p style=\"margin: 0; color: #000; margin-right: 40px; margin-bottom: 10px;\">\n",
    "        <strong>Note:</strong> Skewness analysis is skipped for these raw features. They exist at monthly granularity and will be aggregated per customer in Section 3, fundamentally changing their distributions. Skewness and scaling decisions will be made after feature engineering.\n",
    "    </p>\n",
    "    <p style=\"margin: 0; color: #000; margin-right: 40px; margin-bottom: 10px;\">\n",
    "        <strong>Outlier Summary:</strong> At monthly level, outlier percentages are minimal:\n",
    "    </p>\n",
    "    <ul style=\"margin: 0 40px 10px 20px; color: #000; padding-left: 20px;\">\n",
    "        <li><strong>NumFlights:</strong> 0.27% outliers (504 of 188,245 non-zero records)</li>\n",
    "        <li><strong>NumFlightsWithCompanions:</strong> 1.49% outliers (1,466 of 98,208)</li>\n",
    "        <li><strong>DistanceKM:</strong> 0% outliers</li>\n",
    "        <li><strong>PointsAccumulated:</strong> 0% outliers</li>\n",
    "        <li><strong>PointsRedeemed:</strong> 0.48% outliers (106 of 22,234)</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 0; color: #000; margin-right: 40px; margin-bottom: 10px;\">\n",
    "        These raw features will <strong>not be used directly</strong> for clustering. In Section 3 (Feature Engineering), they will be <strong>aggregated per customer</strong> through transformations such as sum (total flights, total distance), mean (average monthly activity), or derived ratios (companion flight ratio, points redemption rate). This aggregation fundamentally changes the distributions as customer level totals will show different patterns than monthly records.\n",
    "    </p>\n",
    "    <p style=\"margin: 0; color: #000; margin-right: 40px; margin-bottom: 10px;\">\n",
    "        Therefore, the <strong>final scaling decision</strong> for behavioral features will be made in Section 3 after feature engineering is complete. The newly created features will be re-evaluated for skewness and outlier percentage to determine whether StandardScaler or RobustScaler is appropriate. Additionally, as outlined in the strategy, <strong>multivariate outliers</strong> in the behavioral feature space will be identified using DBSCAN in Section 8 before core clustering.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077409a0",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='3'> </a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>3. Feature Engineering</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a4e3a9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px;\"> <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Feature Engineering Overview</h3> <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\"> <li style=\"margin-bottom: 10px;\"> <strong>Loyalty Lifecycle & Focus Groups:</strong> <ul style=\"margin-top: 6px; padding-left: 20px;\"> <li>\"Which segment do they belong to?\"</li> <li>Metrics: is_current_loyalty_member, is_active, Focus_Group (1/2)</li> <li>Focus: Customer filtering and stratification for targeted clustering</li> </ul> </li> <li style=\"margin-bottom: 10px;\"> <strong>Value-Based:</strong> <ul style=\"margin-top: 6px; padding-left: 20px;\"> <li>\"How much value do they generate?\"</li> <li>Metrics: Frequency, Monetary, Recency</li> <li>Focus: Economic contribution</li> </ul> </li> <li style=\"margin-bottom: 10px;\"> <strong>Demographic:</strong> <ul style=\"margin-top: 6px; padding-left: 20px;\"> <li>\"Who are they?\"</li> <li>Metrics: Income, Education, Location, Gender, Marital Status, Province, City, FSA</li> <li>Focus: Socioeconomic segments</li> </ul> </li> <li style=\"margin-bottom: 10px;\"> <strong>Behavioral:</strong> <ul style=\"margin-top: 6px; padding-left: 20px;\"> <li>\"How do they fly?\" (patterns, style, preferences)</li> <li>Metrics: avg_distance_per_flight, distance_variability, companion_flight_ratio, flight_regularity, seasonal_concentration, peak_season_sin/cos, redemption_rate, redemption_frequency</li> <li>Focus: Travel style & engagement patterns</li> </ul> </li> </ul> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27435d9",
   "metadata": {},
   "source": [
    "## **3.1 Loyalty Lifecycle & Focus Group Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09865645",
   "metadata": {},
   "source": [
    "### Is_Active and Is_Loyal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2747270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'is_current_loyalty_member' feature\n",
    "# Customer is a current member if they have no CancellationDate\n",
    "df_Customer['is_current_loyalty_member'] = (\n",
    "    df_Customer['EnrollmentDateOpening'].notna() & \n",
    "    df_Customer['CancellationDate'].isna()\n",
    ")\n",
    "\n",
    "# Create 'is_active' feature: customer had flights in 2021\n",
    "flights_2021_activity = (\n",
    "    df_Flights[df_Flights['Year'] == 2021]\n",
    "    .groupby('Loyalty#')['NumFlights']\n",
    "    .sum()\n",
    "    .rename('total_flights_2021_check')\n",
    ")\n",
    "\n",
    "df_Customer = df_Customer.merge(flights_2021_activity, on='Loyalty#', how='left')\n",
    "df_Customer['is_active'] = (df_Customer['total_flights_2021_check'].fillna(0) > 0)\n",
    "df_Customer.drop('total_flights_2021_check', axis=1, inplace=True)\n",
    "\n",
    "# Summary statistics\n",
    "crosstab = pd.crosstab(\n",
    "    df_Customer['is_current_loyalty_member'],\n",
    "    df_Customer['is_active'],\n",
    "    margins=True\n",
    ")\n",
    "\n",
    "# Rename and reorder\n",
    "crosstab.index = crosstab.index.map({True: 'Loyalty', False: 'Non-Loyalty', 'All': 'Total'})\n",
    "crosstab.columns = crosstab.columns.map({True: 'Active', False: 'Inactive', 'All': 'Total'})\n",
    "crosstab = crosstab.loc[['Loyalty', 'Non-Loyalty', 'Total'], ['Active', 'Inactive', 'Total']]\n",
    "crosstab.index.name = crosstab.columns.name = None\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec89600",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Activity & Loyalty Labels Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        We created <strong>binary labels</strong> to identify customer loyalty status and 2021 flight activity, enabling targeted segmentation for clustering analysis.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Features (Why):</h4>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        These labels define the <strong>scope of our clustering analysis</strong> by filtering customers into relevant focus groups. Active customers (those with 2021 flights) are our primary target for segmentation, while loyalty status determines whether we analyze current members (retention strategies) or ex-members (win-back strategies). This filtering ensures clustering focuses on actionable, engaged customer segments rather than inactive or churned populations.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definitions (What):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>is_current_loyalty_member:</strong> True if customer is currently enrolled in the loyalty program (CancellationDate is NaN). Separates active program members from ex-members.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>is_active:</strong> True if customer had at least one flight in 2021 (total_flights_2021 > 0). Identifies customers with recent engagement versus inactive/dormant customers.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Implementation Details (How):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Why focus on 2021 activity?</strong> 2021 represents the most recent complete year of data, ensuring we cluster customers based on current behavior rather than historical patterns that may no longer be relevant.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Interpretation:</h4>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        The crosstab reveals a <strong>highly engaged customer base</strong>: 13,038 of 13,535 customers (96.3%) were active in 2021, with only 497 inactive. Key insights:\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Loyalty Members:</strong> 12,510 customers (92.4%), of which 12,470 (99.7%) were active in 2021. Extremely high engagement among current members.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Non-Loyalty (Ex-Members):</strong> 1,025 customers (7.6%), with 568 (55.4%) still active despite cancelling membership. These represent win-back opportunities.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Inactive Customers:</strong> Only 497 total (3.7%), predominantly ex-members (457). Minimal inactivity among current loyalty members (40).</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        <strong>Implication:</strong> The dataset is dominated by active loyalty members, making it ideal for behavioral segmentation.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744c2810",
   "metadata": {},
   "source": [
    "### Is_Focus_Group and Focus_Group\n",
    "\n",
    "Define focus groups for targeted analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a04a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'Is_Focus_Group' feature: True if customer belongs to a focus group (Active customers)\n",
    "df_Customer['Is_Focus_Group'] = df_Customer['is_active']\n",
    "\n",
    "# Create 'Focus_Group' feature: \n",
    "# 1 = Loyalty Members & Active (Focus Group 1)\n",
    "# 2 = Ex-Loyalty Members & Active (Focus Group 2)\n",
    "# NaN = Not in any focus group (inactive customers)\n",
    "df_Customer['Focus_Group'] = None\n",
    "df_Customer.loc[\n",
    "    df_Customer['is_current_loyalty_member'] & df_Customer['is_active'], \n",
    "    'Focus_Group'\n",
    "] = 1\n",
    "df_Customer.loc[\n",
    "    ~df_Customer['is_current_loyalty_member'] & df_Customer['is_active'], \n",
    "    'Focus_Group'\n",
    "] = 2\n",
    "\n",
    "# Display summary\n",
    "focus_group_summary = df_Customer['Focus_Group'].value_counts(dropna=False).sort_index()\n",
    "focus_group_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e56f135",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Focus Group Assignment Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        We created <strong>focus group identifiers</strong> to separate active customers into two strategically important segments based on loyalty membership status.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Features (Why):</h4>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        Focus groups enable <strong>parallel clustering strategies</strong> for different customer populations with distinct business objectives. Focus Group 1 (current loyalty & active members) requires retention and upsell strategies, while Focus Group 2 (ex-loyalty but active members) presents win-back opportunities. By clustering these groups separately, we can identify personas with actionable, group-specific marketing interventions rather than generic one-size-fits-all segments.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definitions (What):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Is_Focus_Group:</strong> Boolean flag (True/False) indicating whether a customer belongs to any focus group. Equivalent to <strong>is_active</strong>. All active customers (with 2021 flights) are included in focus groups; inactive customers are excluded from clustering analysis.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Focus_Group:</strong> Categorical label with three possible values:\n",
    "            <ul style=\"padding-left: 20px; margin-top: 5px;\">\n",
    "                <li><strong>1</strong> = Loyalty Members & Active (Focus Group 1): Current program members with recent flight activity</li>\n",
    "                <li><strong>2</strong> = Ex-Loyalty Members & Active (Focus Group 2): Former program members who cancelled but still fly with the airline</li>\n",
    "                <li><strong>NaN</strong> = Inactive customers excluded from clustering (no 2021 flights)</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Implementation Details (How):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Why separate FG1 and FG2?</strong> These groups exhibit fundamentally different relationships with the brand. FG1 customers actively participate in the loyalty program (earning/redeeming points), while FG2 customers fly without program benefits. Their demographics, behaviors, and value profiles may differ substantially, justifying separate clustering analyses.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Why exclude inactive customers (NaN)?</strong> Clustering algorithms work best on homogeneous populations. Including dormant customers (no 2021 activity) would create noise and dilute meaningful patterns among active, engaged customers who represent the airline's current revenue base.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Strategic implication:</strong> This structure allows us to build 2-4 personas per focus group (e.g., \"Champions\", \"Frequent Flyers\", \"Premium Occasional\", \"At Risk\"), then combine insights across groups to understand the full active customer landscape.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba2ea8b",
   "metadata": {},
   "source": [
    "## **3.2 Value based Feature Enginnering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9974a28",
   "metadata": {},
   "source": [
    "FM segments customers by **Frequency** and **Monetary** value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf64dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Step 1: Aggregate 2021 flight data per customer\n",
    "flights_2021 = (\n",
    "    df_Flights[df_Flights[\"Year\"] == 2021]\n",
    "    .groupby(\"Loyalty#\", as_index=False)\n",
    "    .agg(\n",
    "        total_flights_2021=(\"NumFlights\", \"sum\"),\n",
    "        total_distance_2021=(\"DistanceKM\", \"sum\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "df_Customer = df_Customer.merge(flights_2021, on=\"Loyalty#\", how=\"left\")\n",
    "df_Customer[[\"total_flights_2021\", \"total_distance_2021\"]] = (\n",
    "    df_Customer[[\"total_flights_2021\", \"total_distance_2021\"]].fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "# Preparing Step 2: First flight date (earliest month with NumFlights > 0)\n",
    "first_flight = (\n",
    "    df_Flights[df_Flights[\"NumFlights\"] > 0]\n",
    "    .groupby(\"Loyalty#\", as_index=False)[\"YearMonthDate\"]\n",
    "    .min()\n",
    "    .rename(columns={\"YearMonthDate\": \"first_flight_date\"})\n",
    ")\n",
    "\n",
    "df_Customer = df_Customer.merge(first_flight, on=\"Loyalty#\", how=\"left\")\n",
    "\n",
    "\n",
    "# Preparing Step 3: Active months in 2021 based on first flight\n",
    "year_start = pd.to_datetime(\"2021-01-01\")\n",
    "year_end = pd.to_datetime(\"2021-12-31\")\n",
    "\n",
    "df_Customer[\"first_flight_date\"] = pd.to_datetime(df_Customer[\"first_flight_date\"])\n",
    "# Only calculate for customers with flights; NaN for customers without flights\n",
    "start_dates = df_Customer[\"first_flight_date\"].apply(lambda x: max(year_start, x) if pd.notna(x) else pd.NaT)\n",
    "df_Customer[\"months_active_2021\"] = (\n",
    "    ((year_end.year - start_dates.dt.year) * 12 + (year_end.month - start_dates.dt.month) + 1)\n",
    "    .where(start_dates.notna())\n",
    "    .clip(lower=0.1)\n",
    ")\n",
    "\n",
    "\n",
    "# Preparing Step 4: Last flight date over all available years\n",
    "last_flight = (\n",
    "    df_Flights[df_Flights[\"NumFlights\"] > 0]\n",
    "    .groupby(\"Loyalty#\", as_index=False)[\"YearMonthDate\"]\n",
    "    .max()\n",
    "    .rename(columns={\"YearMonthDate\": \"last_flight_date\"})\n",
    ")\n",
    "\n",
    "df_Customer = df_Customer.merge(last_flight, on=\"Loyalty#\", how=\"left\")\n",
    "\n",
    "# F = Frequency (flights per active month in 2021)\n",
    "df_Customer[\"Frequency\"] = (\n",
    "    df_Customer[\"total_flights_2021\"] / df_Customer[\"months_active_2021\"]\n",
    ").fillna(0)\n",
    "\n",
    "# M = Monetary (distance per active month in 2021)\n",
    "df_Customer[\"Monetary\"] = (\n",
    "    df_Customer[\"total_distance_2021\"] / df_Customer[\"months_active_2021\"]\n",
    ").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0444ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FM distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle('FM Features Distribution', fontsize=16, fontweight='bold', y=1.02)\n",
    "\n",
    "# Frequency\n",
    "ax = axes[0]\n",
    "freq_plot = df_Customer['Frequency']\n",
    "ax.hist(freq_plot, bins=50, color=colors[1], edgecolor='white', alpha=0.8)\n",
    "ax.set_title('F - Frequency (Flights per Active Month)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Flights/Month', fontsize=10)\n",
    "ax.set_ylabel('Customers', fontsize=10)\n",
    "ax.axvline(df_Customer['Frequency'].median(), color=colors[3], linestyle='--', \n",
    "           linewidth=2, label=f'Median: {df_Customer[\"Frequency\"].median():.2f}')\n",
    "ax.legend()\n",
    "ax.grid(False)\n",
    "\n",
    "\n",
    "# Monetary\n",
    "ax = axes[1]\n",
    "mon_plot = df_Customer['Monetary']\n",
    "ax.hist(mon_plot, bins=50, color=colors[2], edgecolor='white', alpha=0.8)\n",
    "ax.set_title('M - Monetary (Value Score per Active Month)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Monetary Value', fontsize=10)\n",
    "ax.set_ylabel('Customers', fontsize=10)\n",
    "ax.axvline(df_Customer['Monetary'].median(), color=colors[3], linestyle='--', \n",
    "           linewidth=2, label=f'Median: {df_Customer[\"Monetary\"].median():.4f}')\n",
    "ax.legend()\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851b5115",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Value-Based Features (FM) Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        We implemented <strong>FM-based features</strong> (Frequency, Monetary) to quantify customer engagement and value, focusing exclusively on 2021 flight activity.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Features (Why):</h4>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        FM features serve as the foundation for <strong>value-based customer segmentation</strong>. These features enable us to identify high-value Champions, Frequent Flyers, Premium Occasional travelers, and At-Risk customers using a simple 2D median-split approach that is interpretable for business stakeholders. By normalizing flight activity and distance by active months, we create fair comparisons between customers with different engagement durations throughout 2021.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Why 2021 as Reference Year:</h4>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        <strong>2021 measures current behavior, not historical value.</strong> A customer who flew frequently in 2019-2020 but not in 2021 is churned and no longer valuable for retention strategies. Conversely, a customer who started flying in 2021 and shows high engagement is a hot lead worth prioritizing. Historical flight data would mislead clustering by treating inactive high-value customers as still relevant.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        Customers without 2021 flights are considered <strong>churned/inactive</strong> (497 customers). Reactivating churned customers requires significantly more effort and resources than retaining active ones. Our clustering therefore focuses on the <strong>active customer base</strong>:\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Focus Group 1:</strong> Active + Loyalty Members (12,470 customers) → retention and upsell strategies</li>\n",
    "        <li><strong>Focus Group 2:</strong> Active + Non-Loyalty (568 customers) → win-back to loyalty program</li>\n",
    "        <li><strong>Excluded:</strong> Inactive/Churned (497 customers) → separate reactivation campaigns outside clustering scope</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definitions (What):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Frequency:</strong> Average flights per active month in 2021. Calculated as total_flights_2021 / months_active_2021. Measures flight engagement intensity normalized by customer's actual participation period.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Monetary:</strong> Average distance traveled per active month in 2021. Calculated as total_distance_2021 / months_active_2021. Serves as a proxy for customer value (longer distances typically indicate higher-value routes and ticket prices).</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Implementation Details (How):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>months_active_2021:</strong> Number of months from first flight date until December 2021, bounded by the 2021 calendar year. Clipped to minimum 0.1 to avoid division by zero.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Why normalize by active months?</strong> A customer flying 12 times over 12 months (1/month) has different engagement than one flying 12 times in 3 months (4/month). Monthly averaging captures engagement intensity rather than absolute volume.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5706603e",
   "metadata": {},
   "source": [
    "## **3.3 Demographic based Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c986ae",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Demographic Features Overview</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        We engineer <strong>demographic features</strong> to enable customer profiling through clustering while reducing dimensionality and transforming categorical variables into clustering-friendly numeric formats. These features answer the question: <strong>\"Who is this customer?\"</strong>\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Features:</h4>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        Demographic features provide the foundation for <strong>customer persona development</strong>. By understanding who customers are (education, income, location, gender, marital status), we can create actionable segments like \"Urban High-Earners\", \"Suburban Families\", or \"Budget-Conscious Travelers\". These personas complement behavioral segmentation by adding human context for targeted marketing messages and channel strategies.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Encoding:</h4>\n",
    "    <table style=\"margin: 10px 0; border-collapse: collapse; width: 100%;\">\n",
    "        <tr style=\"background-color: #00411E; color: white;\">\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Feature</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Original</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Encoding</th>\n",
    "        </tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Education_Level_Num</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Education (5 categories)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Ordinal (Low=0, Mid=1, High=2)</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Income_Bin_Num</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Income (continuous)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Binned (Low≤20k=0, Mid≤70k=1, High=2)</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Location_Code_Num</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Location Code</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Ordinal (Rural=0, Suburban=1, Urban=2)</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Province_Encoded</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Province or State</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Frequency Encoding</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">City_Encoded</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">City</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Frequency Encoding</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">FSA_Encoded</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Postal Code (first 3 chars)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Frequency Encoding</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Gender_Encoded</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Gender</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Binary (male=1, female=0)</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Marital_*</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Marital Status</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">One-Hot (Divorced, Married, Single)</td></tr>\n",
    "    </table>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Encoding Rationale:</h4>\n",
    "    <ul style=\"margin: 5px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Education:</strong> Collapsed from 5 to 3 ordinal levels to reduce noise while preserving hierarchy</li>\n",
    "        <li><strong>Income:</strong> Binned into 3 levels based on domain knowledge (Low ≤20k, Mid ≤70k, High >70k)</li>\n",
    "        <li><strong>Location Code:</strong> Ordinal encoding reflecting urbanization level (Rural -> Suburban -> Urban)</li>\n",
    "        <li><strong>Province/City/FSA:</strong> Frequency encoding captures population density without high-cardinality OHE explosion</li>\n",
    "        <li><strong>Gender:</strong> Binary encoding for 2 categories</li>\n",
    "        <li><strong>Marital Status:</strong> One-Hot encoding as no natural order exists between categories</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48ce3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education: Collapse 5 categories into 3 ordinal levels\n",
    "def map_education_level(x):\n",
    "    if x in [\"High School or Below\", \"College\"]:\n",
    "        return \"Low\"\n",
    "    if x in [\"Bachelor\"]:\n",
    "        return \"Mid\"\n",
    "    if x in [\"Master\", \"Doctor\"]:\n",
    "        return \"High\"\n",
    "    return pd.NA\n",
    "\n",
    "df_Customer[\"Education_Level\"] = df_Customer[\"Education\"].apply(map_education_level)\n",
    "\n",
    "edu_mapping = {\"Low\": 0, \"Mid\": 1, \"High\": 2}\n",
    "df_Customer[\"Education_Level_Num\"] = df_Customer[\"Education_Level\"].map(edu_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e85cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, 'Education_Level_Num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07920fcd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Education_Level_Num</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        As we saw in the EDA, Education is completely dominated by Bachelor with roughly 62% and College 25%. The Bachelor educated customers also have the highest average income and customer lifetime value, with the rest being evenly distributed. Collapsing five categories into three ordinal levels reduces dimensionality while preserving the meaningful distinction between education tiers that correlate with customer value.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> One hot encoding five education categories would create five binary features that dominate distance calculations and inflate the feature space. Ordinal encoding into three levels (0, 1, 2) produces a single numeric feature where distances reflect the natural ordering of education (Low → Mid → High). This allows clustering algorithms to group customers with similar education levels together while treating the difference between Low and High as larger than Low and Mid.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Ordinal numeric encoding (0, 1, 2) representing education tier. Bachelor is separated as its own category (Mid) because EDA showed this group has distinct income and value characteristics. College is grouped with High School (Low) rather than Bachelor because their income and CLTV distributions are more similar:\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Low (0):</strong> High School or Below + College (30.3%)</li>\n",
    "        <li><strong>Mid (1):</strong> Bachelor (62.3%, highest value segment)</li>\n",
    "        <li><strong>High (2):</strong> Master + Doctor (7.3%)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler.</strong> Discrete ordinal values (0, 1, 2) have no outliers.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income: Custom bins based on domain knowledge\n",
    "def bin_income(income):\n",
    "    if income <= 20000:\n",
    "        return 0  # Low Income\n",
    "    elif income <= 70000:\n",
    "        return 1  # Middle Income\n",
    "    else:\n",
    "        return 2  # High Income\n",
    "\n",
    "df_Customer[\"Income_Bin_Num\"] = df_Customer[\"Income\"].apply(bin_income)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c48302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, 'Income_Bin_Num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96363b0c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Income_Bin_Num</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        As identified in Section 2.5, Income shows a right skewed distribution. Custom domain knowledge bins transform the continuous skewed feature into a discrete ordinal variable with more balanced category sizes, preventing extreme income values from dominating distance calculations.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Why 3 bins?</strong> Three categories (Low/Middle/High) capture the essential income distinctions relevant for customer segmentation: budget-conscious travelers, mainstream customers, and premium spenders. This granularity aligns with Education_Level_Num (also 0-2), ensuring consistent ordinal scaling across demographic features and avoiding unnecessary dimensionality from fine-grained income splits that offer diminishing business value.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Ordinal numeric encoding (0-2) based on domain knowledge income thresholds:\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>0 (Low Income):</strong> ≤ 20,000 (31.2%)</li>\n",
    "        <li><strong>1 (Middle Income):</strong> 20,001 - 75,000 (49.8%)</li>\n",
    "        <li><strong>2 (High Income):</strong> > 75,000 (19%)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler.</strong> Binning already addressed the original skewness. Discrete ordinal values (0-2) have no outliers.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61391143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSA (Forward Sortation Area): Frequency encoding for geographic feature\n",
    "df_Customer[\"FSA\"] = df_Customer[\"Postal code\"].str[:3]\n",
    "\n",
    "fsa_counts = df_Customer[\"FSA\"].value_counts()\n",
    "\n",
    "df_Customer[\"FSA_Encoded\"] = df_Customer[\"FSA\"].map(fsa_counts).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c63bbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, 'FSA_Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbd6c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['FSA_Encoded'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bbceb5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">FSA_Encoded</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Forward Sortation Area (first 3 characters of postal code) has too many unique categories for one hot encoding. Frequency encoding captures geographic density:\n",
    "        customers from densely populated FSAs get higher values, customers from sparse areas get lower values. This creates a continuous proxy for urban density at the postal code level.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Each FSA is replaced with the count of customers sharing that FSA. Range: 0-1200. Higher values indicate more common postal areas (urban centers),\n",
    "        lower values indicate rare postal areas (rural/remote).\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler (no log transform).</strong> Despite a skewness of 1.6 exceeding our threshold, the distribution is intentionally <strong>multimodal</strong>: high values reflect genuinely dense urban FSAs, while low values represent rural/remote areas. The skewness results from this natural urban/rural split, not from a problematic long tail.\n",
    "        A log transform would compress the upper range and reduce the meaningful separation this feature is designed to carry.\n",
    "        <br><br>\n",
    "        We also do not treat values around 1200 as data errors, they represent legitimate high-density FSAs. However, because our clustering methods are distance-based (and K-Means optimizes squared Euclidean distances),\n",
    "        scaling is still required to prevent raw magnitude from dominating the feature space.\n",
    "        <br><br>\n",
    "        We therefore prefer StandardScaler here: it normalizes the feature using mean and standard deviation, which (given legitimate high urban values) yields a balanced contribution of this density proxy across the overall distance computation.\n",
    "        RobustScaler is robust in how it estimates scale (median/IQR), but it remains a linear transform and does not compress extreme, but valid values meaning very high urban FSAs can still become disproportionately influential in distance based clustering.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9777e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Province: Frequency encoding (number of customers in that province)\n",
    "province_counts = df_Customer[\"Province or State\"].value_counts()\n",
    "df_Customer[\"Province_Encoded\"] = df_Customer[\"Province or State\"].map(province_counts).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe119b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, 'Province_Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06aae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['Province_Encoded'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55082030",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Province_Encoded</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Province has multiple categories but no natural ordering. One hot encoding would create many sparse features. Frequency encoding captures population distribution: customers from populous provinces get higher values, customers from smaller provinces get lower values.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Each province is replaced with the count of customers in that province. Range: 0-5000. Distribution shows a clear gap between small provinces (0-1000, about 22% of customers) and large provinces (2700-5000, about 78% of customers), reflecting the actual population concentration in a few major Canadian provinces.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler (no log transform).</strong> Skewness of -0.78 is below our threshold of 1.0. The left-skewed multimodal distribution reflects real geographic concentration in Canada, with most customers in a few large provinces. Log transformation would compress this meaningful distinction between small and large provinces.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30adbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# City: Frequency encoding (number of customers in that city)\n",
    "city_counts = df_Customer[\"City\"].value_counts()\n",
    "df_Customer[\"City_Encoded\"] = df_Customer[\"City\"].map(city_counts).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bb82f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, 'City_Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff328f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['City_Encoded'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae35489",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">City_Encoded</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        City has many unique values making one hot encoding impractical. Frequency encoding captures city size: customers from large cities get higher values, customers from small towns get lower values. This provides a granular urban/rural signal between Province and FSA level.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Each city is replaced with the count of customers in that city. Range: 0-3000. Distribution shows two clusters: smaller cities (0-500, about 52% of customers) and larger cities (1500-3000, about 48% of customers), with a gap in between reflecting the typical urban hierarchy.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler (no log transform).</strong> Skewness of 0.34 is well below our threshold of 1.0, indicating a balanced distribution. The bimodal pattern reflects natural city size variation in Canada. No transformation needed.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6655a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender: Binary encode (male = 1, female = 0)\n",
    "df_Customer[\"Gender_Encoded\"] = df_Customer[\"Gender\"].map({\"male\": 1, \"female\": 0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_distribution(df_Customer, 'Gender_Encoded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e7322",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Gender_Encoded</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal and Implementation:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Gender has only 2 categories (Male/Female) with no ordinal relationship. Binary encoding (Male=1, Female=0) is the most efficient representation, avoiding unnecessary one hot expansion while maintaining interpretability. Distribution is nearly balanced (50.1% Female, 49.9% Male).\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler.</strong> Binary values (0, 1) with balanced distribution.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1715fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location Code: Map to ordered values (Rural = 0, Suburban = 1, Urban = 2)\n",
    "location_mapping = {\"Rural\": 0, \"Suburban\": 1, \"Urban\": 2}\n",
    "df_Customer[\"Location_Code_Num\"] = df_Customer[\"Location Code\"].map(location_mapping).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical_distribution(df_Customer, 'Location_Code_Num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f112b6bd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Location_Code_Num</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Location Code (Rural/Suburban/Urban) has a natural ordered progression from low to high population density. Ordinal encoding preserves this meaningful ranking, which helps clustering algorithms detect urbanization patterns that correlate with travel behavior.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Ordinal numeric encoding (0, 1, 2) representing population density:\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Rural (0):</strong> 33.3%</li>\n",
    "        <li><strong>Suburban (1):</strong> 33.9%</li>\n",
    "        <li><strong>Urban (2):</strong> 32.7%</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler.</strong> Discrete ordinal values (0, 1, 2) with balanced distribution.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a79653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marital Status: One-hot encode\n",
    "marital_cols = [col for col in df_Customer.columns if col.startswith('Marital_')]\n",
    "if not marital_cols:\n",
    "    marital_dummies = pd.get_dummies(df_Customer[\"Marital Status\"], prefix=\"Marital\", drop_first=False)\n",
    "    df_Customer = pd.concat([df_Customer, marital_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Customer['Marital Status'].value_counts(normalize=True).mul(100).round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4ec4d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Marital Status (One-Hot Encoded)</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Marital Status (Divorced/Married/Single) has no natural ordering. Being divorced is not \"between\" single and married. One-hot encoding treats all categories as equally distinct, preventing false ordinal assumptions. The feature is important for family-oriented segmentation (e.g., married customers may travel with companions more frequently).\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        One-hot encoding creates three binary columns (Marital_Divorced, Marital_Married, Marital_Single). In Section 4 (Feature Selection), we drop Marital_Single as it is fully determined by the other two columns (if Divorced=0 and Married=0, then Single=1). This avoids multicollinearity while retaining full information.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>No scaling.</strong> One-hot encoded features are already binary (0/1) and should not be scaled. Scaling would distort the categorical interpretation and provide no benefit since the values are already bounded.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa107b",
   "metadata": {},
   "source": [
    "## **3.4 Behavorial based Feature Enginnering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef8df7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Behavioral Features Overview</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        We engineer <strong>behavioral features</strong> to capture how customers interact with the airline - their travel patterns, temporal preferences, and loyalty program engagement. These features answer the question: <strong>\"How does this customer fly?\"</strong>\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Features:</h4>\n",
    "    <p style=\"margin: 10px 40px 10px 20px; color: #000;\">\n",
    "        Behavioral features enable <strong>actionable customer segmentation</strong> based on actual travel behavior rather than demographics alone. By understanding how customers fly (distance, regularity, companions), when they fly (seasonality, peak season), and how they engage with the loyalty program (redemption patterns), we can create segments like \"Business Commuters\", \"Seasonal Vacationers\", or \"Points Hoarders\" - each requiring different marketing strategies, service offerings, and retention approaches.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Categories:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>Travel Pattern (4 features):</strong></p>\n",
    "    <ul style=\"margin: 5px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>avg_distance_per_flight:</strong> Short-haul vs long-haul traveler (km)</li>\n",
    "        <li><strong>distance_variability:</strong> Consistent routes vs diverse destinations (CV)</li>\n",
    "        <li><strong>companion_flight_ratio:</strong> Solo vs group traveler (0-1)</li>\n",
    "        <li><strong>flight_regularity:</strong> Routine vs sporadic traveler (0-1)</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>Temporal Pattern (3 features):</strong></p>\n",
    "    <ul style=\"margin: 5px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>seasonal_concentration:</strong> Year-round vs seasonal traveler (Gini 0-1)</li>\n",
    "        <li><strong>peak_season_sin/cos:</strong> Which season is peak (cyclical encoding -1 to +1)</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>Loyalty Engagement (2 features):</strong></p>\n",
    "    <ul style=\"margin: 5px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>redemption_rate:</strong> How much of earned points redeemed (0-1)</li>\n",
    "        <li><strong>redemption_frequency:</strong> How often points are redeemed (0-1)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Strategy:</h4>\n",
    "    <table style=\"margin: 10px 0; border-collapse: collapse; width: 90%;\">\n",
    "        <tr style=\"background-color: #00411E; color: white;\">\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Feature</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Transformation</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Scaling</th>\n",
    "        </tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">avg_distance_per_flight</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Winsorized (0.25%/0.5%)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">StandardScaler</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">distance_variability</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">None</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">StandardScaler</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">companion_flight_ratio</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">None</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">StandardScaler</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">flight_regularity</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">None</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">StandardScaler</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">seasonal_concentration</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">None</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">StandardScaler</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">peak_season_sin/cos</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Cyclical encoding</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">None</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">redemption_rate</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Capped at 1.0</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">StandardScaler</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">redemption_frequency</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">None</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">StandardScaler</td></tr>\n",
    "    </table>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d369409",
   "metadata": {},
   "source": [
    "### Group 1: Travel Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travel Pattern Feature: Average distance per flight\n",
    "# Aggregates all flights per customer, then calculates avg distance\n",
    "\n",
    "flight_totals = (\n",
    "    df_Flights\n",
    "    .groupby(\"Loyalty#\", as_index=False)\n",
    "    .agg(\n",
    "        total_distance=(\"DistanceKM\", \"sum\"),\n",
    "        total_flights=(\"NumFlights\", \"sum\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate average distance per flight (avoid division by zero)\n",
    "flight_totals[\"avg_distance_per_flight\"] = (\n",
    "    flight_totals[\"total_distance\"] / flight_totals[\"total_flights\"].replace(0, np.nan)\n",
    ").fillna(0)\n",
    "\n",
    "# Merge to customer dataframe\n",
    "df_Customer = df_Customer.merge(\n",
    "    flight_totals[[\"Loyalty#\", \"avg_distance_per_flight\"]], \n",
    "    on=\"Loyalty#\", \n",
    "    how=\"left\"\n",
    ")\n",
    "df_Customer[\"avg_distance_per_flight\"] = df_Customer[\"avg_distance_per_flight\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962e9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"avg_distance_per_flight\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Summary\n",
    "avg_distance = analyze_outliers(\n",
    "    df_Customer, \n",
    "    features=[\"avg_distance_per_flight\"],\n",
    "    caption='avg_distance_per_flight Outlier Summary (IQR Method)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec17fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize avg_distance_per_flight to cap extreme outliers\n",
    "# Original IQR analysis: 2.64% outliers (300 upper above 2969 km, 58 lower below 860 km, max 13,852 km)\n",
    "# \n",
    "# Why 0.25% lower, 0.5% upper (conservative thresholds)?\n",
    "# - Lower tail: Few outliers (58), values around 860 km are plausible short-haul averages\n",
    "# - Upper tail: More extreme values (max 13,852 km approaches Earth's max flight distance)\n",
    "# - After plotting: 0.5% upper caps at reasonable upper bound for average flight distance (allows long-haul but removes implausible extremes)\n",
    "# - Higher percentiles (e.g., 1%) would remove too many long-haul travelers\n",
    "\n",
    "df_Customer['avg_distance_per_flight'] = mstats.winsorize(\n",
    "    df_Customer['avg_distance_per_flight'], \n",
    "    limits=[0.0025, 0.005]  # 0.25% lower, 0.5% upper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e8d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"avg_distance_per_flight\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f24d0b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 1: avg_distance_per_flight</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Average flight distance is a primary discriminator between business and leisure travel patterns. Short-haul frequent flyers (500-1500 km) typically represent business commuters on regional routes, while long-haul travelers (4000+ km) indicate leisure or international business travel. This feature captures fundamental differences in trip purpose, route preferences, and service expectations.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Creates clear separation between customer segments with different travel styles. Short-haul customers require different marketing strategies (frequent flyer bonuses, lounge access) compared to long-haul customers (premium upgrades, international partnerships). Distance patterns correlate with value, loyalty tier progression, and lifetime value trajectories.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Continuous ratio feature calculated as total kilometers flown divided by total number of flights over 2019-2021 period.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Outlier Handling & Scaling:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Original distribution showed right skewness with extreme outliers extending to 13,852 km (approaching Earth's maximum flight distance). IQR analysis identified 2.8% outliers (383 total), predominantly in the upper tail (330 customers above 2,984 km, 53 below 833 km).\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Winsorizing applied</strong> instead of log transformation to preserve km interpretation. Conservative thresholds (0.25% lower, 0.5% upper) cap values at roughly 3,500 km - a reasonable upper bound that retains legitimate long-haul travelers while removing physically implausible averages. Log transformation was considered but rejected as the main distribution (1000-3000 km) is already well-behaved; only the extreme tail needed treatment.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>StandardScaler</strong> applied after winsorizing. The capped distribution is compact and suitable for mean/std scaling.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a70d1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travel Pattern Feature: distance_variability\n",
    "# Coefficient of variation (CV) measures how consistent a customer's flight distances are\n",
    "\n",
    "def calculate_cv(values):\n",
    "    \"\"\"Calculate coefficient of variation for a list of values.\"\"\"\n",
    "    if len(values) < 2:\n",
    "        return 0\n",
    "    mean_val = np.mean(values)\n",
    "    return np.std(values) / mean_val if mean_val > 0 else 0\n",
    "\n",
    "# Filter months with flights and calculate avg distance per flight per month\n",
    "flights_with_activity = df_Flights[df_Flights['NumFlights'] > 0].copy()\n",
    "flights_with_activity['avg_dist_month'] = flights_with_activity['DistanceKM'] / flights_with_activity['NumFlights']\n",
    "\n",
    "# Calculate CV of monthly avg distances per customer\n",
    "distance_variability = (\n",
    "    flights_with_activity.groupby('Loyalty#')['avg_dist_month']\n",
    "    .apply(list)\n",
    "    .apply(calculate_cv)\n",
    "    .rename('distance_variability')\n",
    ")\n",
    "\n",
    "# Merge into df_Customer\n",
    "df_Customer = df_Customer.merge(distance_variability, on='Loyalty#', how='left')\n",
    "df_Customer['distance_variability'] = df_Customer['distance_variability'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"distance_variability\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c484a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['distance_variability'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Summary\n",
    "distance_variability_outliers = analyze_outliers(\n",
    "    df_Customer, \n",
    "    features=[\"distance_variability\"],\n",
    "    caption='distance_variability Outlier Summary (IQR Method)'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92c841",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 2: distance_variability</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Route consistency reveals travel purpose and planning behavior. Low variability indicates customers who repeatedly fly similar distances (business commuters, regular visitors), while high variability suggests diverse travel patterns (leisure explorers, varied business destinations). This consistency dimension is orthogonal to average distance.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Separates \"routine travelers\" from \"explorers\" within the same distance category. A customer with avg_distance=2000km could be flying Toronto-Vancouver repeatedly (low CV ~0.3) or mixing Toronto-New York, Toronto-Mexico, Toronto-Calgary trips (high CV ~1.2). These patterns require different marketing approaches and predict different ancillary revenue opportunities.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Coefficient of variation (CV = standard deviation / mean) calculated across monthly average flight distances per customer. CV is a normalized ratio where 0 = identical distances every month, 1 = standard deviation equals mean.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Skewness, Transformation & Scaling:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Skewness of -0.32 is well below threshold, indicating a roughly symmetric distribution centered around 0.85. <strong>No log transformation needed.</strong> IQR analysis identifies only 1.82% outliers (246 total): 174 lower outliers (CV near 0, very consistent travelers) and 72 upper outliers. The lower outliers represent customers with minimal route variation - legitimate behavioral patterns we want to retain, not data errors. <strong>StandardScaler</strong> applied without winsorizing or RobustScaler.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9607e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travel Pattern Feature: companion_flight_ratio\n",
    "# Ratio of flights with companions vs total flights (business alone vs leisure with family/friends)\n",
    "\n",
    "# Filter months with flights\n",
    "flights_with_activity = df_Flights[df_Flights['NumFlights'] > 0]\n",
    "\n",
    "# Aggregate companion flights per customer\n",
    "customer_companions = flights_with_activity.groupby('Loyalty#').agg({\n",
    "    'NumFlightsWithCompanions': 'sum',\n",
    "    'NumFlights': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate ratio\n",
    "customer_companions['companion_flight_ratio'] = (\n",
    "    customer_companions['NumFlightsWithCompanions'] / \n",
    "    customer_companions['NumFlights']\n",
    ")\n",
    "\n",
    "# Merge into df_Customer\n",
    "df_Customer = df_Customer.merge(\n",
    "    customer_companions[['Loyalty#', 'companion_flight_ratio']], \n",
    "    on='Loyalty#', \n",
    "    how='left'\n",
    ")\n",
    "df_Customer['companion_flight_ratio'] = df_Customer['companion_flight_ratio'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a0ce42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"companion_flight_ratio\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['companion_flight_ratio'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135791d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Summary\n",
    "companion_flight_ratio_outliers = analyze_outliers(\n",
    "    df_Customer, \n",
    "    features=[\"companion_flight_ratio\"],\n",
    "    caption='Companion Flight Ratio Outlier Summary (IQR Method)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e86e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check customers with companion_flight_ratio >= 0.9 (almost always fly with companions)\n",
    "# After examining the outliers we found these are mostly inactive customers who flew in 2020 but not in 2021. Since our clustering approach focuses only on active customers anyway, we check what proportion of these extreme values are inactive.\n",
    "\n",
    "companion_always = df_Customer[df_Customer['companion_flight_ratio'] >= 0.9]\n",
    "companion_always_inactive = companion_always[~companion_always['is_active']]\n",
    "\n",
    "print(f'Customers with companion_flight_ratio >= 0.9: {len(companion_always)}')\n",
    "print(f'Of those, inactive: {len(companion_always_inactive)} ({len(companion_always_inactive)/len(companion_always)*100:.1f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafc90db",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 3: companion_flight_ratio</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Companion behavior is a strong proxy for trip purpose and customer lifecycle stage. Solo travelers predominantly represent business trips, while customers flying with companions indicate leisure travel, family vacations, or couple trips. This dimension directly impacts ancillary revenue (extra seats, baggage, meals) and appropriate service offerings.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Enables clear separation between business-focused solo travelers and leisure-oriented group travelers. High companion ratio customers respond to family packages, group discounts, and destination-based marketing, while low ratio customers require business amenities and flexible booking policies. Critical for tailoring service delivery and revenue optimization strategies.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Continuous ratio feature (0-1) calculated as proportion of flights taken with one or more companions out of total flights. Aggregated across all flights from 2019-2021.\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>0.0:</strong> Pure solo traveler - likely business or independent leisure</li>\n",
    "        <li><strong>0.3-0.6:</strong> Mixed traveler - combination of solo and group trips</li>\n",
    "        <li><strong>0.6-1.0:</strong> Predominantly group traveler - family/leisure focus</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Skewness, Transformation & Scaling:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Skewness of 0.80 is below our threshold (1.0), so <strong>no transformation required</strong>. Log transformation would be inappropriate anyway for bounded ratio features (0-1): it's undefined at 0 (where we have a concentration of solo travelers), and the \"right tail\" up to 1.0 is not a magnitude problem but simply customers who always fly with companions.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Outlier Analysis:</strong> IQR analysis detected 306 outliers (2.26%) with upper bound at 0.48 - 206 upper outliers and 100 lower outliers. Notably, all 12 customers with companion_flight_ratio >= 0.9 are inactive (100%), meaning these extreme values won't affect our clustering of active customers. The remaining outliers represent legitimate behavioral patterns valuable for clustering insights.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision:</strong> <strong>StandardScaler</strong> applied without transformation or winsorizing. The bounded nature (0-1) prevents extreme outliers, and retaining the original values preserves important behavioral distinctions between solo and group travelers that are critical for clustering.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86cfb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Travel Pattern Feature: flight_regularity\n",
    "# Measures how consistently a customer flies across months (from first flight onwards)\n",
    "# High regularity = business traveler with consistent schedule, Low = sporadic leisure traveler\n",
    "\n",
    "# Get monthly flight counts per customer\n",
    "monthly_flight_counts = df_Flights.groupby(['Loyalty#', 'YearMonthDate'])['NumFlights'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Sort columns chronologically\n",
    "monthly_flight_counts = monthly_flight_counts.reindex(sorted(monthly_flight_counts.columns), axis=1)\n",
    "\n",
    "# Find first flight month index per customer\n",
    "first_flight = df_Flights[df_Flights['NumFlights'] > 0].groupby('Loyalty#')['YearMonthDate'].min()\n",
    "\n",
    "# Calculate regularity: 1 / (1 + CV) where CV = std/mean, only for months >= first flight\n",
    "def calc_regularity(row):\n",
    "    loyalty_id = row.name\n",
    "    if loyalty_id not in first_flight.index:\n",
    "        return 0\n",
    "    first_date = first_flight[loyalty_id]\n",
    "    # Get column positions from first flight onwards\n",
    "    cols = monthly_flight_counts.columns.tolist()\n",
    "    if first_date not in cols:\n",
    "        return 0\n",
    "    start_idx = cols.index(first_date)\n",
    "    values = row.iloc[start_idx:]\n",
    "    mean_val = values.mean()\n",
    "    if mean_val < 0.01:\n",
    "        return 0\n",
    "    return 1 / (1 + values.std() / mean_val)\n",
    "\n",
    "flight_regularity = monthly_flight_counts.apply(calc_regularity, axis=1).rename('flight_regularity')\n",
    "\n",
    "# Merge into df_Customer\n",
    "df_Customer = df_Customer.merge(flight_regularity, on='Loyalty#', how='left')\n",
    "df_Customer['flight_regularity'] = df_Customer['flight_regularity'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39362781",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"flight_regularity\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a7a8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['flight_regularity'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca7959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Summary\n",
    "flight_regularity_outliers = analyze_outliers(\n",
    "    df_Customer,\n",
    "    features=[\"flight_regularity\"],\n",
    "    caption='Flight Regularity Outlier Summary (IQR Method)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ac5b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after examining the outliers we found these are mostly inactive customers who flew in 2020 but not in 2021. Since our clustering approach focuses only on active customers anyway, we check what proportion of these extreme values are inactive.\n",
    "\n",
    "# Check how many lower-bound outliers are inactive customers\n",
    "lower_bound_outliers = df_Customer[df_Customer['flight_regularity'] <= 0.37]\n",
    "inactive_outliers = lower_bound_outliers[~lower_bound_outliers['is_active']]\n",
    "\n",
    "print(f\"Total outliers below lower bound: {len(lower_bound_outliers)}\")\n",
    "print(f\"Inactive customers among outliers: {len(inactive_outliers)} ({len(inactive_outliers)/len(lower_bound_outliers)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf64bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 77.8% of lower-bound outliers (441 of 567) are inactive customers\n",
    "# Plotting only active customers to show the distribution relevant for clustering\n",
    "plot_numeric_distribution(df_Customer[df_Customer['is_active']], \"flight_regularity\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730b68a1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 4: flight_regularity</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Flight regularity measures <strong>month-to-month consistency</strong> of travel activity. While seasonal_concentration captures year-level patterns (4 seasons), this feature uses monthly data points to distinguish <strong>routine travelers</strong> (consistent monthly flights) from <strong>sporadic travelers</strong> (unpredictable bursts). Two customers with identical total flights can have completely different rhythms.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Separates predictable business commuters (fly every week/month) from event-driven travelers (project-based, vacation-only). This distinction is critical for campaign timing, service design, and capacity planning. High regularity customers respond to subscription offers and routine-based perks, while low regularity customers need flexible policies and triggered campaigns.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Continuous feature (0-1) calculated as the inverse of the coefficient of variation (CV) of monthly flights: <strong>regularity = 1 / (1 + CV)</strong>, where CV = std / mean. Only months from the customer's first flight onwards are considered to avoid penalizing late joiners.\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>0.8-1.0:</strong> Very regular - similar flights each month (business commuter)</li>\n",
    "        <li><strong>0.5-0.7:</strong> Moderate - some variation but consistent presence</li>\n",
    "        <li><strong>0.3-0.5:</strong> Irregular - noticeable gaps between activity periods</li>\n",
    "        <li><strong>0.0-0.3:</strong> Very irregular - sporadic bursts with long gaps</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Skewness, Outliers & Scaling:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Initial skewness of -1.39 indicated left-skew exceeding the threshold (1.0). IQR detected 677 outliers (5.0%), with 569 below the lower bound (0.37) and 108 above the upper bound (0.64).\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Root cause investigation:</strong> Analysis revealed that <strong>77.8% of lower-bound outliers (441 of 567) are inactive customers</strong>. These customers flew in 2020 but stopped flying in 2021, creating long stretches of zero-flight months in their observation window. The regularity formula (based on CV of monthly flights) naturally produces low scores when most months have zero flights with occasional activity bursts. This is not a data quality issue but rather the feature correctly identifying churned or dormant customers.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision:</strong> <strong>No transformation or capping applied.</strong> The left-skew disappears when filtering to active customers only, as shown in the distribution plot above. Since behavioral clustering in Section 8 focuses exclusively on active customers, these low-regularity inactive customers will be naturally excluded from the analysis. The remaining active customers show a well-behaved distribution suitable for <strong>StandardScaler</strong>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9071b2",
   "metadata": {},
   "source": [
    "### Group 2: Temporal Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fbe784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Pattern Feature: seasonal_concentration\n",
    "# Measures how concentrated a customer's flights are across seasons\n",
    "# Low = year-round traveler (business), High = seasonal traveler (leisure/vacation)\n",
    "\n",
    "# Add Season mapping (Winter: Dec, Jan, Feb | Spring: Mar, Apr, May | Summer: Jun, Jul, Aug | Fall: Sep, Oct, Nov)\n",
    "df_Flights['Season'] = df_Flights['Month'].map({\n",
    "    12: 'Winter', 1: 'Winter', 2: 'Winter',\n",
    "    3: 'Spring', 4: 'Spring', 5: 'Spring',\n",
    "    6: 'Summer', 7: 'Summer', 8: 'Summer',\n",
    "    9: 'Fall', 10: 'Fall', 11: 'Fall'\n",
    "})\n",
    "\n",
    "# Aggregate flights per customer per season (across all years)\n",
    "seasonal_flights = df_Flights.groupby(['Loyalty#', 'Season'])['NumFlights'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Ensure all seasons exist\n",
    "for season in ['Winter', 'Spring', 'Summer', 'Fall']:\n",
    "    if season not in seasonal_flights.columns:\n",
    "        seasonal_flights[season] = 0\n",
    "\n",
    "# Calculate Gini coefficient for seasonal concentration\n",
    "def calculate_gini(values):\n",
    "    values = np.sort(values)\n",
    "    n = len(values)\n",
    "    total = values.sum()\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    cumsum = np.cumsum(values)\n",
    "    return (n + 1 - 2 * cumsum.sum() / total) / n\n",
    "\n",
    "seasonal_concentration = seasonal_flights.apply(lambda row: calculate_gini(row.values), axis=1).rename('seasonal_concentration')\n",
    "\n",
    "# Merge into df_Customer\n",
    "df_Customer = df_Customer.merge(seasonal_concentration, on='Loyalty#', how='left')\n",
    "df_Customer['seasonal_concentration'] = df_Customer['seasonal_concentration'].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ae09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"seasonal_concentration\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e8d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['seasonal_concentration'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4a784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check customers with seasonal_concentration >= 0.7 (highly concentrated seasonal flying)\n",
    "# After examining the outliers we found these are mostly inactive customers who flew in 2020 but not in 2021. Since our clustering approach focuses only on active customers anyway, we check what proportion of these extreme values are inactive.\n",
    "\n",
    "seasonal_extreme = df_Customer[df_Customer['seasonal_concentration'] >= 0.7]\n",
    "seasonal_extreme_inactive = seasonal_extreme[~seasonal_extreme['is_active']]\n",
    "\n",
    "print(f'Customers with seasonal_concentration >= 0.7: {len(seasonal_extreme)}')\n",
    "print(f'Of those, inactive: {len(seasonal_extreme_inactive)} ({len(seasonal_extreme_inactive)/len(seasonal_extreme)*100:.1f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a46819a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 5: seasonal_concentration</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Seasonal concentration measures temporal distribution of travel activity throughout the year. Year-round travelers (low concentration) typically represent business customers with consistent travel needs, while highly seasonal travelers (high concentration) indicate vacation-focused leisure customers who concentrate trips in specific seasons. This pattern fundamentally affects campaign timing, inventory allocation, and revenue predictability.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Enables strategic differentiation in marketing timing and resource allocation. Low concentration customers receive continuous engagement throughout the year, while high concentration customers require focused pre-season campaigns (e.g., promoting summer destinations in early spring). This feature also predicts revenue volatility and helps identify customers suitable for off-season promotional targeting to smooth demand.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Continuous feature (0-1) calculated using Gini coefficient across seasonal flight totals (Winter: Dec-Feb, Spring: Mar-May, Summer: Jun-Aug, Fall: Sep-Nov). Gini measures inequality in distribution - perfect equality (flights evenly distributed across seasons) yields 0, while maximum inequality (all flights in one season) approaches 1.\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>Low (0-0.3):</strong> Year-round traveler - consistent across seasons (business pattern)</li>\n",
    "        <li><strong>Medium (0.3-0.6):</strong> Moderate seasonality - some preference but active year-round</li>\n",
    "        <li><strong>High (0.6-1.0):</strong> Highly seasonal - concentrated in 1-2 seasons (vacation pattern)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Skewness, Transformation & Scaling:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Skewness of 1.48 is above our threshold (1.0), indicating right-skew from customers with highly concentrated seasonal patterns. However, <strong>no log transformation applied</strong> because the Gini coefficient is bounded (0-1) and log is undefined at 0. The right tail represents legitimate behavioral patterns of vacation-focused travelers.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Outlier Analysis:</strong> Of 121 customers with seasonal_concentration >= 0.7, 106 (88%) are inactive. These extreme values predominantly represent customers who flew only in 2019/2020 but not in 2021, naturally resulting in concentrated seasonal patterns. Since our clustering focuses on active customers, most extreme values are automatically excluded.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision:</strong> <strong>StandardScaler</strong> applied without transformation. The bounded nature (0-1) limits outlier impact, and the remaining active customers with high concentration represent genuine vacation-focused travelers valuable for cluster differentiation.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f76c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal Pattern Feature: peak_season_sin & peak_season_cos\n",
    "# Identifies WHICH season has the customer's peak travel activity\n",
    "# Cyclical encoding ensures Winter and Fall are mathematically close (adjacent seasons)\n",
    "\n",
    "# Identify peak season for each customer (reuses seasonal_flights from above)\n",
    "peak_season = seasonal_flights.idxmax(axis=1)\n",
    "\n",
    "# Map seasons to numeric values for cyclical encoding (0-3, following calendar cycle)\n",
    "season_to_numeric = {\n",
    "    'Winter': 0,   # Dec, Jan, Feb\n",
    "    'Spring': 1,   # Mar, Apr, May\n",
    "    'Summer': 2,   # Jun, Jul, Aug\n",
    "    'Fall': 3      # Sep, Oct, Nov\n",
    "}\n",
    "peak_season_numeric = peak_season.map(season_to_numeric)\n",
    "\n",
    "# Cyclical encoding: angle = 2π * season / 4 (each season = 90° on unit circle)\n",
    "peak_season_sin = np.sin(2 * np.pi * peak_season_numeric / 4).rename('peak_season_sin')\n",
    "peak_season_cos = np.cos(2 * np.pi * peak_season_numeric / 4).rename('peak_season_cos')\n",
    "\n",
    "# Merge into df_Customer\n",
    "df_Customer = df_Customer.merge(peak_season_sin, on='Loyalty#', how='left')\n",
    "df_Customer = df_Customer.merge(peak_season_cos, on='Loyalty#', how='left')\n",
    "df_Customer['peak_season_sin'] = df_Customer['peak_season_sin'].fillna(0)\n",
    "df_Customer['peak_season_cos'] = df_Customer['peak_season_cos'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c5b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peak Season Distribution\n",
    "df_Customer['peak_season'] = df_Customer['Loyalty#'].map(peak_season)\n",
    "plot_categorical_distribution(df_Customer, 'peak_season')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22850253",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 6 & 7: peak_season_sin & peak_season_cos</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        While <strong>seasonal_concentration</strong> measures <strong>how concentrated</strong> travel is across seasons, it doesn't reveal <strong>when</strong> customers prefer to fly. Two customers with identical concentration (e.g., 0.75) could be a Winter ski traveler vs. a Summer beach traveler - requiring completely different marketing campaigns at different times.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Enables precise campaign timing based on individual peak season. Winter-peak customers receive ski resort promotions in September-November, while Summer-peak customers receive beach destination promotions in March-May. Combined with <strong>seasonal_concentration</strong>, this creates actionable segments for targeted marketing with optimal timing.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Two continuous features (-1 to +1) representing the customer's peak travel season using cyclical sine/cosine encoding. The peak season is determined by which season (Winter, Spring, Summer, Fall) has the highest flight count, then encoded as a position on the unit circle.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Formula:</strong> angle = 2π × season_numeric / 4 (each season = 90° on unit circle)\n",
    "    </p>\n",
    "    <table style=\"margin: 10px 0; border-collapse: collapse; width: 80%;\">\n",
    "        <tr style=\"background-color: #00411E; color: white;\">\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Season</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">Angle</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">sin</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00411E;\">cos</th>\n",
    "        </tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Winter (Dec-Feb)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">0°</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">0.00</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">+1.00</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Spring (Mar-May)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">90°</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">+1.00</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">0.00</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Summer (Jun-Aug)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">180°</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">0.00</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">-1.00</td></tr>\n",
    "        <tr><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">Fall (Sep-Nov)</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">270°</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">-1.00</td><td style=\"padding: 8px; border: 1px solid #ccc; color: #000;\">0.00</td></tr>\n",
    "    </table>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Distribution:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Summer dominates</strong> with 51.8% of customers having their peak travel in summer months - typical for leisure-focused airlines. The remaining customers distribute across Spring (18.8%), Fall (17.4%), and Winter (12.0%), representing business travelers, ski enthusiasts, and holiday-season travelers respectively.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Why Cyclical Encoding (Not Ordinal or One-Hot)?</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Ordinal encoding (0,1,2,3)</strong> incorrectly treats Winter-Fall as maximally distant (|0-3|=3) when they are adjacent seasons. <strong>One-Hot encoding</strong> requires 4 features and treats all seasons as equally different, losing the adjacent/opposite relationship. <strong>Sin/Cos encoding</strong> preserves cyclical nature: adjacent seasons (Winter-Fall, Winter-Spring) have distance 1.41, while opposite seasons (Winter-Summer) have maximum distance 2.0.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Scaling Decision:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>No scaling applied.</strong> Sin/cos values are already bounded between -1 and +1, which is comparable to StandardScaler output. Additionally, scaling would distort the unit circle geometry that makes this encoding mathematically correct for distance-based clustering. No outliers possible due to bounded range.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4645013",
   "metadata": {},
   "source": [
    "### Group 3: Loyalty Engagement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe34fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loyalty Engagement Feature: redemption_rate\n",
    "# Measures what percentage of accumulated points has been redeemed\n",
    "# Low = hoarder or disengaged, High = active program user\n",
    "\n",
    "# Calculate total points accumulated and redeemed per customer (across all months)\n",
    "points_agg = df_Flights.groupby('Loyalty#').agg({\n",
    "    'PointsAccumulated': 'sum',\n",
    "    'PointsRedeemed': 'sum'\n",
    "}).rename(columns={\n",
    "    'PointsAccumulated': 'total_points_accumulated',\n",
    "    'PointsRedeemed': 'total_points_redeemed'\n",
    "})\n",
    "\n",
    "# Calculate redemption rate (handle division by zero for customers with no accumulation)\n",
    "points_agg['redemption_rate'] = np.where(\n",
    "    points_agg['total_points_accumulated'] > 0,\n",
    "    points_agg['total_points_redeemed'] / points_agg['total_points_accumulated'],\n",
    "    0\n",
    ")\n",
    "\n",
    "# Merge into df_Customer\n",
    "df_Customer = df_Customer.merge(points_agg[['redemption_rate']], on='Loyalty#', how='left')\n",
    "df_Customer['redemption_rate'] = df_Customer['redemption_rate'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"redemption_rate\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ac657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Summary\n",
    "redemption_outliers = analyze_outliers(\n",
    "    df_Customer, \n",
    "    features=[\"redemption_rate\"],\n",
    "    caption='Redemption Rate Outlier Summary (IQR Method)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0cfab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most Customers with redemption_rate > 1 all enrolled before 2019\n",
    "over_1 = df_Customer[df_Customer['redemption_rate'] > 1]\n",
    "print(f\"Customers with redemption_rate > 1: {len(over_1)}\")\n",
    "print(f\"Enrollment date range: {over_1['EnrollmentDateOpening'].min()} to {over_1['EnrollmentDateOpening'].max()}\")\n",
    "\n",
    "# Breakdown by enrollment year\n",
    "enrolled_before_2019 = over_1[over_1['EnrollmentDateOpening'] < pd.to_datetime('2019-01-01')]\n",
    "enrolled_2019_or_later = over_1[over_1['EnrollmentDateOpening'] >= pd.to_datetime('2019-01-01')]\n",
    "print(f\"\\nEnrolled before 2019: {len(enrolled_before_2019)} ({len(enrolled_before_2019)/len(over_1)*100:.1f}%)\")\n",
    "print(f\"Enrolled 2019 or later: {len(enrolled_2019_or_later)} ({len(enrolled_2019_or_later)/len(over_1)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea34f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is valid behavior, not a data error - cap at 1.0 to normalize the ratio\n",
    "df_Customer['redemption_rate'] = df_Customer['redemption_rate'].clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8cf4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot final distribution after capping\n",
    "plot_numeric_distribution(df_Customer, \"redemption_rate\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a94214d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 8: redemption_rate</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Redemption rate measures overall loyalty program engagement as the proportion of accumulated points that have been redeemed. This is a fundamental behavioral indicator: active redeemers (high rate) demonstrate program understanding and value perception, while hoarders (low rate) either save for aspirational rewards or lack engagement with redemption options. The rate directly correlates with program satisfaction and stickiness.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Separates engaged users from passive accumulators. High redemption rate customers are active program participants requiring diverse redemption options and frequent reward refreshes. Low rate customers may need education on redemption value, special promotions to trigger engagement, or warnings about point expiration. From EDA, short-haul frequent flyers often show low redemption (hoarding for big rewards), making this feature critical for understanding strategic vs opportunistic program usage.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Continuous ratio feature (0-1) calculated as total points redeemed divided by total points accumulated over 2020-2021. Represents overall utilization of earned loyalty currency.\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>0.0:</strong> Never redeemed - either saving strategically or disengaged from program</li>\n",
    "        <li><strong>0.0-0.2:</strong> Hoarder - accumulates without redeeming (saving or disengaged)</li>\n",
    "        <li><strong>0.2-0.6:</strong> Moderate user - balanced accumulation and redemption</li>\n",
    "        <li><strong>0.6-1.0:</strong> Active redeemer - regularly uses points (engaged, values program)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Data Quality and Capping:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Values > 1:</strong> 158 customers showed redemption_rate > 1. Of these, 143 (90.5%) enrolled before 2019 and accumulated points before our data window, while 15 (9.5%) enrolled in 2019 or later. This is valid behavior: they redeemed more points than they accumulated within 2020-2021 because they had prior balances. These are <strong>capped at 1.0</strong> to normalize the ratio while retaining these engaged customers.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Values = 0:</strong> Customers with zero redemption are valid - they simply never redeemed any points during 2020-2021 (strategic savers or disengaged). No capping needed at lower bound.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Skewness, Outliers & Scaling:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        After capping, distribution shows right-skew with concentration at 0 (non-redeemers) and spread across 0.1-1.0 (active users).\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision:</strong> <strong>StandardScaler</strong> applied after capping. The bounded nature (0-1 after capping) limits extreme outliers, and the concentration at zero represents meaningful behavioral distinction (non-redeemers) rather than problematic outliers.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22716b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loyalty Engagement Feature: redemption_frequency\n",
    "# Measures how often a customer redeems points relative to their time in the program\n",
    "# Calculated from first points accumulation (not enrollment) to end of data window\n",
    "\n",
    "# Find first month with points accumulated per customer\n",
    "first_accumulation = df_Flights[df_Flights['PointsAccumulated'] > 0].groupby('Loyalty#').agg({\n",
    "    'Year': 'min',\n",
    "    'Month': 'first'  # Gets month of the min year row\n",
    "})\n",
    "\n",
    "# Better approach: get the actual first accumulation date\n",
    "first_acc_data = df_Flights[df_Flights['PointsAccumulated'] > 0].sort_values('YearMonthDate').groupby('Loyalty#').first()[['Year', 'Month']]\n",
    "\n",
    "# Calculate months available for redemption (from first accumulation to Dec 2021)\n",
    "# End date: December 2021 = Year 2021, Month 12\n",
    "first_acc_data['months_available'] = (2021 - first_acc_data['Year']) * 12 + (12 - first_acc_data['Month']) + 1\n",
    "\n",
    "# Count months with any redemption activity per customer\n",
    "monthly_redemptions = df_Flights.groupby(['Loyalty#', 'Year', 'Month'])['PointsRedeemed'].sum()\n",
    "months_with_redemptions = (monthly_redemptions > 0).groupby('Loyalty#').sum().rename('months_redeemed')\n",
    "\n",
    "# Calculate frequency (proportion of available months with redemptions)\n",
    "redemption_freq = first_acc_data[['months_available']].merge(months_with_redemptions, on='Loyalty#', how='left')\n",
    "redemption_freq['months_redeemed'] = redemption_freq['months_redeemed'].fillna(0)\n",
    "redemption_freq['redemption_frequency'] = redemption_freq['months_redeemed'] / redemption_freq['months_available']\n",
    "redemption_freq['redemption_frequency'] = redemption_freq['redemption_frequency'].clip(0, 1)\n",
    "\n",
    "# Merge into df_Customer\n",
    "df_Customer = df_Customer.merge(redemption_freq[['redemption_frequency']], on='Loyalty#', how='left')\n",
    "df_Customer['redemption_frequency'] = df_Customer['redemption_frequency'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_numeric_distribution(df_Customer, \"redemption_frequency\", show_pct_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cd3bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check skewness\n",
    "analyze_skewness(df_Customer, features=['redemption_frequency'], skew_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1271badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Summary\n",
    "redemption_frequency_outliers = analyze_outliers(\n",
    "    df_Customer, \n",
    "    features=[\"redemption_frequency\"],\n",
    "    caption='Redemption Frequency Outlier Summary (IQR Method)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdaa778",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature 9: redemption_frequency</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of the Feature (Why):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        While redemption_rate measures <strong>how much</strong> is redeemed (total percentage), redemption_frequency captures <strong>how often</strong> redemptions occur. This temporal dimension distinguishes \"cash-like users\" who redeem small amounts frequently from \"big savers\" who redeem large amounts rarely. Frequency indicates redemption strategy: continuous small redemptions suggest opportunistic point usage for upgrades and minor rewards, while infrequent redemptions indicate strategic saving for major rewards.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Clustering benefit:</strong> Provides orthogonal information to redemption_rate, enabling four distinct behavioral quadrants. High rate + high frequency = active continuous users. High rate + low frequency = strategic savers. Low rate + low frequency = disengaged hoarders. Marketing strategies differ dramatically: frequent redeemers need diverse small-reward options, while infrequent redeemers need aspirational big-ticket promotions.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Definition and Implementation (What and How):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Continuous ratio feature (0-1) calculated as the proportion of available months (from first points accumulation to Dec 2021) where customer had any redemption activity. This normalization ensures fair comparison between customers who joined at different times.\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li><strong>0.0:</strong> Never redeemed - either strategic saver or disengaged</li>\n",
    "        <li><strong>0.0-0.1:</strong> Very infrequent - redeems once or twice per year</li>\n",
    "        <li><strong>0.1-0.2:</strong> Occasional redeemer - redeems every few months</li>\n",
    "        <li><strong>0.2+:</strong> Frequent redeemer - redeems regularly (cash-like usage)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Skewness, Outliers & Scaling:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Skewness of 0.7 is below threshold (1.0), so <strong>no transformation required</strong>. Distribution shows concentration at 0 (non-redeemers) with decreasing frequency toward higher values, max at 0.43.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Outlier Analysis:</strong> IQR detected 75 outliers (0.55%) in upper tail above 0.25. These represent customers who redeem very frequently - legitimate behavioral pattern valuable for clustering.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision:</strong> <strong>StandardScaler</strong> applied without transformation. The bounded nature (0-1) and moderate skewness make standard scaling appropriate. Outliers retained as they represent genuine frequent redeemers.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e67c0",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='4'> </a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>4. Feature Selection</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce59375",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">To be added: Goal and Reference to next step, and also very short outline (Relevance/Redundancy and why only active?) </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fbdc61",
   "metadata": {},
   "source": [
    "## **4.1 Select Value based Features**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f748d9a",
   "metadata": {},
   "source": [
    "### Focus Group 1: Loyalty Members | Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfad3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select FM features for value based clustering\n",
    "fm_features = df_Customer[['Frequency', 'Monetary']]\n",
    "\n",
    "# Filter to Focus Group 1: Loyalty Members | Active\n",
    "fm_features_l_a = fm_features[df_Customer['Focus_Group'] == 1]\n",
    "\n",
    "fm_features_l_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2082065",
   "metadata": {},
   "source": [
    "### Focus Group 2: Ex Loyalty Members | Active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edade63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Focus Group 2: Ex Loyalty Members | Active\n",
    "fm_features_non_l_a = fm_features[df_Customer['Focus_Group'] == 2]\n",
    "\n",
    "fm_features_non_l_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b686c4",
   "metadata": {},
   "source": [
    "### Combined Focus Groups: All Active Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7024165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to all active customers (Focus Groups 1 & 2 combined)\n",
    "fm_features_a = fm_features[df_Customer['Is_Focus_Group'] == True]\n",
    "\n",
    "fm_features_a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d81f185",
   "metadata": {},
   "source": [
    "## **4.2 Select Demographic Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc82e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select demographic features\n",
    "demographic_features = df_Customer[['Province_Encoded', 'City_Encoded', 'FSA_Encoded', 'Gender_Encoded', 'Education_Level_Num', 'Location_Code_Num', 'Income_Bin_Num', 'Marital_Divorced', 'Marital_Married', 'Marital_Single']]\n",
    "\n",
    "# Filter to all active customers (Focus Groups 1 & 2 combined)\n",
    "demographic_features_a = demographic_features[df_Customer['Is_Focus_Group'] == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40debc19",
   "metadata": {},
   "source": [
    "### Remove redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for demographic features (excluding one-hot encoded categoricals)\n",
    "# Remove one-hot encoded marital status columns as they are categorical\n",
    "numeric_demographic_features = demographic_features_a.drop(columns=['Marital_Divorced', 'Marital_Married', 'Marital_Single'])\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "corr = numeric_demographic_features.corr()\n",
    "corr = corr.round(2)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    center=0,\n",
    "    cmap=GROUP80_palette_continuous.reversed(),\n",
    "    annot=np.where(np.absolute(corr) >= 0.5, corr.values, np.full(corr.values.shape, \"\")),\n",
    "    fmt='s',\n",
    "    square=True,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Pearson Correlation'}\n",
    ")\n",
    "plt.title('Correlation Heatmap: Numeric Demographic Features\\n(One-Hot Encoded Variables Excluded)', \n",
    "          fontsize=12, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec31d26",
   "metadata": {},
   "source": [
    "### Evaluate Feature Relevance\n",
    "\n",
    "Variance analysis before scaling is not meaningful here since the demographic features operate on different scales (e.g., frequency-encoded cities vs. ordinal education levels). After StandardScaler normalization, all features will have unit variance by definition. Feature relevance will instead be assessed through correlation analysis (see heatmap above) and cluster-specific feature importance during the clustering phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove \"Marital_Single\" because it is fully determined by the others\n",
    "demographic_final = demographic_features_a.drop(columns=[\"Marital_Single\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b843e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final demographic feature df\n",
    "df_demographic_a = demographic_final.copy()\n",
    "df_demographic_a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922a0e62",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\"> <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Demographic Feature Selection Summary</h3> <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"> We validated the selected demographic features to ensure they are suitable for clustering analysis. </p> <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of Selection:</h4> <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\"> <li style=\"margin-right: 20px;\">Ensure no highly correlated features that would introduce multicollinearity</li> <li style=\"margin-right: 20px;\">Identify potential redundancies between demographic metrics</li> </ul> <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4> <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\"> <li style=\"margin-right: 20px;\"><strong>Moderate correlations acceptable:</strong> Province-City (r=0.54) and Education-Income (r=0.55) correlations are expected as cities belong to provinces and education influences income, but remain below the critical threshold of |r| > 0.7</li> <li style=\"margin-right: 20px;\"><strong>All other correlations acceptable:</strong> Remaining feature pairs show |r| < 0.5, indicating independent information without redundancy</li> <li style=\"margin-right: 20px;\"><strong>Removed one-hot encoded redundancy:</strong> Excluded Marital_Single from the final feature set as it is fully determined by Marital_Married and Marital_Divorced (redundant one-hot encoded category)</li> </ul> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d7d27",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfc4e2",
   "metadata": {},
   "source": [
    "## **4.3 Select Behavorial Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d953c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select behavioral features\n",
    "behavioral_features = df_Customer[['avg_distance_per_flight', 'distance_variability', 'companion_flight_ratio', 'flight_regularity', 'seasonal_concentration', 'peak_season_sin', 'peak_season_cos', 'redemption_rate', 'redemption_frequency']]\n",
    "\n",
    "# Filter to all active customers (Focus Groups 1 & 2 combined)\n",
    "behavioral_features_a = behavioral_features[df_Customer['Is_Focus_Group'] == True]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe336e1",
   "metadata": {},
   "source": [
    "### Remove redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bc32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for behavioral features\n",
    "plt.figure(figsize=(10, 7))\n",
    "corr = behavioral_features_a.corr()\n",
    "corr = corr.round(2)\n",
    "\n",
    "sns.heatmap(\n",
    "    corr,\n",
    "    center=0,\n",
    "    cmap=GROUP80_palette_continuous.reversed(),\n",
    "    annot=np.where(np.absolute(corr) >= 0.5, corr.values, np.full(corr.values.shape, \"\")),\n",
    "    fmt='s',\n",
    "    square=True,\n",
    "    linewidths=.5,\n",
    "    cbar_kws={'label': 'Pearson Correlation'}\n",
    ")\n",
    "plt.title('Correlation Heatmap: Behavioral Features', \n",
    "          fontsize=12, fontweight='bold', pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badd5f64",
   "metadata": {},
   "source": [
    "### Evaluate Feature Relevance\n",
    "\n",
    "Variance analysis before scaling is not meaningful here since the behavioral features operate on vastly different scales (e.g., avg_distance_per_flight in km² vs. ratios bounded 0-1). After StandardScaler normalization, all features will have unit variance by definition. Feature relevance will instead be assessed through correlation analysis (see heatmap above) and cluster-specific feature importance during the clustering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c60187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final behavioral feature df - filtered to 4 selected clustering features\n",
    "behavioral_feats = ['distance_variability', 'companion_flight_ratio', 'flight_regularity', 'redemption_frequency']\n",
    "df_behavioral_a = behavioral_features_a[behavioral_feats].copy()\n",
    "df_behavioral_a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7a5b62",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fce8e8ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #8B0000, #A52A2A, #CD5C5C, #F08080) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #8B0000; font-weight: bold;\">Critical: Feature Selection from 9 to 4 Behavioral Features</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Initial Problem:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        With all 9 behavioral features, clustering produced <strong>Silhouette scores below 0.10</strong> and poor cluster separation. Extensive grid searches over feature combinations revealed problematic correlations: <strong>redemption_rate</strong> and <strong>redemption_frequency</strong> dominated together, <strong>peak_season_sin</strong> and <strong>peak_season_cos</strong> created redundancy, while other features showed little to no discriminative power.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Key Insight: Clustering Features vs. Targeting Attributes</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        True behavioral clustering requires features that capture <strong>behavioral Patterns</strong> (Who the customer is), not <strong>states</strong> or <strong>targeting attributes</strong> (What/When to offer). The distinction:\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li><strong>Clustering Features:</strong> Define customer segments through repeated behavioral patterns over time</li>\n",
    "        <li><strong>Targeting Attributes:</strong> Enable post-hoc personalization within segments (can be filtered directly without clustering)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Excluded Features and Rationale:</h4>\n",
    "    <table style=\"width: 100%; border-collapse: collapse; margin: 10px 0; color: #000;\">\n",
    "        <tr style=\"background-color: rgba(139, 0, 0, 0.1);\">\n",
    "            <th style=\"padding: 8px; text-align: left; border-bottom: 1px solid #8B0000;\">Feature</th>\n",
    "            <th style=\"padding: 8px; text-align: left; border-bottom: 1px solid #8B0000;\">Reason for Exclusion</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>avg_distance_per_flight</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Targeting attribute. Direct filtering possible (IF distance > 3000km -> long-haul offer). Value Preselection already uses total_distance and avg_flights on axes, making avg_distance derivable.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>seasonal_concentration</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Redundant with flight_regularity. Both measure temporal consistency, but flight_regularity (CV of monthly flights) is more interpretable.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>peak_season_sin / peak_season_cos</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Timing attributes for campaign scheduling. Direct filtering possible (IF peak_cos > 0.5 -> winter campaign). Not a behavioral pattern.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>redemption_rate</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">State, not pattern. Measures cumulative ratio (points redeemed / points earned) at a single point in time. One large redemption event changes the entire metric.</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Final 4 Features Selected:</h4>\n",
    "    <table style=\"width: 100%; border-collapse: collapse; margin: 10px 0; color: #000;\">\n",
    "        <tr style=\"background-color: rgba(139, 0, 0, 0.1);\">\n",
    "            <th style=\"padding: 8px; text-align: left; border-bottom: 1px solid #8B0000;\">Feature</th>\n",
    "            <th style=\"padding: 8px; text-align: left; border-bottom: 1px solid #8B0000;\">Pattern Type</th>\n",
    "            <th style=\"padding: 8px; text-align: left; border-bottom: 1px solid #8B0000;\">Strategic Interpretation</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>distance_variability</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Travel Pattern</td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Routinized (same routes) vs. Explorer (variable destinations)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>companion_flight_ratio</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Social Pattern</td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Solo traveler vs. Group/Family traveler (average across all flights)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>flight_regularity</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Temporal Pattern</td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Regular flyer (consistent monthly) vs. Sporadic flyer (irregular bursts)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\"><strong>redemption_frequency</strong></td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Engagement Pattern</td>\n",
    "            <td style=\"padding: 8px; border-bottom: 1px solid rgba(139, 0, 0, 0.3);\">Active redeemer (redeems in many months) vs. Passive accumulator (rarely redeems)</td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Why redemption_frequency over redemption_rate:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Consider two customers with identical redemption_rate = 0.8:\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li><strong>Customer A:</strong> Redeemed once (large amount) -> frequency = 0.1</li>\n",
    "        <li><strong>Customer B:</strong> Redeemed every month (small amounts) -> frequency = 0.9</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Same rate, completely different behavior. <strong>Frequency captures the habit</strong>, rate captures the cumulative state.\n",
    "    </p>\n",
    "     <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Post-Hoc Targeting Strategy:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Excluded features remain valuable for <strong>within-cluster personalization</strong>. After identifying behavioral segments, marketers can apply additional filters using avg_distance (short vs. long-haul offers) or peak_season_cos (winter vs. summer campaigns) to tailor specific promotions within each segment. This separates the question \"Who behaves similarly?\" (clustering) from \"What should we offer them?\" (targeting).\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81151c4d",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='5'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;   max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>5. Feature Scaling</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ace83",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">To be added: Reference on 2.5 and Goal and Reference to next step, and also very short outline (Before/After Scaling) </h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d6c60c",
   "metadata": {},
   "source": [
    "## **5.1 Scale demographical Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc598e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit StandardScaler (excluding one-hot encoded marital status features)\n",
    "scaler_demo = StandardScaler()\n",
    "cols_to_scale = [col for col in df_demographic_a.columns if not col.startswith('Marital_')]\n",
    "df_demographic_a_scaled = df_demographic_a.copy()\n",
    "df_demographic_a_scaled[cols_to_scale] = scaler_demo.fit_transform(df_demographic_a[cols_to_scale])\n",
    "\n",
    "# Ensure one-hot encoded marital status columns remain as float type\n",
    "df_demographic_a_scaled['Marital_Divorced'] = df_demographic_a_scaled['Marital_Divorced'].astype(float)\n",
    "df_demographic_a_scaled['Marital_Married'] = df_demographic_a_scaled['Marital_Married'].astype(float)\n",
    "\n",
    "# Verify scaling: mean = 0, std = 1 for scaled features\n",
    "print(df_demographic_a_scaled[cols_to_scale].describe().loc[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7e20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison: Before (left) and After (right)\n",
    "# Only show scaled features (exclude one-hot encoded marital status)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Left plot: Before scaling (only scaled features)\n",
    "df_before_long = df_demographic_a[cols_to_scale].melt(var_name='Feature', value_name='Value')\n",
    "sns.boxplot(\n",
    "    x='Value',\n",
    "    y='Feature',\n",
    "    data=df_before_long,\n",
    "    ax=axes[0],\n",
    "    color=CUSTOM_HEX[0],\n",
    "    showfliers=False,\n",
    "    orient='h'\n",
    ")\n",
    "axes[0].set_title('Before Standard Scaling', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Original Feature Values', fontsize=11)\n",
    "axes[0].set_ylabel('Feature', fontsize=11)\n",
    "\n",
    "\n",
    "# Right plot: After scaling (only scaled features)\n",
    "df_after_long = df_demographic_a_scaled[cols_to_scale].melt(var_name='Feature', value_name='Value')\n",
    "sns.boxplot(\n",
    "    x='Value',\n",
    "    y='Feature',\n",
    "    data=df_after_long,\n",
    "    ax=axes[1],\n",
    "    color=CUSTOM_HEX[2],\n",
    "    showfliers=False,\n",
    "    orient='h'\n",
    ")\n",
    "axes[1].set_title('After Standard Scaling', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Standardized Values (Z-Score)', fontsize=11)\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_yticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234e5f0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Standard Scaling (Z-Score Normalization)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Distance-based clustering algorithms (K-Means, Hierarchical Clustering) compute Euclidean distances between observations. Features with large numerical ranges (e.g., <strong>Province_Encoded</strong>) dominate distance calculations over features with small ranges (e.g., <strong>Gender_Encoded</strong>: 0-1), leading to biased clustering results. Standard Scaling equalizes feature importance by transforming all features to the same scale (mean = 0, standard deviation = 1).\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal of Scaling:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Standardize features by removing the mean (μ) and scaling to unit variance (σ = 1)</li>\n",
    "        <li style=\"margin-right: 20px;\">Ensure all features contribute equally to distance calculations regardless of their original scale</li>\n",
    "        <li style=\"margin-right: 20px;\">Prevent frequency-encoded geographic features from dominating the clustering algorithm</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Implementation Details:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Why StandardScaler over MinMaxScaler?</strong> StandardScaler is robust to outliers (uses mean/std instead of min/max) and is the industry standard for clustering algorithms. MinMaxScaler compresses data to [0,1] but is sensitive to extreme values in frequency-encoded features.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>One-hot encoded features excluded:</strong> Marital status features (Marital_Divorced, Marital_Married) are not scaled as they are binary indicators that should remain in their original 0/1 form to preserve interpretability.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Critical for frequency-encoded features:</strong> Geographic features (<strong>Province_Encoded</strong>, <strong>City_Encoded</strong>, <strong>FSA_Encoded</strong>) use customer counts (large range), which have variance 100-1000x larger than ordinal/binary features. Without scaling, clustering would be entirely geography-driven.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41448497",
   "metadata": {},
   "source": [
    "## **5.2 Scale behavorial Features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807dcaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit StandardScaler (excluding peak_season_sin/cos which are already bounded -1 to +1)\n",
    "scaler_behav = StandardScaler()\n",
    "cols_to_scale = [col for col in df_behavioral_a.columns if col not in ['peak_season_sin', 'peak_season_cos']]\n",
    "df_behavioral_a_scaled = df_behavioral_a.copy()\n",
    "df_behavioral_a_scaled[cols_to_scale] = scaler_behav.fit_transform(df_behavioral_a[cols_to_scale])\n",
    "\n",
    "# Verify scaling: mean = 0, std = 1 for scaled features\n",
    "print(df_behavioral_a_scaled[cols_to_scale].describe().loc[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac944b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side comparison: Before (left) and After (right)\n",
    "# Only show scaled features (exclude peak_season_sin/cos)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Left plot: Before scaling (only scaled features)\n",
    "df_before_long = df_behavioral_a[cols_to_scale].melt(var_name='Feature', value_name='Value')\n",
    "sns.boxplot(\n",
    "    x='Value',\n",
    "    y='Feature',\n",
    "    data=df_before_long,\n",
    "    ax=axes[0],\n",
    "    color=CUSTOM_HEX[0],\n",
    "    showfliers=False,\n",
    "    orient='h'\n",
    ")\n",
    "axes[0].set_title('Before Standard Scaling', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Original Feature Values', fontsize=11)\n",
    "axes[0].set_ylabel('Feature', fontsize=11)\n",
    "\n",
    "\n",
    "# Right plot: After scaling (only scaled features)\n",
    "df_after_long = df_behavioral_a_scaled[cols_to_scale].melt(var_name='Feature', value_name='Value')\n",
    "sns.boxplot(\n",
    "    x='Value',\n",
    "    y='Feature',\n",
    "    data=df_after_long,\n",
    "    ax=axes[1],\n",
    "    color=CUSTOM_HEX[2],\n",
    "    showfliers=False,\n",
    "    orient='h'\n",
    ")\n",
    "axes[1].set_title('After Standard Scaling', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Standardized Values (Z-Score)', fontsize=11)\n",
    "axes[1].set_ylabel('')\n",
    "axes[1].set_yticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8cee5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Standard Scaling (Z-Score Normalization)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Same scaling approach as for demographic features (see above). Key difference for behavioral features:\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Behavioral-Specific Implementation:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cyclical features excluded:</strong> peak_season_sin and peak_season_cos are not scaled as they are already bounded between -1 and +1 by their trigonometric encoding, preserving their cyclical nature.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2b7a3",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='6'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>6. Value Based Preselection</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f79ff64",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Goal of Section 6: Value-Based Preselection</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        This section performs <strong>FM segmentation</strong> using a <strong>rule-based median-split approach</strong> rather than advanced clustering algorithms. This value-based preselection creates interpretable customer segments that provide a foundation for understanding customer value before demographic and behavioral clustering.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #000000ff; font-weight: bold;\">Why Rule-Based Segmentation Instead of Advanced Clustering?</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Limited Feature Space:</strong> Only 2 features (Frequency and Monetary) - insufficient dimensionality for complex clustering algorithms to provide meaningful advantages</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Business Interpretability:</strong> Median-split creates clearly defined, explainable segments (High/Low F × High/Low M) that stakeholders can easily understand and act upon</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Benchmark Standardization:</strong> FM matrix is an established framework in customer analytics, enabling comparison with industry standards and best practices</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Reserve Complexity for Rich Feature Sets:</strong> Advanced clustering (Hierarchical, K-Means etc.) will be applied to demographic (6+ features) and behavioral (8+ features) datasets where they can capture complex multi-dimensional patterns</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Computational Efficiency:</strong> Simple rule-based segmentation is fast, deterministic, and doesn't require hyperparameter tuning or convergence iterations</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #000000ff; font-weight: bold;\">Specific Objectives:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Create 4 FM Segments:</strong> Champions (High F & High M), Frequent Flyer (High F & Low M), Premium Occasional (Low F & High M), At Risk (Low F & Low M)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Identify Elite Tier:</strong> Top 10% customers in both Frequency and Monetary dimensions within Champions segment</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Separate Analysis by Focus Group:</strong> Apply segmentation independently to Focus Group 1 (Loyalty Members | Active) and Focus Group 2 (Ex-Loyalty Members | Active)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Combined View Analysis:</strong> Segment ALL active customers together using unified thresholds to identify win-back opportunities for Focus Group 2 customers</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Segment Migration Analysis:</strong> Track how Focus Group 2 customers' segments change when evaluated against combined thresholds (e.g., Elite → Champions, Champions → At Risk)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Reference for Integration:</strong> Store FM segments as categorical features to be combined with clustering results in Chapter 9</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #000000ff; font-weight: bold;\">Business Value:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The Combined View demonstrates the <strong>opportunity of re-enrolling Focus Group 2 customers</strong> into the loyalty program. By showing how ex-loyalty members perform when benchmarked against current loyalty members, we can identify high-value win-back targets and quantify the potential revenue uplift from successful re-enrollment campaigns.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d8bf0",
   "metadata": {},
   "source": [
    "## **6.1 Categorize Customers in FM Segments**\n",
    "\n",
    "We create a **2x2 FM Matrix** using median as the cutoff for both Frequency and Monetary dimensions. Additionally, we identify **Elite customers** (Top 10% in both dimensions) within the Champions segment for premium targeting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6090283",
   "metadata": {},
   "source": [
    "### **Focus Group 1: Loyalty Members | Active**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe3950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-filtered DataFrame from Section 3.1\n",
    "fg1_customers = df_Customer.loc[fm_features_l_a.index].copy()\n",
    "\n",
    "# Calculate thresholds\n",
    "freq_median_fg1 = fg1_customers['Frequency'].median()\n",
    "mon_median_fg1 = fg1_customers['Monetary'].median()\n",
    "freq_p90_fg1 = fg1_customers['Frequency'].quantile(0.90)\n",
    "mon_p90_fg1 = fg1_customers['Monetary'].quantile(0.90)\n",
    "\n",
    "# Segmentation functions\n",
    "def assign_fm_segment(row, freq_med, mon_med):\n",
    "    if row['Frequency'] >= freq_med and row['Monetary'] >= mon_med:\n",
    "        return 'Champions'\n",
    "    elif row['Frequency'] >= freq_med and row['Monetary'] < mon_med:\n",
    "        return 'Frequent Flyer'\n",
    "    elif row['Frequency'] < freq_med and row['Monetary'] >= mon_med:\n",
    "        return 'Premium Occasional'\n",
    "    else:\n",
    "        return 'At Risk'\n",
    "\n",
    "def assign_fm_tier(row, freq_med, mon_med, freq_p90, mon_p90):\n",
    "    if row['Frequency'] >= freq_p90 and row['Monetary'] >= mon_p90:\n",
    "        return 'Elite'\n",
    "    elif row['Frequency'] >= freq_med and row['Monetary'] >= mon_med:\n",
    "        return 'High'\n",
    "    elif row['Frequency'] >= freq_med or row['Monetary'] >= mon_med:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Apply segmentation\n",
    "fg1_customers['fm_segment_fg1'] = fg1_customers.apply(\n",
    "    lambda row: assign_fm_segment(row, freq_median_fg1, mon_median_fg1), axis=1\n",
    ")\n",
    "fg1_customers['fm_tier_fg1'] = fg1_customers.apply(\n",
    "    lambda row: assign_fm_tier(row, freq_median_fg1, mon_median_fg1, freq_p90_fg1, mon_p90_fg1), axis=1\n",
    ")\n",
    "\n",
    "# Summary DataFrame\n",
    "summary_fg1 = pd.DataFrame({\n",
    "    'Segment': ['Champions', 'Frequent Flyer', 'Premium Occasional', 'At Risk'],\n",
    "    'Count': [(fg1_customers['fm_segment_fg1'] == seg).sum() for seg in ['Champions', 'Frequent Flyer', 'Premium Occasional', 'At Risk']],\n",
    "    'Pct': [(fg1_customers['fm_segment_fg1'] == seg).sum() / len(fg1_customers) * 100 for seg in ['Champions', 'Frequent Flyer', 'Premium Occasional', 'At Risk']]\n",
    "})\n",
    "summary_fg1['Elite_Count'] = [\n",
    "    (fg1_customers['fm_tier_fg1'] == 'Elite').sum() if seg == 'Champions' else 0 \n",
    "    for seg in summary_fg1['Segment']\n",
    "]\n",
    "\n",
    "# Merge back\n",
    "df_Customer = df_Customer.merge(\n",
    "    fg1_customers[['Loyalty#', 'fm_segment_fg1', 'fm_tier_fg1']], \n",
    "    on='Loyalty#', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "summary_fg1.style.format({'Pct': '{:.1f}%'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24a042c",
   "metadata": {},
   "source": [
    "### **Focus Group 2: Ex-Loyalty Members | Active**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0913d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-filtered DataFrame from Section 3.1\n",
    "fg2_customers = df_Customer.loc[fm_features_non_l_a.index].copy()\n",
    "\n",
    "# Calculate thresholds\n",
    "freq_median_fg2 = fg2_customers['Frequency'].median()\n",
    "mon_median_fg2 = fg2_customers['Monetary'].median()\n",
    "freq_p90_fg2 = fg2_customers['Frequency'].quantile(0.90)\n",
    "mon_p90_fg2 = fg2_customers['Monetary'].quantile(0.90)\n",
    "\n",
    "# Apply segmentation\n",
    "fg2_customers['fm_segment_fg2'] = fg2_customers.apply(\n",
    "    lambda row: assign_fm_segment(row, freq_median_fg2, mon_median_fg2), axis=1\n",
    ")\n",
    "fg2_customers['fm_tier_fg2'] = fg2_customers.apply(\n",
    "    lambda row: assign_fm_tier(row, freq_median_fg2, mon_median_fg2, freq_p90_fg2, mon_p90_fg2), axis=1\n",
    ")\n",
    "\n",
    "# Summary DataFrame\n",
    "summary_fg2 = pd.DataFrame({\n",
    "    'Segment': ['Champions', 'Frequent Flyer', 'Premium Occasional', 'At Risk'],\n",
    "    'Count': [(fg2_customers['fm_segment_fg2'] == seg).sum() for seg in ['Champions', 'Frequent Flyer', 'Premium Occasional', 'At Risk']],\n",
    "    'Pct': [(fg2_customers['fm_segment_fg2'] == seg).sum() / len(fg2_customers) * 100 for seg in ['Champions', 'Frequent Flyer', 'Premium Occasional', 'At Risk']]\n",
    "})\n",
    "summary_fg2['Elite_Count'] = [\n",
    "    (fg2_customers['fm_tier_fg2'] == 'Elite').sum() if seg == 'Champions' else 0 \n",
    "    for seg in summary_fg2['Segment']\n",
    "]\n",
    "\n",
    "# Merge back\n",
    "df_Customer = df_Customer.merge(\n",
    "    fg2_customers[['Loyalty#', 'fm_segment_fg2', 'fm_tier_fg2']], \n",
    "    on='Loyalty#', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "summary_fg2.style.format({'Pct': '{:.1f}%'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b4de51",
   "metadata": {},
   "source": [
    "## **6.2 FM Matrix Visualization**\n",
    "\n",
    "Visualize the 2x2 FM matrix for both focus groups, highlighting the **Elite segment** (Top 10% in both dimensions) within the Champions quadrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab50b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2x2 FM Matrix Visualization for both Focus Groups\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "fig.suptitle('FM Matrix: 2x2 Customer Segmentation with Elite Tier', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# Define colors\n",
    "segment_colors = {\n",
    "    'Champions': colors[3],\n",
    "    'Frequent Flyer': colors[1],\n",
    "    'Premium Occasional': colors[2],\n",
    "    'At Risk': colors[4]\n",
    "}\n",
    "elite_color = colors[0]\n",
    "label_color = \"#00411E\"  # Color for all quadrant labels\n",
    "\n",
    "# --- Focus Group 1: Loyalty Members | Active ---\n",
    "ax = axes[0]\n",
    "\n",
    "# Plot reference lines\n",
    "ax.axvline(freq_median_fg1, color='#313131', linestyle='--', linewidth=2, alpha=0.7, zorder=2)\n",
    "ax.axhline(mon_median_fg1, color='#313131', linestyle='--', linewidth=2, alpha=0.7, zorder=2)\n",
    "\n",
    "# Highlight Champions quadrant\n",
    "max_freq_fg1 = fg1_customers['Frequency'].max() * 1.05\n",
    "max_mon_fg1 = fg1_customers['Monetary'].max() * 1.05\n",
    "ax.axvspan(freq_median_fg1, max_freq_fg1, alpha=0.08, color=colors[3], zorder=0)\n",
    "ax.axhspan(mon_median_fg1, max_mon_fg1, alpha=0.08, color=colors[3], zorder=0)\n",
    "\n",
    "# Highlight Elite zone (Top 10% in both)\n",
    "ax.axvspan(freq_p90_fg1, max_freq_fg1, alpha=0.15, color=elite_color, zorder=1)\n",
    "ax.axhspan(mon_p90_fg1, max_mon_fg1, alpha=0.15, color=elite_color, zorder=1)\n",
    "\n",
    "# Scatter plot by segment\n",
    "for segment in ['At Risk', 'Premium Occasional', 'Frequent Flyer', 'Champions']:\n",
    "    segment_data = fg1_customers[fg1_customers['fm_segment_fg1'] == segment]\n",
    "    \n",
    "    # Separate Elite from Champions\n",
    "    if segment == 'Champions':\n",
    "        elite_data = segment_data[segment_data['fm_tier_fg1'] == 'Elite']\n",
    "        non_elite_data = segment_data[segment_data['fm_tier_fg1'] != 'Elite']\n",
    "        \n",
    "        # Plot non-Elite Champions\n",
    "        ax.scatter(non_elite_data['Frequency'], non_elite_data['Monetary'],\n",
    "                  c=segment_colors[segment], s=50, alpha=0.6, \n",
    "                  edgecolor='white', linewidth=0.3, zorder=3,\n",
    "                  label=f'Champions (n={len(non_elite_data):,})')\n",
    "        \n",
    "        # Plot Elite with distinct marker\n",
    "        ax.scatter(elite_data['Frequency'], elite_data['Monetary'],\n",
    "                  c=elite_color, s=80, alpha=0.8, marker='D',\n",
    "                  edgecolor='white', linewidth=0.5, zorder=4,\n",
    "                  label=f'Elite (n={len(elite_data):,})')\n",
    "    else:\n",
    "        ax.scatter(segment_data['Frequency'], segment_data['Monetary'],\n",
    "                  c=segment_colors[segment], s=50, alpha=0.6,\n",
    "                  edgecolor='white', linewidth=0.3, zorder=3,\n",
    "                  label=f'{segment} (n={len(segment_data):,})')\n",
    "\n",
    "# Add quadrant labels (with high zorder to stay on top)\n",
    "label_style = {'fontsize': 14, 'fontweight': 'bold', 'zorder': 10, 'color': '#00411E'}\n",
    "ax.text(freq_median_fg1 * 1.4, mon_median_fg1 * 1.4, 'Champions', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_fg1 * 0.3, mon_median_fg1 * 1.4, 'Premium\\nOccasional', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_fg1 * 1.4, mon_median_fg1 * 0.3, 'Frequent\\nFlyer', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_fg1 * 0.3, mon_median_fg1 * 0.3, 'At Risk', \n",
    "        ha='center', va='center', **label_style)\n",
    "# Add Elite label in center of Elite zone with highest z-order\n",
    "elite_label_style = {'fontsize': 13, 'fontweight': 'bold', 'zorder': 100, 'color': '#00411E', 'style': 'italic'}\n",
    "elite_center_x = (freq_p90_fg1 + max_freq_fg1) / 2\n",
    "elite_center_y = (mon_p90_fg1 + max_mon_fg1) / 2\n",
    "ax.text(elite_center_x, elite_center_y, 'Elite', \n",
    "        ha='center', va='center', **elite_label_style)\n",
    "\n",
    "ax.set_xlabel(\"Frequency (Flights per Active Month)\", fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel(\"Monetary (Distance per Active Month)\", fontsize=11, fontweight='bold')\n",
    "ax.set_title(f\"Focus Group 1: Loyalty Members | Active\\n(n={len(fg1_customers):,} customers)\", \n",
    "             fontsize=12, fontweight='bold', pad=10)\n",
    "ax.legend(loc='lower right', fontsize=8, frameon=True, shadow=True)\n",
    "ax.grid(True, alpha=0.2, linestyle=':', zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# --- Focus Group 2: Ex-Loyalty Members | Active ---\n",
    "ax = axes[1]\n",
    "\n",
    "# Plot reference lines\n",
    "ax.axvline(freq_median_fg2, color='#313131', linestyle='--', linewidth=2, alpha=0.7, zorder=2)\n",
    "ax.axhline(mon_median_fg2, color='#313131', linestyle='--', linewidth=2, alpha=0.7, zorder=2)\n",
    "\n",
    "# Highlight Champions quadrant\n",
    "max_freq_fg2 = fg2_customers['Frequency'].max() * 1.05\n",
    "max_mon_fg2 = fg2_customers['Monetary'].max() * 1.05\n",
    "ax.axvspan(freq_median_fg2, max_freq_fg2, alpha=0.08, color=colors[3], zorder=0)\n",
    "ax.axhspan(mon_median_fg2, max_mon_fg2, alpha=0.08, color=colors[3], zorder=0)\n",
    "\n",
    "# Highlight Elite zone (Top 10% in both)\n",
    "ax.axvspan(freq_p90_fg2, max_freq_fg2, alpha=0.15, color=elite_color, zorder=1)\n",
    "ax.axhspan(mon_p90_fg2, max_mon_fg2, alpha=0.15, color=elite_color, zorder=1)\n",
    "\n",
    "# Scatter plot by segment\n",
    "for segment in ['At Risk', 'Premium Occasional', 'Frequent Flyer', 'Champions']:\n",
    "    segment_data = fg2_customers[fg2_customers['fm_segment_fg2'] == segment]\n",
    "    \n",
    "    # Separate Elite from Champions\n",
    "    if segment == 'Champions':\n",
    "        elite_data = segment_data[segment_data['fm_tier_fg2'] == 'Elite']\n",
    "        non_elite_data = segment_data[segment_data['fm_tier_fg2'] != 'Elite']\n",
    "        \n",
    "        # Plot non-Elite Champions\n",
    "        ax.scatter(non_elite_data['Frequency'], non_elite_data['Monetary'],\n",
    "                  c=segment_colors[segment], s=50, alpha=0.6, \n",
    "                  edgecolor='white', linewidth=0.3, zorder=3,\n",
    "                  label=f'Champions (n={len(non_elite_data):,})')\n",
    "        \n",
    "        # Plot Elite with distinct marker\n",
    "        ax.scatter(elite_data['Frequency'], elite_data['Monetary'],\n",
    "                  c=elite_color, s=80, alpha=0.8, marker='D',\n",
    "                  edgecolor='white', linewidth=0.5, zorder=4,\n",
    "                  label=f'Elite (n={len(elite_data):,})')\n",
    "    else:\n",
    "        ax.scatter(segment_data['Frequency'], segment_data['Monetary'],\n",
    "                  c=segment_colors[segment], s=50, alpha=0.6,\n",
    "                  edgecolor='white', linewidth=0.3, zorder=3,\n",
    "                  label=f'{segment} (n={len(segment_data):,})')\n",
    "\n",
    "# Add quadrant labels\n",
    "ax.text(freq_median_fg2 * 1.4, mon_median_fg2 * 1.4, 'Champions', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_fg2 * 0.3, mon_median_fg2 * 1.4, 'Premium\\nOccasional', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_fg2 * 1.4, mon_median_fg2 * 0.3, 'Frequent\\nFlyer', \n",
    "        ha='center', va='center', **label_style)\n",
    "# Add Elite label in center of Elite zone with highest z-order\n",
    "elite_center_x = (freq_p90_fg2 + max_freq_fg2) / 2\n",
    "elite_center_y = (mon_p90_fg2 + max_mon_fg2) / 2\n",
    "ax.text(elite_center_x, elite_center_y, 'Elite', \n",
    "        ha='center', va='center', **elite_label_style)\n",
    "\n",
    "ax.set_xlabel(\"Frequency (Flights per Active Month)\", fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel(\"Monetary (Distance per Active Month)\", fontsize=11, fontweight='bold')\n",
    "ax.set_title(f\"Focus Group 2: Ex-Loyalty Members | Active\\n(n={len(fg2_customers):,} customers)\", \n",
    "             fontsize=12, fontweight='bold', pad=10)\n",
    "ax.legend(loc='lower right', fontsize=8, frameon=True, shadow=True)\n",
    "ax.grid(True, alpha=0.2, linestyle=':', zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6391a4a8",
   "metadata": {},
   "source": [
    "## **6.3 Segment Profiling**\n",
    "\n",
    "Analyze the characteristics of each FM segment across both focus groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626208b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile Focus Group 1: Loyalty Members | Active\n",
    "profile_fg1 = fg1_customers.groupby('fm_segment_fg1').agg({\n",
    "    'Frequency': ['mean', 'median'],\n",
    "    'Monetary': ['mean', 'median'],\n",
    "    'Loyalty#': 'count'\n",
    "}).round(2)\n",
    "profile_fg1.columns = ['_'.join(col).strip() for col in profile_fg1.columns.values]\n",
    "profile_fg1.rename(columns={'Loyalty#_count': 'Count'}, inplace=True)\n",
    "\n",
    "# Profile Focus Group 2: Ex-Loyalty Members | Active\n",
    "profile_fg2 = fg2_customers.groupby('fm_segment_fg2').agg({\n",
    "    'Frequency': ['mean', 'median'],\n",
    "    'Monetary': ['mean', 'median'],\n",
    "    'Loyalty#': 'count'\n",
    "}).round(2)\n",
    "profile_fg2.columns = ['_'.join(col).strip() for col in profile_fg2.columns.values]\n",
    "profile_fg2.rename(columns={'Loyalty#_count': 'Count'}, inplace=True)\n",
    "\n",
    "# Elite Tier Summary\n",
    "elite_summary = pd.DataFrame({\n",
    "    'Focus Group': ['Loyalty Members | Active', 'Ex-Loyalty Members | Active'],\n",
    "    'Elite Count': [\n",
    "        (fg1_customers['fm_tier_fg1'] == 'Elite').sum(),\n",
    "        (fg2_customers['fm_tier_fg2'] == 'Elite').sum()\n",
    "    ],\n",
    "    'Elite %': [\n",
    "        (fg1_customers['fm_tier_fg1'] == 'Elite').sum() / len(fg1_customers) * 100,\n",
    "        (fg2_customers['fm_tier_fg2'] == 'Elite').sum() / len(fg2_customers) * 100\n",
    "    ],\n",
    "    'Avg Frequency': [\n",
    "        fg1_customers[fg1_customers['fm_tier_fg1'] == 'Elite']['Frequency'].mean(),\n",
    "        fg2_customers[fg2_customers['fm_tier_fg2'] == 'Elite']['Frequency'].mean()\n",
    "    ],\n",
    "    'Avg Monetary': [\n",
    "        fg1_customers[fg1_customers['fm_tier_fg1'] == 'Elite']['Monetary'].mean(),\n",
    "        fg2_customers[fg2_customers['fm_tier_fg2'] == 'Elite']['Monetary'].mean()\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display all profiles\n",
    "print(\"FOCUS GROUP 1: Loyalty Members | Active - Segment Profile\")\n",
    "display(profile_fg1)\n",
    "print(\"\\nFOCUS GROUP 2: Ex-Loyalty Members | Active - Segment Profile\")\n",
    "display(profile_fg2)\n",
    "print(\"\\nELITE TIER SUMMARY (Top 10% in both F & M)\")\n",
    "display(elite_summary.style.format({'Elite %': '{:.1f}%', 'Avg Frequency': '{:.2f}', 'Avg Monetary': '{:.2f}'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628372c2",
   "metadata": {},
   "source": [
    "## **6.4 Combined View: All Active Customers**\n",
    "\n",
    "Segment **all active customers** together (Focus Group 1 + Focus Group 2) using unified thresholds to benchmark ex-loyalty members against current loyalty members.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecb047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pre-filtered DataFrame from Section 3.1 (Combined: All Active Customers)\n",
    "combined_customers = df_Customer.loc[fm_features_a.index].copy()\n",
    "\n",
    "# Calculate COMBINED thresholds (across ALL active customers)\n",
    "freq_median_combined = combined_customers['Frequency'].median()\n",
    "mon_median_combined = combined_customers['Monetary'].median()\n",
    "freq_p90_combined = combined_customers['Frequency'].quantile(0.90)\n",
    "mon_p90_combined = combined_customers['Monetary'].quantile(0.90)\n",
    "\n",
    "# Apply segmentation to ALL active customers\n",
    "combined_customers['fm_segment_combined'] = combined_customers.apply(\n",
    "    lambda row: assign_fm_segment(row, freq_median_combined, mon_median_combined), axis=1\n",
    ")\n",
    "combined_customers['fm_tier_combined'] = combined_customers.apply(\n",
    "    lambda row: assign_fm_tier(row, freq_median_combined, mon_median_combined, freq_p90_combined, mon_p90_combined), axis=1\n",
    ")\n",
    "\n",
    "# Add Focus Group identifier\n",
    "combined_customers['focus_group'] = combined_customers['is_current_loyalty_member'].map({\n",
    "    True: 'FG1: Loyalty | Active',\n",
    "    False: 'FG2: Ex-Loyalty | Active'\n",
    "})\n",
    "\n",
    "# Summary by Focus Group\n",
    "summary_combined = combined_customers.groupby(['focus_group', 'fm_segment_combined']).size().unstack(fill_value=0)\n",
    "summary_combined['Total'] = summary_combined.sum(axis=1)\n",
    "summary_combined.loc['Total'] = summary_combined.sum()\n",
    "\n",
    "# Percentage distribution\n",
    "summary_combined_pct = summary_combined.div(summary_combined['Total'], axis=0) * 100\n",
    "\n",
    "# Merge back to main dataframe\n",
    "df_Customer = df_Customer.merge(\n",
    "    combined_customers[['Loyalty#', 'fm_segment_combined', 'fm_tier_combined']], \n",
    "    on='Loyalty#', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Combined: Segment Distribution by Focus Group\")\n",
    "\n",
    "display(summary_combined)\n",
    "print(\"\\nPercentage Distribution:\")\n",
    "display(summary_combined_pct.round(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78126194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Combined View with FG2 highlighted in RED\n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n",
    "fig.suptitle('Combined FM Matrix: All Active Customers\\nFocus Group 2 (Ex-Loyalty) Highlighted in Red', \n",
    "             fontsize=16, fontweight='bold', y=0.98)\n",
    "\n",
    "# Plot reference lines\n",
    "ax.axvline(freq_median_combined, color='#313131', linestyle='--', linewidth=2, alpha=0.7, zorder=2)\n",
    "ax.axhline(mon_median_combined, color='#313131', linestyle='--', linewidth=2, alpha=0.7, zorder=2)\n",
    "\n",
    "# Highlight Champions quadrant\n",
    "max_freq_combined = combined_customers['Frequency'].max() * 1.05\n",
    "max_mon_combined = combined_customers['Monetary'].max() * 1.05\n",
    "ax.axvspan(freq_median_combined, max_freq_combined, alpha=0.08, color=colors[3], zorder=0)\n",
    "ax.axhspan(mon_median_combined, max_mon_combined, alpha=0.08, color=colors[3], zorder=0)\n",
    "\n",
    "# Highlight Elite zone\n",
    "ax.axvspan(freq_p90_combined, max_freq_combined, alpha=0.15, color=elite_color, zorder=1)\n",
    "ax.axhspan(mon_p90_combined, max_mon_combined, alpha=0.15, color=elite_color, zorder=1)\n",
    "\n",
    "# Separate FG1 and FG2\n",
    "fg1_data = combined_customers[combined_customers['focus_group'] == 'FG1: Loyalty | Active']\n",
    "fg2_data = combined_customers[combined_customers['focus_group'] == 'FG2: Ex-Loyalty | Active']\n",
    "\n",
    "# Plot FG1 (Loyalty Members) - Green/Gray tones\n",
    "ax.scatter(fg1_data['Frequency'], fg1_data['Monetary'],\n",
    "          c=colors[2], s=30, alpha=0.4, \n",
    "          edgecolor='white', linewidth=0.2, zorder=3,\n",
    "          label=f'FG1: Loyalty Members (n={len(fg1_data):,})')\n",
    "\n",
    "# Plot FG2 (Ex-Loyalty Members) - RED with stronger visibility\n",
    "ax.scatter(fg2_data['Frequency'], fg2_data['Monetary'],\n",
    "          c='#D32F2F', s=50, alpha=0.7, marker='o',\n",
    "          edgecolor='darkred', linewidth=0.5, zorder=5,\n",
    "          label=f'FG2: Ex-Loyalty Members (n={len(fg2_data):,})')\n",
    "\n",
    "# Add quadrant labels\n",
    "label_style = {'fontsize': 14, 'fontweight': 'bold', 'zorder': 10, 'color': '#00411E'}\n",
    "ax.text(freq_median_combined * 1.4, mon_median_combined * 1.4, 'Champions', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_combined * 0.3, mon_median_combined * 1.4, 'Premium\\nOccasional', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_combined * 1.4, mon_median_combined * 0.3, 'Frequent\\nFlyer', \n",
    "        ha='center', va='center', **label_style)\n",
    "ax.text(freq_median_combined * 0.3, mon_median_combined * 0.3, 'At Risk', \n",
    "        ha='center', va='center', **label_style)\n",
    "\n",
    "# Add Elite label\n",
    "elite_label_style = {'fontsize': 13, 'fontweight': 'bold', 'zorder': 100, 'color': '#00411E', 'style': 'italic'}\n",
    "elite_center_x = (freq_p90_combined + max_freq_combined) / 2\n",
    "elite_center_y = (mon_p90_combined + max_mon_combined) / 2\n",
    "ax.text(elite_center_x, elite_center_y, 'Elite', \n",
    "        ha='center', va='center', **elite_label_style)\n",
    "\n",
    "ax.set_xlabel(\"Frequency (Flights per Active Month)\", fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel(\"Monetary (Distance per Active Month)\", fontsize=11, fontweight='bold')\n",
    "ax.set_title(f\"Combined Segmentation | Total Active Customers: n={len(combined_customers):,}\", \n",
    "             fontsize=12, fontweight='bold', pad=10)\n",
    "ax.legend(loc='lower right', fontsize=10, frameon=True, shadow=True)\n",
    "ax.grid(True, alpha=0.2, linestyle=':', zorder=0)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d1f184",
   "metadata": {},
   "source": [
    "Compare how **Focus Group 2 customers** are distributed across segments in their isolated view versus the combined view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8758a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple comparison table for FG2\n",
    "fg2_isolated_counts = fg2_customers.groupby('fm_segment_fg2').size().reset_index(name='FG2_Isolated')\n",
    "fg2_combined_counts = fg2_data.groupby('fm_segment_combined').size().reset_index(name='Combined')\n",
    "\n",
    "# Merge tables\n",
    "segment_comparison = fg2_isolated_counts.merge(\n",
    "    fg2_combined_counts, \n",
    "    left_on='fm_segment_fg2', \n",
    "    right_on='fm_segment_combined', \n",
    "    how='outer'\n",
    ").fillna(0)\n",
    "\n",
    "# Clean up\n",
    "segment_comparison = segment_comparison[['fm_segment_fg2', 'FG2_Isolated', 'Combined']]\n",
    "segment_comparison.columns = ['Segment', 'FG2 Isolated View', 'Combined View']\n",
    "segment_comparison = segment_comparison.set_index('Segment')\n",
    "\n",
    "# Ensure correct order\n",
    "segment_order = ['Champions', 'Frequent Flyer', 'Premium Occasional', 'At Risk']\n",
    "segment_comparison = segment_comparison.reindex([s for s in segment_order if s in segment_comparison.index])\n",
    "\n",
    "# Add percentages\n",
    "segment_comparison['FG2 Isolated (%)'] = (segment_comparison['FG2 Isolated View'] / segment_comparison['FG2 Isolated View'].sum() * 100).round(1)\n",
    "segment_comparison['Combined (%)'] = (segment_comparison['Combined View'] / segment_comparison['Combined View'].sum() * 100).round(1)\n",
    "\n",
    "# Reorder columns\n",
    "segment_comparison = segment_comparison[['FG2 Isolated View', 'FG2 Isolated (%)', 'Combined View', 'Combined (%)']]\n",
    "print(\"Focus Group 2: Segment Distribution Comparison\")\n",
    "print(f\"Total FG2 Customers: {len(fg2_data):,}\\n\")\n",
    "display(segment_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e9db63",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Value-Based Segmentation Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        We performed a comprehensive <strong>FM-based customer segmentation</strong> using a rule-based median-split approach to segment active customers by flight frequency and monetary value. Three distinct analyses were conducted: Focus Group 1 (Loyalty Members), Focus Group 2 (Ex-Loyalty Members), and a Combined View to benchmark performance across all active customers.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">What We Did:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Created FM Features:</strong> Engineered Frequency (flights/active month) and Monetary (distance/active month) metrics based on 2021 flight activity</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Separate Group Analysis:</strong> Applied median-split segmentation independently to FG1 (current loyalty members) and FG2 (ex-loyalty members) to understand each group's characteristics</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Combined Benchmarking:</strong> Segmented ALL active customers using unified thresholds to directly compare FG2 performance against FG1 standards</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Elite Tier Identification:</strong> Flagged Top 10% performers in both dimensions within the Champions segment for premium targeting</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>4 Actionable Segments:</strong> Champions (High F & M), Frequent Flyer (High F, Low M), Premium Occasional (Low F, High M), At Risk (Low F & M)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Performance Disparity:</strong> Current loyalty members (FG1) demonstrate significantly higher Frequency and Monetary values compared to ex-loyalty members (FG2), validating the loyalty program's effectiveness in attracting and retaining high-value customers</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Segment Redistribution:</strong> When benchmarked against combined thresholds, many FG2 customers shift from higher segments (in their isolated peer group) to lower segments, revealing a performance gap that represents win-back opportunity</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>High-Value Win-Back Targets:</strong> FG2 customers who remain in Champions/Elite tiers even in the combined view (competing against FG1) represent exceptional performers who are flying at loyalty-member levels despite not being enrolled—these are the highest-priority re-enrollment targets</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Segment Characteristics:</strong> Champions generate the most value across both frequency and distance; Frequent Flyers have engagement potential through route/class upgrades; Premium Occasional customers are retention-focused targets; At Risk requires re-engagement strategies</li>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bbdc22",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fefde9ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #909090, #d4d400, #e6e600) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #6b6b00; font-weight: bold;\">To be updated -- Next Steps: Multi-Dimensional Clustering (Chapters 7-9)</h3>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px; margin-bottom: 10px;\">\n",
    "        The FM segments created in this chapter serve as the <strong>value-based foundation</strong> for our customer segmentation strategy. To develop comprehensive, actionable customer personas, we will layer <strong>demographic</strong> and <strong>behavioral</strong> insights on top of these value segments through advanced clustering algorithms.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #6b6b00; font-weight: bold;\">Strategic Rationale:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Combined Target Group (ALL Active Customers):</strong> From Chapter 7 onwards, we analyze FG1 + FG2 together rather than separately. The Combined View demonstrated that unified benchmarking provides consistent performance standards and enables direct comparison for win-back prioritization.</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Multi-Perspective Segmentation:</strong> Value (FM) + Demographics + Behavior = comprehensive understanding of WHO customers are (demographics), HOW they behave (patterns), and WHAT they're worth (value)</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Focus Group Identifier Preserved:</strong> While clustering on combined data, we retain FG1/FG2 labels to identify win-back targets versus retention priorities in final personas</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #6b6b00; font-weight: bold;\">Chapter 7: Demographic Clustering</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Features:</strong> Gender, Education, Income, Marital Status, Location Code and more</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Algorithms:</strong> Hierarchical Clustering, K-Means, DBSCAN | Compare performance using Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Score</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Business Goal:</strong> Identify demographic profiles</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #6b6b00; font-weight: bold;\">Chapter 8: Behavioral Clustering</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Features:</strong> Flight patterns (frequency distribution, seasonality), points accumulation/redemption behavior, companion travel frequency, route preferences, booking patterns and more</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Algorithms:</strong> Hierarchical Clustering, K-Means, DBSCAN | Compare performance using evaluation metrics</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Business Goal:</strong> Identify behavioral archetypes</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #6b6b00; font-weight: bold;\">Chapter 9: Final Persona Integration</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Integration Strategy:</strong> Combine FM segments (value), demographic clusters, and behavioral clusters to create multi-dimensional customer personas</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Win-Back Prioritization:</strong> Use Focus Group identifier to flag FG2 customers within high-value personas for targeted re-enrollment campaigns</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Deliverables:</strong> Final customer personas with actionable marketing strategies, service personalization recommendations, and revenue opportunity sizing</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Business Impact:</strong> Quantify revenue uplift from converting FG2 Champions to FG1 engagement levels | Design persona-specific campaigns with projected ROI</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d748387",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d461b",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='7'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>7. Demographical Clustering</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280f4de",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 20px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Section 7: Demographic Clustering</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Why, Rationale:</h4>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        While FM segmentation (Section 6) captures customer value through transactional behavior (frequency, monetary value), demographic characteristics provide complementary insights into <strong>who</strong> our customers are beyond their purchasing patterns. Understanding demographic profiles enables more targeted marketing strategies, personalized communication, and product recommendations tailored to specific customer groups. By combining value-based segmentation with demographic clustering, we can create multi-dimensional customer personas that capture both <strong>what customers do</strong> (transactional behavior) and <strong>who they are</strong> (socio-demographic profile). This section systematically explores multiple clustering algorithms to identify natural demographic groupings within our customer base. The resulting demographic segments will be synthesized with FM segments & the behavorial segments in Section 9 to create comprehensive, actionable customer personas for strategic decision-making.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">What, Objectives:</h4>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Multi-Algorithm Comparison:</strong> Evaluate four distinct clustering approaches, Hierarchical Clustering (agglomerative, bottom-up), K-Means (centroid-based, partition), Mean Shift (density-based, non-parametric), and GMM (probabilistic, soft assignments), plus a SOM-based two-stage approach to understand how different algorithmic assumptions and optimization criteria segment the same demographic data differently</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Optimal Cluster Selection:</strong> For each algorithm, determine the optimal number of clusters using multiple validation metrics (Silhouette Score, Calinski-Harabasz Index, Davies-Bouldin Index, R² variance explained) while consistently favoring parsimonious solutions that balance statistical fit with business interpretability</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Initial Cluster Profiling:</strong> Characterize each algorithm's clusters using Z-score profile analysis (standardized centroid deviations from population mean) and feature importance analysis (variance of centroids across clusters). This initial profiling identifies which demographic features drive segmentation for each method. Comprehensive profiling with business interpretation, cross-tabulations, and strategic recommendations will follow in Section 9 after final segment selection and comparison across all clustering approaches</li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>Algorithm Behavior Analysis:</strong> Document how different algorithms handle the demographic feature space, which features emerge as primary differentiators, how cluster sizes distribute, and what segment structures each method reveals, to inform the final clustering method selection in Section 9</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">How, Method & Structure:</h4>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        <strong>Dataset:</strong> All active customers (FG1 + FG2 combined, n=14,527) from the demographic feature set prepared in Section 5.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        <strong>Features (9 variables, Z-score standardized):</strong>\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 4px;\"><strong>Geographic:</strong> Province_Encoded, City_Encoded, FSA_Encoded, Location_Code_Num</li>\n",
    "        <li style=\"margin-bottom: 4px;\"><strong>Personal:</strong> Gender_Encoded, Education_Level_Num, Income_Bin_Num</li>\n",
    "        <li style=\"margin-bottom: 4px;\"><strong>Marital Status:</strong> Marital_Divorced, Marital_Married (one-hot encoded)</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        <strong>Notebook Structure:</strong>\n",
    "    </p>\n",
    "    <ul style=\"margin: 10px 0; padding-left: 20px; color: #000; margin-right: 40px;\">\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>7.1 Hierarchical Clustering:</strong> Agglomerative clustering with linkage method comparison (Ward, Complete, Average, Single), optimal k selection via dendrogram analysis and validation metrics → <strong>k=6 selected</strong></li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>7.2 K-Means Clustering:</strong> Centroid-based partitioning with Elbow method and multi-metric evaluation → <strong>k=3 selected</strong></li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>7.3 Mean Shift Clustering:</strong> Non-parametric density-based clustering with bandwidth optimization via quantile grid search → <strong>4 clusters (q=0.132)</strong></li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>7.4 GMM Clustering:</strong> Gaussian Mixture Model with full covariance, probabilistic soft assignments, component selection via BIC/AIC → <strong>n=4 selected</strong></li>\n",
    "        <li style=\"margin-bottom: 6px;\"><strong>7.5 SOM + K-Means:</strong> Two-stage approach using 40×40 Self-Organizing Map for topology-preserving dimensionality reduction, followed by K-Means on learned neuron weights → <strong>k=4 selected</strong></li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; color: #000; margin-right: 40px;\">\n",
    "        Each subsection follows a consistent structure: (1) Methodology explanation, (2) Parameter/hyperparameter optimization, (3) Candidate solution comparison, (4) Final model selection, (5) Initial cluster profiling with Z-score heatmaps and feature importance analysis.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d526446",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87c5ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for variance decomposition and R² calculation\n",
    "\n",
    "def get_ss(df, feats):\n",
    "    \"\"\"\n",
    "    Calculate Total Sum of Squares (SST) for given features.\n",
    "    \n",
    "    SST = Σ(n-1) * Var(feature_j) across all features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset\n",
    "    feats : list\n",
    "        List of feature column names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Total sum of squares\n",
    "    \"\"\"\n",
    "    df_ = df[feats]\n",
    "    ss = np.sum(df_.var() * (df_.count() - 1))\n",
    "    return ss\n",
    "\n",
    "\n",
    "def get_ssb(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate Between-Cluster Sum of Squares (SSB).\n",
    "    \n",
    "    SSB = Σ n_k * (x̄_k - x̄)²\n",
    "    Measures variance between cluster centroids and overall mean.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset with cluster labels\n",
    "    feats : list\n",
    "        List of feature column names\n",
    "    label_col : str\n",
    "        Column name containing cluster labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Between-cluster sum of squares\n",
    "    \"\"\"\n",
    "    ssb_i = 0\n",
    "    X_ = df[feats].values\n",
    "    overall_mean = X_.mean(axis=0)\n",
    "    \n",
    "    for cluster_id in np.unique(df[label_col]):\n",
    "        X_k = df.loc[df[label_col] == cluster_id, feats].values\n",
    "        n_k = X_k.shape[0]\n",
    "        cluster_mean = X_k.mean(axis=0)\n",
    "        ssb_i += n_k * np.square(cluster_mean - overall_mean)\n",
    "    \n",
    "    return np.sum(ssb_i)\n",
    "\n",
    "\n",
    "def get_ssw(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate Within-Cluster Sum of Squares (SSW).\n",
    "    \n",
    "    SSW = Σ Σ (x_i - x̄_k)² for all points in each cluster\n",
    "    Measures total variance within all clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset with cluster labels\n",
    "    feats : list\n",
    "        List of feature column names\n",
    "    label_col : str\n",
    "        Column name containing cluster labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Within-cluster sum of squares\n",
    "    \"\"\"\n",
    "    feats_label = feats + [label_col]\n",
    "    df_k = df[feats_label].groupby(by=label_col).apply(\n",
    "        lambda col: get_ss(col, feats), \n",
    "        include_groups=False\n",
    "    )\n",
    "    return df_k.sum()\n",
    "\n",
    "\n",
    "def get_rsq(df, feats, label_col):\n",
    "    \"\"\"\n",
    "    Calculate R² (coefficient of determination) for clustering solution.\n",
    "    \n",
    "    R² = SSB / SST = (SST - SSW) / SST\n",
    "    \n",
    "    Interpretation:\n",
    "    - R² close to 1: High separation between clusters (good clustering)\n",
    "    - R² close to 0: Low separation between clusters (poor clustering)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset with cluster labels\n",
    "    feats : list\n",
    "        List of feature column names\n",
    "    label_col : str\n",
    "        Column name containing cluster labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : R² value between 0 and 1\n",
    "    \"\"\"\n",
    "    df_sst = get_ss(df, feats)\n",
    "    df_ssw = get_ssw(df, feats, label_col)\n",
    "    df_ssb = df_sst - df_ssw\n",
    "    \n",
    "    return df_ssb / df_sst\n",
    "\n",
    "\n",
    "def get_r2_hc(df, link_method, max_nclus, min_nclus=1, dist=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    Compute R² for hierarchical clustering across multiple k values.\n",
    "    \n",
    "    Applies hierarchical clustering for k = min_nclus to max_nclus and\n",
    "    calculates R² for each solution to identify optimal cluster count.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataset (should be scaled)\n",
    "    link_method : str\n",
    "        Linkage method: 'ward', 'complete', 'average', or 'single'\n",
    "    max_nclus : int\n",
    "        Maximum number of clusters to test\n",
    "    min_nclus : int, default=1\n",
    "        Minimum number of clusters to test\n",
    "    dist : str, default='euclidean'\n",
    "        Distance metric for clustering\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray : Array of R² values for each k\n",
    "    \"\"\"\n",
    "    r2 = []\n",
    "    feats = df.columns.tolist()\n",
    "    \n",
    "    for i in range(min_nclus, max_nclus + 1):\n",
    "        cluster = AgglomerativeClustering(\n",
    "            linkage=link_method, \n",
    "            metric=dist, \n",
    "            n_clusters=i\n",
    "        )\n",
    "        hclabels = cluster.fit_predict(df[feats])\n",
    "        \n",
    "        df_concat = pd.concat([\n",
    "            df, \n",
    "            pd.Series(hclabels, name='labels', index=df.index)\n",
    "        ], axis=1)\n",
    "        \n",
    "        r2.append(get_rsq(df_concat, feats, 'labels'))\n",
    "    \n",
    "    return np.array(r2)\n",
    "\n",
    "# Reusable clustering utility functions\n",
    "\n",
    "def compute_cophenetic_correlation(df, linkage_method, metric='euclidean'):\n",
    "    \"\"\"\n",
    "    Compute the Cophenetic Correlation Coefficient (CCC) for hierarchical clustering.\n",
    "    \n",
    "    The CCC measures how well the dendrogram preserves the original pairwise distances.\n",
    "    Higher values (closer to 1) indicate better preservation of the data structure.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame or np.ndarray\n",
    "        Scaled feature data\n",
    "    linkage_method : str\n",
    "        Linkage method: 'ward', 'complete', 'average', 'single'\n",
    "    metric : str, default='euclidean'\n",
    "        Distance metric\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    float : Cophenetic correlation coefficient (range: -1 to 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute linkage matrix\n",
    "    Z = linkage(df, method=linkage_method, metric=metric)\n",
    "    \n",
    "    # Compute cophenetic distances\n",
    "    c, coph_dists = cophenet(Z, pdist(df, metric=metric))\n",
    "    \n",
    "    return c\n",
    "\n",
    "\n",
    "def plot_linkage_comparison(linkage_results, palette):\n",
    "    \"\"\"\n",
    "    Visualize comparison of linkage methods using CCC scores.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    linkage_results : pd.DataFrame\n",
    "        DataFrame with columns: 'Linkage Method', 'CCC'\n",
    "    palette : list\n",
    "        Color palette for visualization\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars = ax.bar(linkage_results['Linkage Method'], \n",
    "                  linkage_results['CCC'],\n",
    "                  color=palette[:len(linkage_results)],\n",
    "                  edgecolor='black',\n",
    "                  linewidth=1.5,\n",
    "                  alpha=0.85)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    ax.set_ylabel('Cophenetic Correlation Coefficient (CCC)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Linkage Method', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Hierarchical Clustering: Linkage Method Comparison\\nCophenetic Correlation Coefficient (Higher is Better)', \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_ylim(0, 1.05)\n",
    "    ax.grid(False)\n",
    "    ax.legend(loc='lower right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_clustering_metrics(df, labels, algorithm_name=''):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive clustering evaluation metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame or np.ndarray\n",
    "        Scaled feature data\n",
    "    labels : np.ndarray\n",
    "        Cluster labels\n",
    "    algorithm_name : str, optional\n",
    "        Name for display purposes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary with metric names and values\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = {\n",
    "        'Silhouette Score': silhouette_score(df, labels),\n",
    "        'Calinski-Harabasz Index': calinski_harabasz_score(df, labels),\n",
    "        'Davies-Bouldin Index': davies_bouldin_score(df, labels)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_cluster_profiles_heatmap(cluster_profiles, population_mean, palette_continuous, title='Cluster Profiles'):\n",
    "    \"\"\"\n",
    "    Create heatmap visualization of cluster profiles with z-scores.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cluster_profiles : pd.DataFrame\n",
    "        Mean feature values per cluster\n",
    "    population_mean : pd.Series\n",
    "        Population mean for comparison\n",
    "    palette_continuous : matplotlib colormap\n",
    "        Continuous color palette\n",
    "    title : str\n",
    "        Plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    profiles_with_pop = pd.concat([\n",
    "        cluster_profiles.T,\n",
    "        population_mean.rename('Population')\n",
    "    ], axis=1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        profiles_with_pop,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap=palette_continuous,\n",
    "        center=0,\n",
    "        ax=ax,\n",
    "        cbar_kws={'label': 'Z-Score'},\n",
    "        linewidths=0.5,\n",
    "        linecolor='white'\n",
    "    )\n",
    "    \n",
    "    ax.set_title(title, fontweight='bold', fontsize=14, pad=15)\n",
    "    ax.set_ylabel('Features', fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel('Cluster', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cluster_sizes(labels, k, palette, title='Cluster Size Distribution'):\n",
    "    \"\"\"\n",
    "    Visualize cluster size distribution with counts and percentages.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels : np.ndarray\n",
    "        Cluster labels\n",
    "    k : int\n",
    "        Number of clusters\n",
    "    palette : list\n",
    "        Color palette\n",
    "    title : str\n",
    "        Plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    cluster_sizes = pd.Series(labels).value_counts().sort_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    bars = ax.bar(cluster_sizes.index, cluster_sizes.values,\n",
    "                  color=[palette[i % len(palette)] for i in range(k)], linewidth=1.2, alpha=0.85)\n",
    "\n",
    "    total = cluster_sizes.sum()\n",
    "    for i, (idx, count) in enumerate(cluster_sizes.items()):\n",
    "        percentage = (count / total) * 100\n",
    "        ax.text(idx, count + (max(cluster_sizes.values) * 0.02), \n",
    "                f'{count}\\n({percentage:.1f}%)',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=12)\n",
    "    \n",
    "    ax.set_xlabel('Cluster', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Number of Customers', fontsize=12, fontweight='bold')\n",
    "    ax.set_title(title, fontweight='bold', fontsize=13, pad=15)\n",
    "    ax.set_xticks(range(k))\n",
    "    ax.grid(False)\n",
    "\n",
    "    ax.set_ylim(0, max(cluster_sizes.values) * 1.15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_feature_importance(feature_variance, palette, title='Feature Importance: Variance Analysis'):\n",
    "    \"\"\"\n",
    "    Visualize feature importance based on variance across clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_variance : pd.Series\n",
    "        Variance of each feature across clusters (sorted descending)\n",
    "    palette : list\n",
    "        Color palette\n",
    "    title : str\n",
    "        Plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    \n",
    "    ax.bar(range(len(feature_variance)), feature_variance.values, color=palette[0])\n",
    "    ax.set_xticks(range(len(feature_variance)))\n",
    "    ax.set_xticklabels(feature_variance.index, rotation=45, ha='right', fontsize=10)\n",
    "    ax.set_ylabel('Variance Across Clusters', fontweight='bold', fontsize=11)\n",
    "    ax.set_title(title, fontweight='bold', fontsize=13, pad=15)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_cluster_size_comparison(labels_dict, palette, title='Cluster Size Comparison'):\n",
    "    \"\"\"\n",
    "    Compare cluster size distributions across multiple clustering solutions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    labels_dict : dict\n",
    "        Dictionary with format {k_value: labels_array}\n",
    "        e.g., {6: labels_k6, 8: labels_k8}\n",
    "    palette : list\n",
    "        Color palette\n",
    "    title : str\n",
    "        Plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    n_solutions = len(labels_dict)\n",
    "    fig, axes = plt.subplots(1, n_solutions, figsize=(7*n_solutions, 5), sharey=True)\n",
    "    \n",
    "    # If only one solution, axes is not a list\n",
    "    if n_solutions == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Find max cluster size for consistent y-axis\n",
    "    max_size = 0\n",
    "    for labels in labels_dict.values():\n",
    "        cluster_sizes = pd.Series(labels).value_counts()\n",
    "        max_size = max(max_size, cluster_sizes.max())\n",
    "    \n",
    "    # Plot each solution\n",
    "    for idx, (k_value, labels) in enumerate(labels_dict.items()):\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Calculate cluster sizes\n",
    "        cluster_sizes = pd.Series(labels).value_counts().sort_index()\n",
    "        \n",
    "        # Plot bars\n",
    "        ax.bar(cluster_sizes.index, cluster_sizes.values,\n",
    "               color=palette[:len(cluster_sizes)], \n",
    "               linewidth=1.2, alpha=0.85)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, v in enumerate(cluster_sizes.values):\n",
    "            percentage = v/len(labels)*100\n",
    "            ax.text(i, v + (max_size * 0.02), f'{v}\\n({percentage:.1f}%)',\n",
    "                   ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "        \n",
    "        # Styling\n",
    "        ax.set_title(f'k={k_value}', fontweight='bold', fontsize=14, pad=10)\n",
    "        ax.set_xlabel('Cluster', fontweight='bold', fontsize=12)\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('Number of Customers', fontweight='bold', fontsize=12)\n",
    "        ax.grid(False)\n",
    "        ax.set_ylim(0, max_size * 1.15)\n",
    "        ax.set_xticks(range(len(cluster_sizes)))\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_elbow_method(k_range, inertia_values, palette, title='Elbow Method: Optimal k Selection'):\n",
    "    \"\"\"\n",
    "    Visualize the Elbow Method plot for K-Means clustering.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    k_range : range or list\n",
    "        Range of k values tested\n",
    "    inertia_values : list\n",
    "        Inertia (WCSS) values for each k\n",
    "    palette : list\n",
    "        Color palette\n",
    "    title : str\n",
    "        Plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(k_range, inertia_values, marker='o', linewidth=2.5, markersize=8,\n",
    "            color=palette[2], markerfacecolor=palette[3])\n",
    "    ax.set_xticks(k_range)\n",
    "    ax.set_ylabel(\"Inertia (Within-Cluster Sum of Squares)\", fontsize=11, fontweight='bold')\n",
    "    ax.set_xlabel(\"Number of Clusters (k)\", fontsize=11, fontweight='bold')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.grid(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_clustering_metrics(metrics_df, k_range, palette, title='Clustering Metrics Evaluation'):\n",
    "    \"\"\"\n",
    "    Visualize clustering evaluation metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics_df : pd.DataFrame\n",
    "        DataFrame with columns: k, Silhouette, Calinski-Harabasz, Davies-Bouldin\n",
    "    k_range : range or list\n",
    "        Range of k values\n",
    "    palette : list\n",
    "        Color palette\n",
    "    title : str\n",
    "        Plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    None (displays plot)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    \n",
    "    # Plot 1: Silhouette Score (maximize)\n",
    "    axes[0].plot(metrics_df['k'], metrics_df['Silhouette'],\n",
    "                 marker='o', linewidth=2.5, markersize=8, color=palette[0])\n",
    "    max_sil_k = metrics_df.loc[metrics_df['Silhouette'].idxmax(), 'k']\n",
    "    axes[0].axvline(x=max_sil_k, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    axes[0].set_xlabel('Number of Clusters (k)', fontweight='bold', fontsize=11)\n",
    "    axes[0].set_ylabel('Silhouette Score', fontweight='bold', fontsize=11)\n",
    "    axes[0].set_title('Silhouette Score (Higher is Better)', fontweight='bold', fontsize=12)\n",
    "    axes[0].grid(False)\n",
    "    axes[0].set_xticks(k_range)\n",
    "    \n",
    "    # Plot 2: Calinski-Harabasz Index (maximize)\n",
    "    axes[1].plot(metrics_df['k'], metrics_df['Calinski-Harabasz'],\n",
    "                 marker='o', linewidth=2.5, markersize=8, color=palette[1])\n",
    "    max_ch_k = metrics_df.loc[metrics_df['Calinski-Harabasz'].idxmax(), 'k']\n",
    "    axes[1].axvline(x=max_ch_k, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    axes[1].set_xlabel('Number of Clusters (k)', fontweight='bold', fontsize=11)\n",
    "    axes[1].set_ylabel('Calinski-Harabasz Index', fontweight='bold', fontsize=11)\n",
    "    axes[1].set_title('Calinski-Harabasz Index (Higher is Better)', fontweight='bold', fontsize=12)\n",
    "    axes[1].grid(False)\n",
    "    axes[1].set_xticks(k_range)\n",
    "    \n",
    "    # Plot 3: Davies-Bouldin Index (minimize)\n",
    "    axes[2].plot(metrics_df['k'], metrics_df['Davies-Bouldin'],\n",
    "                 marker='o', linewidth=2.5, markersize=8, color=palette[2])\n",
    "    min_db_k = metrics_df.loc[metrics_df['Davies-Bouldin'].idxmin(), 'k']\n",
    "    axes[2].axvline(x=min_db_k, color='red', linestyle='--', alpha=0.5, linewidth=2)\n",
    "    axes[2].set_xlabel('Number of Clusters (k)', fontweight='bold', fontsize=11)\n",
    "    axes[2].set_ylabel('Davies-Bouldin Index', fontweight='bold', fontsize=11)\n",
    "    axes[2].set_title('Davies-Bouldin Index (Lower is Better)', fontweight='bold', fontsize=12)\n",
    "    axes[2].grid(False)\n",
    "    axes[2].set_xticks(k_range)\n",
    "    \n",
    "    fig.suptitle(title, fontsize=15, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_linkage_comparison(df, linkage_methods, palette, title='Linkage Method Comparison'):\n",
    "    \"\"\"\n",
    "    Compare hierarchical clustering linkage methods using CCC and R² metrics.\n",
    "    \n",
    "    Creates a side-by-side visualization showing:\n",
    "    - Left: CCC bar chart (dendrogram preservation quality)\n",
    "    - Right: R² line chart (variance explained across k values)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Scaled feature dataset\n",
    "    linkage_methods : list\n",
    "        List of linkage method names (e.g., ['ward', 'complete', 'average', 'single'])\n",
    "    palette : list\n",
    "        Color palette - each method gets consistent color across both plots\n",
    "    title : str\n",
    "        Overall plot title\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (ccc_df, r2_results_all)\n",
    "        - ccc_df: DataFrame with CCC values\n",
    "        - r2_results_all: List of R² arrays for each method\n",
    "    \"\"\"\n",
    "    # Define consistent color mapping for linkage methods\n",
    "    method_colors = {\n",
    "        'ward': palette[0],\n",
    "        'complete': palette[1],\n",
    "        'average': palette[2],\n",
    "        'single': palette[3]\n",
    "    }\n",
    "    \n",
    "    # Compute CCC for each linkage method\n",
    "    ccc_results = []\n",
    "    for method in linkage_methods:\n",
    "        ccc = compute_cophenetic_correlation(df, method, metric='euclidean')\n",
    "        ccc_results.append({'Linkage Method': method.capitalize(), 'CCC': ccc})\n",
    "    \n",
    "    # Create CCC comparison DataFrame\n",
    "    ccc_df = pd.DataFrame(ccc_results)\n",
    "    \n",
    "    # Compute R² for each linkage method\n",
    "    r2_results_all = []\n",
    "    for method in linkage_methods:\n",
    "        r2_values = get_r2_hc(df, link_method=method, max_nclus=10, min_nclus=2)\n",
    "        r2_results_all.append(r2_values)\n",
    "    \n",
    "    # Create side-by-side visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: CCC Bar Chart (preserve original order for consistent colors)\n",
    "    ax1 = axes[0]\n",
    "    bar_colors = [method_colors[method] for method in linkage_methods]\n",
    "    bars = ax1.bar(ccc_df['Linkage Method'], ccc_df['CCC'], \n",
    "                   color=bar_colors, alpha=0.85, linewidth=1.2)\n",
    "    ax1.set_ylabel('Cophenetic Correlation Coefficient', fontweight='bold', fontsize=12)\n",
    "    ax1.set_xlabel('Linkage Method', fontweight='bold', fontsize=12)\n",
    "    ax1.set_title('CCC: Dendrogram Preservation Quality\\n(Higher = Better)', \n",
    "                  fontweight='bold', fontsize=13, pad=15)\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.grid(False)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Plot 2: R² Line Chart (same colors as bar chart)\n",
    "    ax2 = axes[1]\n",
    "    k_values = list(range(2, 11))\n",
    "    for idx, method in enumerate(linkage_methods):\n",
    "        ax2.plot(k_values, r2_results_all[idx], marker='o', linewidth=2, \n",
    "                 markersize=7, label=method.capitalize(), color=method_colors[method])\n",
    "    \n",
    "    ax2.set_xlabel('Number of Clusters (k)', fontweight='bold', fontsize=12)\n",
    "    ax2.set_ylabel('R² (Variance Explained)', fontweight='bold', fontsize=12)\n",
    "    ax2.set_title('R²: Clustering Variance Explained\\n(Higher = Better)', \n",
    "                  fontweight='bold', fontsize=13, pad=15)\n",
    "    ax2.set_xticks(k_values)\n",
    "    ax2.grid(False)\n",
    "    ax2.legend(title='Linkage Method', fontsize=10, title_fontsize=11)\n",
    "    ax2.set_ylim(0, 1.0)\n",
    "    \n",
    "    plt.suptitle(title, fontsize=15, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return ccc_df, r2_results_all\n",
    "\n",
    "def plot_meanshift_quantile_vs_clusters(\n",
    "    ms_results_df: pd.DataFrame,\n",
    "    title: str = \"Mean Shift: Quantile vs Number of Clusters\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the relationship between Mean Shift bandwidth quantile and the resulting number of clusters.\n",
    "\n",
    "    This visualization helps identify:\n",
    "    - Regimes where the clustering solution is stable (plateaus in n_clusters)\n",
    "    - Thresholds where clusters merge rapidly as the bandwidth increases\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ms_results_df : pd.DataFrame\n",
    "        DataFrame containing Mean Shift evaluation results with at least:\n",
    "        - 'quantile' (float): bandwidth quantile used in estimate_bandwidth()\n",
    "        - 'n_clusters' (int): number of clusters produced by MeanShift for that bandwidth\n",
    "    title : str, optional\n",
    "        Plot title.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Displays the plot.\n",
    "    \"\"\"\n",
    "    dfp = ms_results_df.sort_values(\"quantile\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(dfp[\"quantile\"], dfp[\"n_clusters\"], marker=\"o\", linewidth=2)\n",
    "\n",
    "    ymin = int(dfp[\"n_clusters\"].min())\n",
    "    ymax = int(dfp[\"n_clusters\"].max())\n",
    "    ax.set_yticks(list(range(ymin, ymax + 1, 1)))\n",
    "\n",
    "    ax.set_xlabel(\"Bandwidth Quantile\", fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Number of Clusters\", fontweight=\"bold\")\n",
    "    ax.set_title(title, fontweight=\"bold\", pad=12)\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_gmm_grid(\n",
    "    X: pd.DataFrame,\n",
    "    feats: list,\n",
    "    n_components_list: list,\n",
    "    covariance_types: list,\n",
    "    random_state: int = 1,\n",
    "    n_init: int = 5,\n",
    "    max_iter: int = 500\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Grid-evaluate Gaussian Mixture Models (GMM) across n_components and covariance_type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Scaled feature matrix (e.g., df_demographic_a_scaled[feats]).\n",
    "    feats : list[str]\n",
    "        Feature column names (needed for R² calculation via get_rsq).\n",
    "    n_components_list : list[int]\n",
    "        Candidate numbers of mixture components.\n",
    "    covariance_types : list[str]\n",
    "        Candidate covariance types: [\"full\", \"tied\", \"diag\", \"spherical\"].\n",
    "    random_state : int, default=1\n",
    "        Random seed for reproducibility.\n",
    "    n_init : int, default=5\n",
    "        Number of initializations to reduce local optimum risk.\n",
    "    max_iter : int, default=500\n",
    "        Maximum EM iterations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        One row per configuration with:\n",
    "        n_components, covariance_type, n_clusters, BIC, AIC, R2, Silhouette\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    X_np = X[feats].values\n",
    "\n",
    "    for cov in covariance_types:\n",
    "        for k in n_components_list:\n",
    "            gmm = GaussianMixture(\n",
    "                n_components=k,\n",
    "                covariance_type=cov,\n",
    "                random_state=random_state,\n",
    "                init_params=\"kmeans\",\n",
    "                n_init=n_init\n",
    "            )\n",
    "\n",
    "            labels = gmm.fit_predict(X_np)\n",
    "            n_clusters = len(np.unique(labels))\n",
    "\n",
    "            # R² (your helper)\n",
    "            df_tmp = X[feats].copy()\n",
    "            df_tmp[\"labels\"] = labels\n",
    "            r2 = get_rsq(df_tmp, feats, \"labels\")\n",
    "\n",
    "            # Silhouette (only defined if >= 2 clusters)\n",
    "            sil = silhouette_score(X_np, labels) if n_clusters >= 2 else np.nan\n",
    "\n",
    "            results.append({\n",
    "                \"n_components\": int(k),\n",
    "                \"covariance_type\": cov,\n",
    "                \"n_clusters\": int(n_clusters),\n",
    "                \"BIC\": float(gmm.bic(X_np)),\n",
    "                \"AIC\": float(gmm.aic(X_np)),\n",
    "                \"R2\": float(r2),\n",
    "                \"Silhouette\": float(sil) if not np.isnan(sil) else np.nan\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def plot_gmm_covtype_bic_aic(gmm_results_df, gmm_cov_types, gmm_n_components):\n",
    "    \"\"\"\n",
    "    Compare covariance_type options using BIC and AIC side-by-side (1x2).\n",
    "    Each line = one covariance_type across n_components.\n",
    "\n",
    "    Use this to pick the best covariance_type family (primarily by lowest BIC/AIC).\n",
    "    \"\"\"\n",
    "    dfp = gmm_results_df.copy()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharex=True)\n",
    "\n",
    "    for cov in gmm_cov_types:\n",
    "        d = dfp[dfp[\"covariance_type\"] == cov].sort_values(\"n_components\")\n",
    "        axes[0].plot(d[\"n_components\"], d[\"BIC\"], marker=\"o\", linewidth=2, label=cov)\n",
    "        axes[1].plot(d[\"n_components\"], d[\"AIC\"], marker=\"o\", linewidth=2, label=cov)\n",
    "\n",
    "    axes[0].set_title(\"BIC by covariance_type (Lower is Better)\", fontweight=\"bold\", pad=10)\n",
    "    axes[1].set_title(\"AIC by covariance_type (Lower is Better)\", fontweight=\"bold\", pad=10)\n",
    "\n",
    "    for ax, yl in zip(axes, [\"BIC\", \"AIC\"]):\n",
    "        ax.set_xlabel(\"n_components\", fontweight=\"bold\")\n",
    "        ax.set_ylabel(yl, fontweight=\"bold\")\n",
    "        ax.set_xticks(gmm_n_components)\n",
    "        ax.grid(False)\n",
    "        ax.legend(title=\"covariance_type\", fontsize=9, title_fontsize=9, loc=\"lower left\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_gmm_n_selection_for_covtype(gmm_results_df, chosen_covariance_type, gmm_n_components):\n",
    "    \"\"\"\n",
    "    After selecting a winning covariance_type, plot n_components selection in 3 charts:\n",
    "    1) BIC + AIC together\n",
    "    2) R²\n",
    "    3) Silhouette\n",
    "\n",
    "    Legend:\n",
    "    - BIC/AIC and Silhouette: lower left\n",
    "    - R²: lower right\n",
    "    \"\"\"\n",
    "    dfc = (gmm_results_df[gmm_results_df[\"covariance_type\"] == chosen_covariance_type]\n",
    "           .sort_values(\"n_components\")\n",
    "           .copy())\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5), sharex=True)\n",
    "\n",
    "    # 1) BIC + AIC in one chart\n",
    "    axes[0].plot(dfc[\"n_components\"], dfc[\"BIC\"], marker=\"o\", linewidth=2, label=\"BIC\")\n",
    "    axes[0].plot(dfc[\"n_components\"], dfc[\"AIC\"], marker=\"o\", linewidth=2, label=\"AIC\")\n",
    "    axes[0].set_title(f\"{chosen_covariance_type}: BIC & AIC (Lower is Better)\", fontweight=\"bold\", pad=10)\n",
    "    axes[0].set_ylabel(\"Score\", fontweight=\"bold\")\n",
    "    axes[0].legend(loc=\"lower left\", fontsize=9)\n",
    "\n",
    "    # 2) R²\n",
    "    axes[1].plot(dfc[\"n_components\"], dfc[\"R2\"], marker=\"o\", linewidth=2, label=\"R²\")\n",
    "    axes[1].set_title(f\"{chosen_covariance_type}: R² (Higher is Better)\", fontweight=\"bold\", pad=10)\n",
    "    axes[1].set_ylabel(\"R²\", fontweight=\"bold\")\n",
    "    axes[1].legend(loc=\"lower right\", fontsize=9)\n",
    "\n",
    "    # 3) Silhouette\n",
    "    axes[2].plot(dfc[\"n_components\"], dfc[\"Silhouette\"], marker=\"o\", linewidth=2, label=\"Silhouette\")\n",
    "    axes[2].set_title(f\"{chosen_covariance_type}: Silhouette (Higher is Better)\", fontweight=\"bold\", pad=10)\n",
    "    axes[2].set_ylabel(\"Silhouette\", fontweight=\"bold\")\n",
    "    axes[2].legend(loc=\"lower left\", fontsize=9)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel(\"n_components\", fontweight=\"bold\")\n",
    "        ax.set_xticks(gmm_n_components)\n",
    "        ax.grid(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_som_grid(som_model, values, plot_title, fig_width=9, fig_height=6, ax=None):\n",
    "    \"\"\"\n",
    "    Visualize SOM grid with hexagonal cells.\n",
    "    \n",
    "    Parameters:\n",
    "    - ax: Optional matplotlib axis. If None, creates new figure.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "        show_plot = True\n",
    "    else:\n",
    "        show_plot = False\n",
    "    \n",
    "    color_norm = Normalize(vmin=values.min(), vmax=values.max())\n",
    "    \n",
    "    for row_idx in range(values.shape[0]):\n",
    "        for col_idx in range(values.shape[1]):\n",
    "            x, y = som_model.convert_map_to_euclidean((row_idx, col_idx))\n",
    "            ax.add_patch(RegularPolygon(\n",
    "                (x, y), numVertices=6, radius=np.sqrt(1/3),\n",
    "                facecolor=cm.RdYlBu_r(color_norm(values[row_idx, col_idx])),\n",
    "                edgecolor='white', linewidth=0.5\n",
    "            ))\n",
    "    \n",
    "    ax.set_xlim(-1, values.shape[1])\n",
    "    ax.set_ylim(-1, values.shape[0])\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(plot_title, fontsize=13 if show_plot else 11, fontweight='bold', pad=15 if show_plot else 0)\n",
    "    \n",
    "    sm = mpl.cm.ScalarMappable(cmap=cm.RdYlBu_r, norm=color_norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "    if show_plot:\n",
    "        cbar.set_label('Value', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    if show_plot:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25b39e",
   "metadata": {},
   "source": [
    "## **7.1 Hierarchical Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b8ffd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Hierarchical Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Hierarchical clustering is an <strong>agglomerative bottom-up algorithm</strong> that builds a tree-like structure (dendrogram) by iteratively merging the closest data points or clusters. Unlike K-Means, it does not require pre-specifying the number of clusters and provides a complete hierarchical view of data relationships.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialization:</strong> Start with each data point as its own cluster (n clusters for n points)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Distance Calculation:</strong> Compute pairwise distances between all clusters using a linkage criterion</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Merge Step:</strong> Iteratively merge the two closest clusters into one larger cluster</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Repeat:</strong> Continue until all points are merged into a single cluster, forming a hierarchical tree (dendrogram)</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Linkage Method Selection:</strong> Test Ward, Complete, Average, and Single linkage methods using CCC (Cophenetic Correlation Coefficient) and R² (Variance Explained)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Optimal k Selection:</strong> For the best linkage method (Ward), identify optimal k using Silhouette, Calinski-Harabasz, Davies-Bouldin indices and dendrogram visual inspection</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Solution Comparison:</strong> Compare k=3 vs k=7 solutions by examining cluster size distributions and feature profiles</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model & Profiling:</strong> Fit final Ward linkage model with k=3 and analyze demographic characteristics of each cluster</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7474c09e",
   "metadata": {},
   "source": [
    "### **7.1.1 Finding the best Linkage Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fe817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linkage methods using two complementary metrics:\n",
    "# 1. CCC (Cophenetic Correlation Coefficient)\n",
    "# 2. R²\n",
    "\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "# Use plot_linkage_comparison\n",
    "ccc_df, r2_results_all = plot_linkage_comparison(\n",
    "    df=df_demographic_a_scaled,\n",
    "    linkage_methods=linkage_methods,\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='Linkage Method Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f317a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Linkage Method Selection Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Compared four linkage methods (Ward, Complete, Average, Single) using dendrogram quality and clustering performance metrics to identify the optimal approach for hierarchical clustering.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Determine which linkage criterion best preserves hierarchical structure while maximizing clustering quality</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Linkage Methods Tested:</strong> Ward (minimizes within-cluster variance), Complete (maximum distance), Average (mean distance), Single (minimum distance)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>CCC (Cophenetic Correlation Coefficient):</strong> Measures how faithfully the dendrogram preserves pairwise distances (higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>R² (Variance Explained):</strong> Proportion of variance explained by clustering across k=2-10 (higher is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>CCC Results:</strong> Average (0.680) achieves highest dendrogram preservation, followed by Complete (0.596), Ward (0.578), and Single (0.527)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>R² Results:</strong> Ward consistently explains the most variance across all k values, outperforming other methods</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Decision:</strong> Selected Ward linkage - while Average has better CCC, Ward's superior R² indicates it creates more compact, well-separated clusters that better capture the underlying data structure for customer segmentation</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d4aad3",
   "metadata": {},
   "source": [
    "### **7.1.2 Defining the number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc32331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal k using multiple evaluation metrics\n",
    "# Ward linkage selected from previous CCC analysis\n",
    "\n",
    "k_range = range(2, 11)\n",
    "hc_metrics = {\n",
    "    'k': [],\n",
    "    'Silhouette': [],\n",
    "    'Calinski-Harabasz': [],\n",
    "    'Davies-Bouldin': []\n",
    "}\n",
    "\n",
    "for k in k_range:\n",
    "    hc = AgglomerativeClustering(n_clusters=k, linkage='ward', metric='euclidean')\n",
    "    labels = hc.fit_predict(df_demographic_a_scaled)\n",
    "    \n",
    "    metrics = evaluate_clustering_metrics(df_demographic_a_scaled, labels)\n",
    "    \n",
    "    hc_metrics['k'].append(k)\n",
    "    hc_metrics['Silhouette'].append(metrics['Silhouette Score'])\n",
    "    hc_metrics['Calinski-Harabasz'].append(metrics['Calinski-Harabasz Index'])\n",
    "    hc_metrics['Davies-Bouldin'].append(metrics['Davies-Bouldin Index'])\n",
    "    \n",
    "\n",
    "# Create metrics DataFrame\n",
    "hc_metrics_df = pd.DataFrame(hc_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e95e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering metrics\n",
    "plot_clustering_metrics(\n",
    "    hc_metrics_df,\n",
    "    k_range,\n",
    "    CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering: Optimal k Evaluation (Ward Linkage)'\n",
    ")\n",
    "\n",
    "# Display metrics table\n",
    "hc_metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf61072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogram visualization for visual confirmation of cluster structure\n",
    "\n",
    "linkage_matrix = linkage(df_demographic_a_scaled, method='ward', metric='euclidean')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    ax=ax,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,\n",
    "    leaf_font_size=10,\n",
    "    show_leaf_counts=True,\n",
    "    color_threshold=0.7*max(linkage_matrix[:,2])\n",
    ")\n",
    "\n",
    "ax.set_title('Hierarchical Clustering Dendrogram (Ward Linkage)\\nVisual Confirmation of Natural Grouping Structure', \n",
    "             fontweight='bold', fontsize=14, pad=15)\n",
    "ax.set_xlabel('Sample Index or Cluster Size', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Euclidean Distance', fontsize=11, fontweight='bold')\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb06fa",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Optimal k Selection Summary (Hierarchical Clustering)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Multi-metric approach combining internal validation indices and dendrogram visual inspection to determine optimal k for hierarchical clustering with Ward linkage.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Identify the number of clusters (k) that maximizes cluster quality while maintaining business interpretability</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Measures how similar data points are to their own cluster vs. neighboring clusters (range -1 to 1, higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster to within-cluster variance (higher values indicate better-defined clusters)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Index:</strong> Average similarity between each cluster and its most similar cluster (lower values indicate better separation)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Dendrogram Analysis:</strong> Visual inspection of hierarchical tree structure to identify natural cluster boundaries</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Increases steadily from k=2 (0.186) to k=10 (0.214), with notable improvement at k=7 (0.200)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Peak:</strong> Maximum at k=3 (2779), then declining; k=7 (2176) still maintains reasonable separation</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Minimum:</strong> Best separation at k=10 (1.55); k=4 (1.74) and k=7 (1.70) show good mid-range performance</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate Selection:</strong> k=3 and k=7 emerge as candidates - k=3 achieves highest Calinski-Harabasz (2779) indicating strong cluster separation, while k=7 provides better Silhouette (0.200) with improved DBI (1.70) for more granular segmentation</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b179a",
   "metadata": {},
   "source": [
    "### **7.1.3 Comparison of Clustering Solutions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f19295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate solutions based on clustering metrics analysis\n",
    "# k=3\n",
    "# k=7\n",
    "\n",
    "hc_k_candidate_1 = 3\n",
    "hc_k_candidate_2 = 7\n",
    "\n",
    "# Fit both candidate solutions\n",
    "hc_k1 = AgglomerativeClustering(n_clusters=hc_k_candidate_1, linkage='ward', metric='euclidean')\n",
    "hc_k2 = AgglomerativeClustering(n_clusters=hc_k_candidate_2, linkage='ward', metric='euclidean')\n",
    "\n",
    "hc_labels_k1 = hc_k1.fit_predict(df_demographic_a_scaled)\n",
    "hc_labels_k2 = hc_k2.fit_predict(df_demographic_a_scaled)\n",
    "\n",
    "# Create temporary DataFrames with cluster labels\n",
    "df_temp_k1 = df_demographic_a_scaled.copy()\n",
    "df_temp_k1['Cluster'] = hc_labels_k1\n",
    "\n",
    "df_temp_k2 = df_demographic_a_scaled.copy()\n",
    "df_temp_k2['Cluster'] = hc_labels_k2\n",
    "\n",
    "# Calculate cluster profiles (mean values per cluster)\n",
    "cluster_profiles_k1 = df_temp_k1.groupby('Cluster').mean()\n",
    "cluster_profiles_k2 = df_temp_k2.groupby('Cluster').mean()\n",
    "\n",
    "# Display both profiles for comparison\n",
    "print(f\"\\nCluster Profiles for k={hc_k_candidate_1}:\")\n",
    "display(cluster_profiles_k1.round(3))\n",
    "\n",
    "print(f\"\\n\\nCluster Profiles for k={hc_k_candidate_2}:\")\n",
    "display(cluster_profiles_k2.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={hc_k_candidate_1: hc_labels_k1, hc_k_candidate_2: hc_labels_k2},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering: Cluster Size Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae52fe6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Compared k=3 and k=7 candidate solutions to determine the optimal granularity for customer segmentation.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: k=3 selected</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K=3 achieves the highest Calinski-Harabasz Index (2779), indicating optimal between-cluster separation. While k=7 offers better Silhouette (0.200 vs 0.190) and DBI (1.70 vs 1.83) scores, the additional clusters in k=7 differentiate only on a few features with many features remaining similar across clusters. K=3 provides more distinct, interpretable segments with balanced cluster sizes (33.1%, 27.7%, 39.2%) where each cluster has a clearer demographic profile. This enables actionable marketing strategies without over-segmentation.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161bd01e",
   "metadata": {},
   "source": [
    "### **7.1.4 Final Hierarchical Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e766454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Hierarchical Clustering Solution\n",
    "# Using k=3 from comparison analysis\n",
    "\n",
    "hc_final_k = hc_k_candidate_1  # k=3\n",
    "\n",
    "# Reuse labels from comparison step\n",
    "hc_labels_final = hc_labels_k1\n",
    "\n",
    "# Create labeled dataset\n",
    "df_demographic_a_scaled_labeled = df_demographic_a_scaled.copy()\n",
    "df_demographic_a_scaled_labeled['Cluster'] = hc_labels_final\n",
    "\n",
    "# Calculate final metrics\n",
    "final_metrics = evaluate_clustering_metrics(df_demographic_a_scaled, hc_labels_final)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "if 'demo_clustering_results' not in dir():\n",
    "    demo_clustering_results = {}\n",
    "\n",
    "demo_clustering_results['Hierarchical'] = {\n",
    "    'k': hc_final_k,\n",
    "    'Silhouette': final_metrics['Silhouette Score'],\n",
    "    'Calinski-Harabasz': final_metrics['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': final_metrics['Davies-Bouldin Index'],\n",
    "    'R2': get_rsq(df_demographic_a_scaled_labeled, df_demographic_a_scaled.columns.tolist(), 'Cluster'),\n",
    "    'labels': hc_labels_final\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d2a24c",
   "metadata": {},
   "source": [
    "### **7.1.5 Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068f89d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cluster Profiles Heatmap - Z-scores of demographic features per cluster\n",
    "feats = df_demographic_a_scaled.columns.tolist()\n",
    "hc_cluster_profiles = df_demographic_a_scaled_labeled.groupby('Cluster')[feats].mean()\n",
    "population_mean = df_demographic_a_scaled[feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    hc_cluster_profiles, \n",
    "    population_mean, \n",
    "    GROUP80_palette_continuous,\n",
    "    title='Hierarchical Clustering: Demographic Profiles (k=3)\\nStandardized Z-Scores per Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecd5ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    hc_labels_final, \n",
    "    hc_final_k, \n",
    "    CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering - Final Cluster Sizes'\n",
    ")\n",
    "\n",
    "# Display cluster size statistics\n",
    "cluster_sizes = pd.Series(hc_labels_final).value_counts().sort_index()\n",
    "cluster_dist_df = pd.DataFrame({\n",
    "    'Cluster': cluster_sizes.index,\n",
    "    'Count': cluster_sizes.values,\n",
    "    'Percentage': (cluster_sizes.values / len(hc_labels_final) * 100).round(2)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e0157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "# Features with high variance differentiate clusters most effectively\n",
    "\n",
    "hc_feature_variance = hc_cluster_profiles.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    hc_feature_variance,\n",
    "    CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering: Feature Importance Analysis\\nWhich Demographics Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance ranking\n",
    "hc_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': hc_feature_variance.index,\n",
    "    'Variance': hc_feature_variance.values.round(4),\n",
    "})\n",
    "\n",
    "hc_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d9bda",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Hierarchical Clustering Profiling Summary (k=3)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed demographic characteristics of the final 3 hierarchical clusters to identify distinct customer segments. Ward linkage produces well-separated clusters with Education, Income, and City as primary differentiators.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (33.1%, n=4,314) - Common Regions, Higher-Income Higher-Education:</strong> Common cities (Z=+0.98), common FSA regions (Z=+0.74), common provinces (Z=+0.47), higher education (Z=+0.30), higher income (Z=+0.45).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (27.7%, n=3,610) - Lower-Income Lower-Education:</strong> Significantly lower education (Z=-1.35), significantly lower income (Z=-1.14), average geographic frequency, lower married rate (Z=+0.38).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (39.2%, n=5,114) - Rare Regions, Higher-Education:</strong> Rare provinces (Z=-0.38), rare cities (Z=-0.77), rare FSA regions (Z=-0.45), higher education (Z=+0.70), higher income (Z=+0.43).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Segmentation Driver - Education (Variance: 1.19):</strong> Education creates the clearest separation between clusters, with Cluster 1 showing significantly lower education levels.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Secondary Drivers - Income (Variance: 0.83) & City (Variance: 0.77):</strong> Income strongly correlates with education, while city frequency differentiates mainstream (Cluster 0) from niche geographic segments.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Gender Independence (Variance: 0.0003):</strong> Gender shows virtually no differentiation across hierarchical clusters.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89596cea",
   "metadata": {},
   "source": [
    "## **7.2 K-Means Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303c542",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">K-Means Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K-Means is an <strong>iterative partitioning algorithm</strong> that assigns data points to k clusters by minimizing within-cluster variance (Sum of Squared Errors). Unlike hierarchical clustering, it requires pre-specifying k and uses a centroid-based approach to create spherical, compact clusters.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Choose Seeds:</strong> Select k initial centroids (using k-means++ for better starting positions)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Assignment:</strong> Associate each data point with the nearest seed/centroid based on Euclidean distance</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Update Centroids:</strong> Calculate the centroids of the formed clusters as the mean of all assigned points</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Iterate:</strong> Go back to step 2 and repeat until centroids cease to be recentered (convergence)</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Optimal k Selection:</strong> Test k=2-13 using Elbow Method (Inertia/SSE), Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Solution Comparison:</strong> Compare k=3 vs k=7 solutions by examining cluster sizes and feature profiles</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model & Profiling:</strong> Fit final K-Means model with k=3 (k-means++ initialization) and analyze demographic characteristics of each cluster</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992b5c80",
   "metadata": {},
   "source": [
    "### **7.2.1 Defining the number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067fbc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Means clustering across range of k values\n",
    "# Using k-means++ initialization and multiple runs for stability\n",
    "\n",
    "km_k_range = range(2, 14)\n",
    "km_metrics = {\n",
    "    'k': [],\n",
    "    'Inertia': [],\n",
    "    'Silhouette': [],\n",
    "    'Calinski-Harabasz': [],\n",
    "    'Davies-Bouldin': []\n",
    "}\n",
    "\n",
    "# Store labels and silhouette samples for visualization\n",
    "km_fitted_labels = {}\n",
    "km_silhouette_samples = {}\n",
    "\n",
    "for k in km_k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=15, random_state=42, max_iter=300)\n",
    "    km_labels = kmeans.fit_predict(df_demographic_a_scaled)\n",
    "    \n",
    "    # Store labels and silhouette samples\n",
    "    km_fitted_labels[k] = km_labels\n",
    "    km_silhouette_samples[k] = silhouette_samples(df_demographic_a_scaled, km_labels)\n",
    "    \n",
    "    metrics = evaluate_clustering_metrics(df_demographic_a_scaled, km_labels)\n",
    "    \n",
    "    km_metrics['k'].append(k)\n",
    "    km_metrics['Inertia'].append(kmeans.inertia_)\n",
    "    km_metrics['Silhouette'].append(metrics['Silhouette Score'])\n",
    "    km_metrics['Calinski-Harabasz'].append(metrics['Calinski-Harabasz Index'])\n",
    "    km_metrics['Davies-Bouldin'].append(metrics['Davies-Bouldin Index'])\n",
    "\n",
    "km_metrics_df = pd.DataFrame(km_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Elbow Method\n",
    "plot_elbow_method(\n",
    "    km_k_range,\n",
    "    km_metrics_df['Inertia'].tolist(),\n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Elbow Method: Optimal k Selection'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2754b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Analysis\n",
    "\n",
    "for nclus in km_k_range:\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    km_labels = km_fitted_labels[nclus]\n",
    "    sample_silhouette_values = km_silhouette_samples[nclus]\n",
    "    silhouette_avg = km_metrics_df[km_metrics_df['k'] == nclus]['Silhouette'].values[0]\n",
    "    \n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[km_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        color = CUSTOM_HEX[i % len(CUSTOM_HEX)]\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, ith_cluster_silhouette_values,\n",
    "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontweight='bold', fontsize=11)\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax.set_title(f\"K-Means Silhouette Analysis for k={nclus}\\nAverage Silhouette Score: {silhouette_avg:.4f}\", \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel(\"Silhouette Coefficient Values\", fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(\"Cluster\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=2.5, \n",
    "               label=f'Average: {silhouette_avg:.4f}')\n",
    "    \n",
    "    xmin = max(-0.3, np.round(sample_silhouette_values.min() - 0.1, 2))\n",
    "    xmax = min(1.0, np.round(sample_silhouette_values.max() + 0.1, 2))\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([0, len(df_demographic_a_scaled) + (nclus + 1) * 10])\n",
    "    \n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(np.arange(xmin, xmax + 0.1, 0.1))\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56786738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering metrics\n",
    "plot_clustering_metrics(\n",
    "    km_metrics_df,\n",
    "    km_k_range,\n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Clustering: Optimal k Evaluation'\n",
    ")\n",
    "\n",
    "# Display metrics table\n",
    "km_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5175ef5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Optimal k Selection Summary (K-Means Clustering)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Evaluated k=2 to k=13 using multiple validation indices to identify the optimal number of clusters for K-Means partitioning of airline customer demographics.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Determine k that balances cluster quality metrics with business interpretability for actionable customer segmentation</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Elbow Method (Inertia):</strong> Total within-cluster sum of squared distances - look for \"elbow\" where adding clusters yields diminishing returns</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Measures cluster cohesion and separation (range -1 to 1, higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster to within-cluster variance (higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Index:</strong> Average similarity between clusters (lower is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>k=3 - Statistical Optimum:</strong> Clear elbow in Inertia curve, peak Calinski-Harabasz (3071), good Silhouette (0.203), DBI (1.74).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>k=5 - Balanced Solution:</strong> Good Inertia reduction, good DBI (1.59), solid Silhouette (0.207), reasonable CH (2594).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate Selection:</strong> k=3 vs k=5 - k=3 provides statistically optimal broad segments, k=5 offers better cluster separation (lowest DBI) with moderate granularity.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5435f1",
   "metadata": {},
   "source": [
    "### **7.2.2 Comparison of Clustering Solutions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e9c4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate solutions based on clustering metrics analysis\n",
    "# k=3\n",
    "# k=5\n",
    "\n",
    "km_k_candidate_1 = 3\n",
    "km_k_candidate_2 = 5\n",
    "\n",
    "# Use pre-fitted labels from 7.2.1\n",
    "km_labels_k1 = km_fitted_labels[km_k_candidate_1]\n",
    "km_labels_k2 = km_fitted_labels[km_k_candidate_2]\n",
    "\n",
    "# Create temporary DataFrames with cluster labels\n",
    "df_temp_k1 = df_demographic_a_scaled.copy()\n",
    "df_temp_k1['Cluster'] = km_labels_k1\n",
    "\n",
    "df_temp_k2 = df_demographic_a_scaled.copy()\n",
    "df_temp_k2['Cluster'] = km_labels_k2\n",
    "\n",
    "# Calculate cluster profiles (mean values per cluster)\n",
    "cluster_profiles_k1 = df_temp_k1.groupby('Cluster').mean()\n",
    "cluster_profiles_k2 = df_temp_k2.groupby('Cluster').mean()\n",
    "\n",
    "# Display both profiles for comparison\n",
    "print(f\"\\nCluster Profiles for k={km_k_candidate_1}:\")\n",
    "display(cluster_profiles_k1.round(3))\n",
    "\n",
    "print(f\"\\n\\nCluster Profiles for k={km_k_candidate_2}:\")\n",
    "display(cluster_profiles_k2.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={km_k_candidate_1: km_labels_k1, km_k_candidate_2: km_labels_k2},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='K-Means Clustering: Cluster Size Comparison (k=3 vs k=5)'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b3e797",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: k=3 selected</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K=3 shows clear elbow in Inertia curve and achieves highest Calinski-Harabasz Index (3071). Balanced cluster sizes (32.0%, 29.3%, 38.7%) with distinct profiles across Education and Income. Higher k values produce clusters that differentiate only on few features while remaining similar across most demographics, reducing interpretability.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4305199",
   "metadata": {},
   "source": [
    "### **7.2.3 Final K-Means Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bddb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means Clustering Solution\n",
    "# Using k=3 from comparison analysis\n",
    "\n",
    "km_final_k = km_k_candidate_1  # k=3\n",
    "\n",
    "# Reuse labels from comparison step\n",
    "km_labels_final = km_labels_k1\n",
    "\n",
    "# Create labeled dataset\n",
    "df_demographic_a_scaled_labeled_km = df_demographic_a_scaled.copy()\n",
    "df_demographic_a_scaled_labeled_km['Cluster'] = km_labels_final\n",
    "\n",
    "# Calculate final metrics\n",
    "km_final_metrics = evaluate_clustering_metrics(df_demographic_a_scaled, km_labels_final)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "demo_clustering_results['K-Means'] = {\n",
    "    'k': km_final_k,\n",
    "    'Silhouette': km_final_metrics['Silhouette Score'],\n",
    "    'Calinski-Harabasz': km_final_metrics['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': km_final_metrics['Davies-Bouldin Index'],\n",
    "    'R2': get_rsq(df_demographic_a_scaled_labeled_km, df_demographic_a_scaled.columns.tolist(), 'Cluster'),\n",
    "    'labels': km_labels_final\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c80e4c9",
   "metadata": {},
   "source": [
    "### **7.2.4 Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5dd091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cluster Profiles Heatmap - Z-scores of demographic features per cluster\n",
    "km_cluster_profiles = df_demographic_a_scaled_labeled_km.groupby('Cluster')[feats].mean()\n",
    "km_population_mean = df_demographic_a_scaled[feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    km_cluster_profiles, \n",
    "    km_population_mean, \n",
    "    GROUP80_palette_continuous,\n",
    "    title='K-Means Clustering: Demographic Profiles (k=3)\\nStandardized Z-Scores per Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e401467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    km_labels_final, \n",
    "    km_final_k, \n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Clustering - Final Cluster Sizes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0074b763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "km_feature_variance = km_cluster_profiles.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    km_feature_variance,\n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Clustering: Feature Importance Analysis\\nWhich Demographics Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance ranking\n",
    "km_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': km_feature_variance.index,\n",
    "    'Variance': km_feature_variance.values.round(4)\n",
    "})\n",
    "\n",
    "km_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf09538",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">K-Means Clustering Profiling Summary (k=3)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed demographic characteristics of the final 3 K-Means clusters to identify distinct customer segments. Education, Income, and City emerge as primary differentiators.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (32.0%, n=4,170) - Common Regions, Higher-Income Higher-Education:</strong> Common cities (Z=+1.03), common FSA (Z=+0.59), common provinces (Z=+0.53), higher education (Z=+0.56), higher income (Z=+0.50).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (29.3%, n=3,824) - Lower-Income Lower-Education:</strong> Significantly lower education (Z=-1.35), significantly lower income (Z=-1.19), average geographic frequency, lower married rate (Z=+0.37).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (38.7%, n=5,044) - Rare Regions, Higher-Education:</strong> Rare provinces (Z=-0.45), rare cities (Z=-0.85), rare FSA (Z=-0.50), higher education (Z=+0.57), higher income (Z=+0.49).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Segmentation Driver - Education (Variance: 1.22):</strong> Education creates the clearest separation, with Cluster 1 showing significantly lower education levels.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Secondary Drivers - Income (Variance: 0.94) & City (Variance: 0.89):</strong> Income strongly correlates with education, while city frequency differentiates mainstream (Cluster 0) from niche geographic segments.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Gender Independence (Variance: 0.0002):</strong> Gender shows virtually no differentiation across K-Means clusters.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae63dc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b482f",
   "metadata": {},
   "source": [
    "## **7.3 Mean Shift Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cf2085",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Mean Shift Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Mean Shift is a <strong>density-based mode-seeking algorithm</strong> that identifies clusters by shifting a sliding window toward regions of highest point density. Unlike K-Means, it does not require pre-specifying k and can discover clusters of arbitrary shape by following the gradient of the underlying density estimate.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialize a Sliding Window:</strong> Begin with a circular kernel window centered at a point <strong>C</strong> (randomly selected or one per data point) with radius <strong>r</strong> (bandwidth)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Shift Toward Higher Density:</strong> At each iteration, compute the mean of all points inside the window and shift the center <strong>C</strong> to this mean, gradually moving toward higher-density regions</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Convergence:</strong> Repeat the shift step until the movement of the window center becomes negligible (the center has converged to a mode)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Merge Modes and Assign Clusters:</strong> Run the process from many initial centers, then merge converged centers that are within a tolerance distance. Assign each data point to the cluster of the nearest converged center. If sliding windows overlap, the densest mode (window containing the most points) is preserved and points are grouped accordingly</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Selecting the Best Bandwidth:</strong> Tested a targeted set of <strong>bandwidth quantiles</strong> using <strong>estimate_bandwidth()</strong>, then fitted Mean Shift for each bandwidth and tracked <strong>n_clusters</strong>, <strong>R² (variance explained)</strong>, and <strong>Silhouette</strong> (only if ≥ 2 clusters). Visualized <strong>quantile vs. cluster count</strong> to detect stability regions and avoid extreme cases (too large -> 1 cluster, too small -> fragmentation)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Evaluation of Mean Shift Solutions:</strong> Selected two candidate regimes from the results table and compared them by fitting both solutions: <strong>Candidate A</strong> (quantile=0.061 -> 3 clusters) vs <strong>Candidate B</strong> (quantile=0.06 -> 5 clusters)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Mean Shift Clustering Solution:</strong> Selected quantile=0.06 (5 clusters) for stable plateau regime with meaningful geographic differentiation</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Mean Shift Cluster Profiling:</strong> Profiled the final solution using (1) a <strong>cluster profile heatmap</strong> (cluster means vs population mean), (2) <strong>final cluster size distribution</strong>, and (3) <strong>feature importance</strong> via variance across cluster centroids</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0be46c",
   "metadata": {},
   "source": [
    "### **7.3.1 Selecting the best Bandwidth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc14310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate bandwidth values\n",
    "bandwidth_quantiles = [0.0635, 0.061, 0.06095, 0.06, 0.0475, 0.04747, 0.0473,0.0472, 0.04715, 0.0471, 0.046, 0.045, 0.0435, 0.043, 0.04, 0.035, 0.03]\n",
    "ms_results = []\n",
    "\n",
    "X_ms = df_demographic_a_scaled[feats]\n",
    "\n",
    "for q in bandwidth_quantiles:\n",
    "    bw = estimate_bandwidth(X_ms, quantile=q, random_state=1)\n",
    "    ms = MeanShift(bandwidth=bw, bin_seeding=True, n_jobs=-1)\n",
    "    labels = ms.fit_predict(X_ms)\n",
    "\n",
    "    n_clusters = len(np.unique(labels))\n",
    "\n",
    "    # R²\n",
    "    df_tmp = X_ms.copy()\n",
    "    df_tmp[\"labels\"] = labels\n",
    "    r2 = get_rsq(df_tmp, feats, \"labels\")\n",
    "\n",
    "    # Silhouette (only defined if >= 2 clusters)\n",
    "    sil = silhouette_score(X_ms, labels) if n_clusters >= 2 else np.nan\n",
    "\n",
    "    ms_results.append({\n",
    "        \"quantile\": q,\n",
    "        \"bandwidth\": float(bw),\n",
    "        \"n_clusters\": int(n_clusters),\n",
    "        \"R2\": float(r2),\n",
    "        \"Silhouette\": float(sil) if not np.isnan(sil) else np.nan\n",
    "    })\n",
    "\n",
    "ms_results_df = pd.DataFrame(ms_results).sort_values(\"quantile\", ascending=False)\n",
    "ms_results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_meanshift_quantile_vs_clusters(ms_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3417a2d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Mean Shift Bandwidth Selection Summary</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Determined the Mean Shift kernel bandwidth by testing a targeted set of bandwidth quantiles (0.03 to 0.0635). For each quantile, bandwidth was estimated via <strong>estimate_bandwidth()</strong>, Mean Shift was fitted, and the solution was evaluated using <strong>cluster count</strong>, <strong>R²</strong>, and <strong>Silhouette</strong> to balance segmentation granularity and separation quality.\n",
    "  </p>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\">Select a bandwidth regime that yields interpretable, stable clusters with good separation, while avoiding over-fragmentation (many micro-clusters) or collapse into too few clusters</li>\n",
    "  </ul>\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Quantile scan (bandwidth selection):</strong> Evaluated 17 quantiles (0.03-0.0635) to observe how cluster count changes as bandwidth varies.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Evaluation criteria:</strong> Compared <strong>n_clusters</strong> (solution granularity), <strong>Silhouette</strong> (cluster separation) and <strong>R²</strong> (variance explained).</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Stable 5-cluster plateau:</strong> quantile=0.06 and 0.0475 yield stable 5 clusters (R²=0.35-0.36, Silhouette=0.14)</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>3-cluster solution:</strong> quantile=0.061 yields 3 clusters with good Silhouette (0.15) - comparable to previous k=3 solutions</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Candidate A (compact):</strong> quantile=0.061 yields 3 clusters - interpretable, consistent with previous algorithms</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Candidate B (plateau):</strong> quantile=0.06 yields 5 clusters - stable regime across multiple quantiles</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Decision for next step:</strong> Compare 3 clusters vs 5 clusters</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f03da82",
   "metadata": {},
   "source": [
    "### **7.3.2 Evaluation of Mean Shift Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4cd174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate Mean Shift solutions based on previous evaluation results\n",
    "# Candidate A: quantile=0.061 → 3 clusters\n",
    "# Candidate B: quantile=0.06 → 5 clusters\n",
    "\n",
    "ms_q_1 = 0.061\n",
    "ms_q_2 = 0.06\n",
    "\n",
    "X_ms = df_demographic_a_scaled[feats]\n",
    "\n",
    "# Get bandwidths from the already computed results table\n",
    "ms_bw_1 = float(ms_results_df.loc[ms_results_df[\"quantile\"] == ms_q_1, \"bandwidth\"].iloc[0])\n",
    "ms_bw_2 = float(ms_results_df.loc[ms_results_df[\"quantile\"] == ms_q_2, \"bandwidth\"].iloc[0])\n",
    "\n",
    "# Fit both candidate solutions\n",
    "ms_cand_1 = MeanShift(bandwidth=ms_bw_1, bin_seeding=True, n_jobs=-1)\n",
    "ms_cand_2 = MeanShift(bandwidth=ms_bw_2, bin_seeding=True, n_jobs=-1)\n",
    "\n",
    "ms_labels_1 = ms_cand_1.fit_predict(X_ms)\n",
    "ms_labels_2 = ms_cand_2.fit_predict(X_ms)\n",
    "\n",
    "# Cluster profiles\n",
    "df_temp_ms1 = df_demographic_a_scaled.copy()\n",
    "df_temp_ms1[\"Cluster\"] = ms_labels_1\n",
    "cluster_profiles_ms1 = df_temp_ms1.groupby(\"Cluster\")[feats].mean()\n",
    "\n",
    "df_temp_ms2 = df_demographic_a_scaled.copy()\n",
    "df_temp_ms2[\"Cluster\"] = ms_labels_2\n",
    "cluster_profiles_ms2 = df_temp_ms2.groupby(\"Cluster\")[feats].mean()\n",
    "\n",
    "display(cluster_profiles_ms1.round(3))\n",
    "display(cluster_profiles_ms2.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={\n",
    "        f\"q={ms_q_1} (bw={ms_bw_1:.2f})\": ms_labels_1,\n",
    "        f\"q={ms_q_2} (bw={ms_bw_2:.2f})\": ms_labels_2,\n",
    "    },\n",
    "    palette=CUSTOM_HEX,\n",
    "    title=\"Mean Shift Clustering: Cluster Size Comparison (Quantile & Bandwidth)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ec661",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    <strong>Decision: Mean Shift quantile=0.06 selected (5 clusters)</strong>\n",
    "  </p>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Compared 3 clusters (q=0.061) vs 5 clusters (q=0.06). Both solutions share a stable core: Cluster 1 (Lower-Education Lower-Income) is nearly identical in size (29.7% vs 29.9%) and profile (Education Z=-1.35, Income Z=-1.16). The 5-cluster solution preserves this core while splitting the dominant cluster into more meaningful segments instead of merging into one large mainstream group. This maintains interpretability while providing additional detail on geographic and FSA patterns.\n",
    "  </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cefc60e",
   "metadata": {},
   "source": [
    "### **7.3.3 Final Mean Shift Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cbe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Mean Shift Clustering Solution\n",
    "# Using quantile=0.06 (5 clusters) from comparison analysis\n",
    "\n",
    "chosen_quantile_ms = ms_q_2  # 0.06\n",
    "chosen_bandwidth_ms = ms_bw_2\n",
    "\n",
    "# Reuse labels from comparison step\n",
    "ms_labels_final = ms_labels_2\n",
    "\n",
    "df_demographic_a_scaled[\"ms_cluster\"] = ms_labels_final\n",
    "\n",
    "# Calculate final metrics\n",
    "ms_final_metrics = evaluate_clustering_metrics(df_demographic_a_scaled[feats], ms_labels_final)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "demo_clustering_results['Mean Shift'] = {\n",
    "    'k': len(np.unique(ms_labels_final)),\n",
    "    'Silhouette': ms_final_metrics['Silhouette Score'],\n",
    "    'Calinski-Harabasz': ms_final_metrics['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': ms_final_metrics['Davies-Bouldin Index'],\n",
    "    'R2': ms_results_df[ms_results_df['quantile'] == chosen_quantile_ms]['R2'].values[0],\n",
    "    'labels': ms_labels_final\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b2b340",
   "metadata": {},
   "source": [
    "### **7.3.4 Mean Shift Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696b4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Profile Heatmap - Z-scores of demographic features per cluster (Mean Shift)\n",
    "ms_cluster_profiles = (df_demographic_a_scaled\n",
    "                       .groupby(\"ms_cluster\")[feats]\n",
    "                       .mean())\n",
    "ms_population_mean = df_demographic_a_scaled[feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    ms_cluster_profiles,\n",
    "    ms_population_mean,\n",
    "    GROUP80_palette_continuous,\n",
    "    title=\"Mean Shift - Cluster Profiles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster sizes\n",
    "nclus_ms = len(np.unique(ms_labels_final))\n",
    "plot_cluster_sizes(\n",
    "    ms_labels_final,\n",
    "    nclus_ms,\n",
    "    CUSTOM_HEX,\n",
    "    title=\"Mean Shift - Final Cluster Sizes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1638ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "ms_feature_variance = ms_cluster_profiles.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    ms_feature_variance,\n",
    "    CUSTOM_HEX,\n",
    "    title=\"Mean Shift Clustering: Feature Importance Analysis\\nWhich Demographics Differentiate Clusters?\"\n",
    ")\n",
    "\n",
    "# Display feature importance ranking\n",
    "ms_feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": ms_feature_variance.index,\n",
    "    \"Variance\": ms_feature_variance.values.round(4),\n",
    "})\n",
    "\n",
    "ms_feature_importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaeb277",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Mean Shift Profiling Summary (5 clusters)</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Profiled the final Mean Shift solution (quantile=0.06) with <strong>5 clusters</strong>. Segmentation driven by <strong>FSA</strong>, <strong>Education</strong>, and <strong>Province/City</strong>.\n",
    "  </p>\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Cluster 0 (44.1%, n=5,748) - Mainstream Higher-Income:</strong> Average geography, higher income (Z=+0.70), moderate education (Z=+0.40).</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Cluster 1 (29.9%, n=3,898) - Lower-Education Lower-Income:</strong> Significantly lower education (Z=-1.35), lower income (Z=-1.16). Stable across solutions.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Cluster 2 (5.7%, n=741) - Common Regions Highly-Educated:</strong> Common provinces (Z=+0.83), common cities (Z=+1.07), highest education (Z=+1.31).</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Cluster 3 (13.8%, n=1,801) - Rare Regions Higher-Education:</strong> Rare provinces (Z=-1.51), rare cities (Z=-0.91), rare FSA (Z=-0.67), higher education (Z=+0.79).</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Cluster 4 (6.5%, n=850) - High FSA Segment:</strong> Very high FSA (Z=+2.58), common cities (Z=+0.80), moderate education (Z=+0.69).</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Primary Driver - FSA (Variance: 1.68):</strong> FSA frequency creates strongest separation, isolating Cluster 4.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Secondary Drivers - Education (1.03) & Province (0.80):</strong> Education separates Cluster 1, Province differentiates Cluster 3.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Gender Independence (Variance: 0.0006):</strong> No differentiation across clusters.</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783bfc65",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481bb717",
   "metadata": {},
   "source": [
    "## **7.4 Gaussian Mixture Models (GMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a8b561",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Gaussian Mixture Model (GMM) Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        GMM is a <strong>probabilistic clustering algorithm</strong> that models data as a mixture of k Gaussian distributions. Unlike hard clustering (K-Means, Hierarchical), GMM provides soft assignments where each point has a probability of belonging to each cluster, enabling uncertainty quantification and overlapping segments.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialization:</strong> Randomly initialize k Gaussian components (each with mean, covariance, and mixing weight)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Expectation Step (E-step):</strong> Calculate probability that each data point belongs to each Gaussian component using current parameters</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Maximization Step (M-step):</strong> Update component parameters (means, covariances, weights) to maximize the likelihood of the data given current assignments</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Convergence:</strong> Iterate E-step and M-step until parameters stabilize (EM algorithm converges to local maximum)</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Component Selection:</strong> Test n=2-10 Gaussian components, evaluate using AIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) to penalize model complexity</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Candidate Comparison:</strong> Compare n=3 vs n=4 components using validation metrics (Silhouette, Calinski-Harabasz, Davies-Bouldin, R²)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model & Profiling:</strong> Selected n=4 components for superior feature utilization and model fit, producing four segments (30.7%, 31.4%, 14.2%, 23.7%) differentiated primarily by FSA, Education, Income, and Gender</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e8c34a",
   "metadata": {},
   "source": [
    "### **7.4.1 Selecting covariance_type & n_components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9face",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidates\n",
    "gmm_n_components = list(range(2, 11))\n",
    "gmm_cov_types = [\"full\", \"tied\", \"diag\", \"spherical\"]\n",
    "\n",
    "X_gmm = df_demographic_a_scaled[feats].copy()\n",
    "\n",
    "gmm_results_df = evaluate_gmm_grid(\n",
    "    X=X_gmm,\n",
    "    feats=feats,\n",
    "    n_components_list=gmm_n_components,\n",
    "    covariance_types=gmm_cov_types,\n",
    "    n_init=10,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "gmm_results_df_sorted = (\n",
    "    gmm_results_df\n",
    "    .sort_values([\"BIC\", \"AIC\"], ascending=[True, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "gmm_results_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43b21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) pick covariance_type\n",
    "plot_gmm_covtype_bic_aic(gmm_results_df, gmm_cov_types, gmm_n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Find best n_components for chosen covariance_type\n",
    "chosen_covariance_type_gmm = \"full\" # based on previous plot\n",
    "plot_gmm_n_selection_for_covtype(gmm_results_df, chosen_covariance_type_gmm, gmm_n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc2ecce",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">\n",
    "    GMM Parameter Selection Summary\n",
    "  </h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Evaluated Gaussian Mixture Models by running a grid over <strong>n_components</strong> (2–10) and\n",
    "    <strong>covariance_type</strong> (<strong>full</strong>, <strong>tied</strong>, <strong>diag</strong>, <strong>spherical</strong>)\n",
    "    using <strong>init_params=\"kmeans\"</strong> to stabilize initialization. Each configuration was scored with\n",
    "    <strong>BIC</strong> and <strong>AIC</strong> (model selection), plus <strong>R²</strong> (variance explained) and\n",
    "    <strong>Silhouette</strong> (cluster separation sanity check).\n",
    "  </p>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\">\n",
    "      Select a <strong>covariance structure</strong> that best fits the data distribution, then choose\n",
    "      <strong>n_components</strong> that balances model fit (BIC/AIC), separation (Silhouette), and interpretability\n",
    "      (avoid unnecessary micro-segmentation).\n",
    "    </li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\">\n",
    "      <strong>Step 1 – covariance_type selection:</strong>\n",
    "      Compared BIC &amp; AIC curves across all covariance types. Since BIC/AIC are the standard\n",
    "      likelihood-penalized criteria for GMMs, the best covariance_type is the one that achieves the\n",
    "      <strong>lowest BIC/AIC</strong> consistently across n.\n",
    "    </li>\n",
    "    <li style=\"margin-right: 20px;\">\n",
    "      <strong>Step 2 – choose n_components within the winning covariance_type:</strong>\n",
    "      For the selected covariance_type, inspected (i) <strong>BIC/AIC</strong> for the best trade-off between fit and\n",
    "      complexity, (ii) <strong>Silhouette</strong> to avoid \"overfitting into overlapping components\", and (iii)\n",
    "      <strong>R²</strong> to understand how much additional variance is gained by increasing n.\n",
    "    </li>\n",
    "    <li style=\"margin-right: 20px;\">\n",
    "      <strong>Important note on negative scores:</strong>\n",
    "      <strong>BIC/AIC can be negative</strong> because they are derived from the (negative) log-likelihood scale; only\n",
    "      <strong>relative differences</strong> matter (lower is always better), not the absolute sign.\n",
    "    </li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "      <li style=\"margin-right: 20px;\">\n",
    "        <strong>Best covariance_type:</strong>\n",
    "        <strong>Full</strong> and <strong>diag</strong> achieve the lowest BIC/AIC at higher n_components. However, while BIC/AIC improves with higher n (minimum at n=9-10), high component counts produce micro-segments that reduce interpretability. <strong>Full covariance</strong> is selected for capturing feature correlations.\n",
    "      </li>\n",
    "      <li style=\"margin-right: 20px;\">\n",
    "        <strong>Candidate A (best separation):</strong>\n",
    "        With <strong>full</strong>, <strong>n=3</strong> achieves the <strong>highest Silhouette</strong> (0.200) among lower n values, indicating well-separated clusters. This aligns with Hierarchical and K-Means 3-cluster solutions, enabling cross-method comparison.\n",
    "      </li>\n",
    "      <li style=\"margin-right: 20px;\">\n",
    "        <strong>Candidate B (balanced fit):</strong>\n",
    "        With <strong>full</strong>, <strong>n=4</strong> shows <strong>substantial BIC improvement</strong> (16,643 vs 119,653) and higher R² (0.350 vs 0.316). While Silhouette (0.183) is slightly lower, the additional component may reveal a meaningful fourth segment.\n",
    "      </li>\n",
    "      <li style=\"margin-right: 20px;\">\n",
    "        <strong>Why n=3 and n=4 instead of higher n?</strong>\n",
    "        Silhouette scores at n=3-4 (0.183-0.200) are comparable to or better than higher n (n=7: 0.168), indicating separation does not improve with more components. Following the principle of parsimony, we prioritize actionable segments over maximum statistical fit.\n",
    "      </li>\n",
    "      <li style=\"margin-right: 20px;\">\n",
    "        <strong>Decision:</strong>\n",
    "        Proceed with <strong>full, n=3</strong> (best separation) and <strong>full, n=4</strong> (balanced fit) for deeper profiling.\n",
    "      </li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cafb19",
   "metadata": {},
   "source": [
    "### **7.4.2 Evaluation of GMM Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0877a1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=3\n",
    "# n=4\n",
    "\n",
    "# Fit only the selected candidates\n",
    "gmm_3 = GaussianMixture(n_components=3, covariance_type='full', n_init=10, init_params='kmeans', random_state=1)\n",
    "gmm_4 = GaussianMixture(n_components=4, covariance_type='full', n_init=10, init_params='kmeans', random_state=1)\n",
    "\n",
    "gmm_labels_3 = gmm_3.fit_predict(df_demographic_a_scaled[feats])\n",
    "gmm_labels_4 = gmm_4.fit_predict(df_demographic_a_scaled[feats])\n",
    "\n",
    "# Calculate cluster means\n",
    "df_temp_3 = df_demographic_a_scaled[feats].copy()\n",
    "df_temp_3['Cluster'] = gmm_labels_3\n",
    "cluster_means_3 = df_temp_3.groupby('Cluster').mean()\n",
    "\n",
    "df_temp_4 = df_demographic_a_scaled[feats].copy()\n",
    "df_temp_4['Cluster'] = gmm_labels_4\n",
    "cluster_means_4 = df_temp_4.groupby('Cluster').mean()\n",
    "\n",
    "display(cluster_means_3.round(3))\n",
    "display(cluster_means_4.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={'GMM (n=3)': gmm_labels_3, 'GMM (n=4)': gmm_labels_4},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='GMM Clustering: Cluster Size Comparison'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445da008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty Analysis: Assignment probability < 70%\n",
    "uncertainty_threshold = 0.7\n",
    "\n",
    "gmm_max_probs_3 = gmm_3.predict_proba(df_demographic_a_scaled[feats]).max(axis=1)\n",
    "gmm_max_probs_4 = gmm_4.predict_proba(df_demographic_a_scaled[feats]).max(axis=1)\n",
    "\n",
    "uncertain_pct_3 = (gmm_max_probs_3 < uncertainty_threshold).sum() / len(gmm_max_probs_3) * 100\n",
    "uncertain_pct_4 = (gmm_max_probs_4 < uncertainty_threshold).sum() / len(gmm_max_probs_4) * 100\n",
    "\n",
    "uncertainty_summary = pd.DataFrame({\n",
    "    'n_components': [3, 4],\n",
    "    'uncertain_pct': [uncertain_pct_3, uncertain_pct_4],\n",
    "    'mean_prob': [gmm_max_probs_3.mean(), gmm_max_probs_4.mean()],\n",
    "    'min_prob': [gmm_max_probs_3.min(), gmm_max_probs_4.min()]\n",
    "})\n",
    "\n",
    "display(uncertainty_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f7a79",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: n=4 components selected with full covariance</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Compared n=3 vs n=4 components using <strong>full covariance structure</strong>. While n=3 achieves slightly higher Silhouette (0.200 vs 0.183), n=4 provides <strong>superior feature utilization</strong>: it leverages Gender as a primary differentiator (Z-scores of +1.00 and -1.00 for male/female segments) and isolates a distinct High-FSA segment (Z=+2.12), whereas n=3 ignores Gender entirely (all clusters near Z=0). The \"Lower Education/Income\" segment remains stable across both solutions (Education Z=-1.35, Income Z=-1.26). N=4 achieves substantially better model fit (<strong>BIC=16,643</strong> vs 119,653) and higher R² (0.350 vs 0.316). Both solutions show <strong>0.0% uncertain assignments</strong> at the 70% probability threshold. Cluster sizes remain balanced and actionable (30.7%, 31.4%, 14.2%, 23.7%). Following the principle of maximizing demographic differentiation, <strong>n=4 with full covariance</strong> is selected.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec72d184",
   "metadata": {},
   "source": [
    "### **7.4.3 Final GMM Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b9420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final parameters based on evaluation previous section\n",
    "# Selected: n_components=4, covariance_type='full'\n",
    "chosen_n_components_gmm = 4\n",
    "\n",
    "gmm_labels_final = gmm_labels_4\n",
    "gmm_final = gmm_4\n",
    "\n",
    "df_demographic_a_scaled['gmm_cluster'] = gmm_labels_final\n",
    "\n",
    "# Calculate final metrics\n",
    "gmm_final_metrics = evaluate_clustering_metrics(df_demographic_a_scaled[feats], gmm_labels_final)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "demo_clustering_results['GMM'] = {\n",
    "    'k': chosen_n_components_gmm,\n",
    "    'Silhouette': gmm_final_metrics['Silhouette Score'],\n",
    "    'Calinski-Harabasz': gmm_final_metrics['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': gmm_final_metrics['Davies-Bouldin Index'],\n",
    "    'R2': gmm_results_df[(gmm_results_df['n_components'] == 4) & (gmm_results_df['covariance_type'] == 'full')]['R2'].values[0],\n",
    "    'labels': gmm_labels_final\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cb247d",
   "metadata": {},
   "source": [
    "### **7.4.4 GMM Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a42a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Profile Heatmap - Z-scores of demographic features per cluster (GMM)\n",
    "df_temp_gmm = df_demographic_a_scaled[feats].copy()\n",
    "df_temp_gmm['Cluster'] = gmm_labels_final\n",
    "cluster_profiles_gmm = df_temp_gmm.groupby('Cluster').mean()\n",
    "gmm_population_mean = df_demographic_a_scaled[feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    cluster_profiles_gmm,\n",
    "    gmm_population_mean,\n",
    "    GROUP80_palette_continuous,\n",
    "    title='GMM Clustering: Demographic Profiles (n=4)\\nStandardized Z-Scores per Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb89d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    gmm_labels_final,\n",
    "    chosen_n_components_gmm,\n",
    "    CUSTOM_HEX,\n",
    "    title='GMM Clustering - Final Cluster Sizes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "gmm_feature_variance = cluster_profiles_gmm.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    gmm_feature_variance,\n",
    "    CUSTOM_HEX,\n",
    "    title='GMM Clustering: Feature Importance Analysis\\nWhich Demographics Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance as DataFrame\n",
    "gmm_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': gmm_feature_variance.index,\n",
    "    'Variance': gmm_feature_variance.values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "display(gmm_feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded3b281",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">GMM Clustering Profiling Summary (n=4)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed demographic characteristics of the final 4 GMM clusters (full covariance) to identify probabilistic customer segments. The model captures feature correlations through full covariance, with FSA, Education, Income, and Gender emerging as primary differentiators.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (30.7%, n=3,999) - Higher-Income Males:</strong> Predominantly male (Z=+1.00), higher education (Z=+0.52), higher income (Z=+0.49), married (Z=+0.67).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (31.4%, n=4,096) - Higher-Income Females:</strong> Predominantly female (Z=-1.00), higher education (Z=+0.53), higher income (Z=+0.48), married (Z=+0.66).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (14.2%, n=1,857) - High-FSA Segment:</strong> Very high FSA frequency (Z=+2.12), common cities (Z=+0.65), average education (Z=-0.03), average income (Z=-0.02).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 3 (23.7%, n=3,086) - Lower-Income Lower-Education:</strong> Significantly lower education (Z=-1.35), significantly lower income (Z=-1.26), less married (Z=+0.36).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Segmentation Driver - FSA (Variance: 1.52):</strong> FSA frequency emerges as the strongest differentiator, primarily isolating Cluster 2.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Secondary Drivers - Education (0.78), Income (0.68), Gender (0.67):</strong> Education and Income separate Cluster 3 from the rest; Gender differentiates Cluster 0 (male) from Cluster 1 (female).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Geographic & Marital Independence (Variance: <0.15):</strong> Province, City, and Marital Status show minimal differentiation across GMM clusters.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3702530",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa5c7a",
   "metadata": {},
   "source": [
    "## **7.5 Self Organizing Maps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f219b0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Self-Organizing Maps (SOM) + K-Means Two-Stage Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Self-Organizing Maps are <strong>unsupervised neural networks</strong> that project high-dimensional data onto a low-dimensional grid while preserving topological relationships. Each neuron represents a weight vector in the input space, and during training, neurons are \"pulled\" toward data patterns, dragging their neighbors along. The two-stage approach combines SOM's dimensionality reduction with K-Means clustering on the learned neuron weights.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialization:</strong> Randomly initialize neuron weight vectors and set neighborhood radius (σ) and learning rate (α)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>BMU Selection:</strong> For each input pattern, find the Best Matching Unit (BMU) - the neuron with minimum Euclidean distance to the input</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Weight Update:</strong> Update the BMU and its neighbors: w(new) = w(old) + α[x - w(old)], with neighborhood function controlling update strength</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Parameter Decay:</strong> Gradually reduce learning rate and neighborhood radius over iterations</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Two-Stage Clustering:</strong> Apply K-Means to the trained SOM weight vectors, then map customers to clusters via their BMUs</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Key Quality Metrics:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Quantization Error (QE):</strong> Average distance between data points and their BMUs - measures data representation accuracy (lower is better)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Topographic Error (TE):</strong> Proportion of data points where 1st and 2nd BMUs are not adjacent - measures topology preservation (lower is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Parameter Grid Search:</strong> Test 18 combinations of grid sizes (10×10, 20×20, 40×40), learning rates (0.5, 0.75, 1.0), and sigma values (0.5, 1.0) to optimize QE/TE trade-off</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>SOM Visualization:</strong> Analyze trained SOM using Component Planes (feature distributions), U-Matrix (cluster boundaries), and Hit Map (customer density)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Two-Stage Clustering:</strong> Apply K-Means to 1,600 neuron weight vectors (40×40 grid), evaluate k=2-13 using Silhouette, Calinski-Harabasz, and Davies-Bouldin indices</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Solution Comparison:</strong> Compare k=3 vs k=6 solutions by examining SOM grid visualizations, cluster sizes, and feature profiles</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model & Profiling:</strong> Select k=3 solution and analyze demographic characteristics, comparing segmentation patterns to standalone K-Means results</li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ihiqalj5l",
   "metadata": {},
   "source": [
    "### **7.5.1 SOM Parameter Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7gip98liet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for SOM optimization\n",
    "grid_sizes = [10, 20, 40]\n",
    "learning_rates = [0.5, 0.75, 1.0]\n",
    "sigma_values = [0.5, 1.0]\n",
    "\n",
    "# Prepare scaled data for SOM training\n",
    "som_data = df_demographic_a_scaled[feats].values\n",
    "\n",
    "\n",
    "# Store grid search results\n",
    "'''grid_search_results = []\n",
    "\n",
    "# Perform grid search\n",
    "for grid_size in grid_sizes:\n",
    "    for lr in learning_rates:\n",
    "        for sigma in sigma_values:\n",
    "            # Initialize SOM with current parameters\n",
    "            som = MiniSom(\n",
    "                x=grid_size,\n",
    "                y=grid_size,\n",
    "                input_len=len(feats),\n",
    "                sigma=sigma,\n",
    "                learning_rate=lr,\n",
    "                neighborhood_function='gaussian',\n",
    "                topology='hexagonal',\n",
    "                activation_distance='euclidean',\n",
    "                random_seed=1\n",
    "            )\n",
    "            \n",
    "            # Initialize weights randomly from data\n",
    "            som.random_weights_init(som_data)\n",
    "            \n",
    "            # Train SOM\n",
    "            # Scale iterations with map size (500 per neuron): larger grids have more neurons that need sufficient updates to converge properly\n",
    "            num_iterations = 500 * grid_size * grid_size\n",
    "            som.train_batch(som_data, num_iteration=num_iterations, verbose=False)\n",
    "            \n",
    "            # Calculate quality metrics\n",
    "            qe = som.quantization_error(som_data)\n",
    "            te = som.topographic_error(som_data)\n",
    "            \n",
    "            # Store results\n",
    "            grid_search_results.append({\n",
    "                'grid_size': f'{grid_size}x{grid_size}',\n",
    "                'learning_rate': lr,\n",
    "                'sigma': sigma,\n",
    "                'units': grid_size * grid_size,\n",
    "                'quantization_error': qe,\n",
    "                'topographic_error': te\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "grid_search_df = pd.DataFrame(grid_search_results)\n",
    "grid_search_df = grid_search_df.sort_values(['quantization_error', 'topographic_error']).reset_index(drop=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fa001",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''grid_search_df.to_csv('data/output_data/som_grid_search_results.csv', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db9618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df = pd.read_csv('data/output_data/som_grid_search_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1rfujl5bykl",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Quantization Error by Grid Size\n",
    "for lr in learning_rates:\n",
    "    for sigma in sigma_values:\n",
    "        subset = grid_search_df[(grid_search_df['learning_rate'] == lr) & (grid_search_df['sigma'] == sigma)]\n",
    "        axes[0].plot(subset['units'], subset['quantization_error'], \n",
    "                    marker='o', alpha=0.6, label=f'LR={lr}, σ={sigma}')\n",
    "\n",
    "axes[0].set_xlabel('Number of SOM Units', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Quantization Error (QE)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('SOM Grid Search: Quantization Error', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(False)\n",
    "\n",
    "# Plot 2: Topographic Error by Grid Size\n",
    "for lr in learning_rates:\n",
    "    for sigma in sigma_values:\n",
    "        subset = grid_search_df[(grid_search_df['learning_rate'] == lr) & (grid_search_df['sigma'] == sigma)]\n",
    "        axes[1].plot(subset['units'], subset['topographic_error'], \n",
    "                    marker='o', alpha=0.6, label=f'LR={lr}, σ={sigma}')\n",
    "\n",
    "axes[1].set_xlabel('Number of SOM Units', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Topographic Error (TE)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('SOM Grid Search: Topographic Error', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(False)\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display best parameter combinations\n",
    "grid_search_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k3tp593nev",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best parameters based on lowest QE and TE\n",
    "best_params = grid_search_df.iloc[0]\n",
    "\n",
    "# Extract parameters for later use\n",
    "selected_grid_size = int(best_params['grid_size'].split('x')[0])\n",
    "selected_lr = best_params['learning_rate']\n",
    "selected_sigma = best_params['sigma']\n",
    "\n",
    "# Display best parameters\n",
    "best_params_display = pd.DataFrame({\n",
    "    'Parameter': ['Grid Size', 'Learning Rate', 'Sigma', 'Total Units', 'Quantization Error', 'Topographic Error'],\n",
    "    'Value': [\n",
    "        best_params['grid_size'],\n",
    "        best_params['learning_rate'],\n",
    "        best_params['sigma'],\n",
    "        best_params['units'],\n",
    "        f\"{best_params['quantization_error']:.4f}\",\n",
    "        f\"{best_params['topographic_error']:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "best_params_display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a3c45",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM Parameter Grid Search Summary</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Performed systematic grid search over 18 parameter combinations to identify optimal SOM configuration for demographic clustering. Evaluated 3 grid sizes (10×10, 20×20, 40×40), 3 learning rates (0.5, 0.75, 1.0), and 2 sigma values (0.5, 1.0), with iterations scaled proportionally to grid size (500 × number of neurons) to ensure convergence.\n",
    "  </p>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\">Find optimal SOM parameters that balance low Quantization Error (accurate data representation) with low Topographic Error (preserved neighborhood topology) for subsequent two-stage clustering</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Quality metrics:</strong> Quantization Error (QE) measures average distance between data points and their Best Matching Units (lower = better data representation). Topographic Error (TE) measures proportion of data points where first and second BMUs are not adjacent (lower = better topology preservation)</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Iteration scaling:</strong> Training iterations set to 500 × grid_size² (e.g., 50,000 for 10×10, 200,000 for 20×20, 800,000 for 40×40) following the standard heuristic of ~500 iterations per neuron to ensure proper convergence across all grid sizes</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Parameter selection rationale:</strong> For two-stage clustering (SOM + K-Means), moderate grid sizes are preferred to ensure meaningful data compression. While larger grids minimize QE, they reduce the SOM's ability to aggregate similar customers into prototype neurons, diminishing the benefit of the two-stage approach</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Grid size dominates QE:</strong> Larger grids yield substantially lower QE. 10×10 produced QE 0.84–0.92, 20×20 achieved 0.44–0.48, and 40×40 reached 0.16–0.22.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Sigma controls QE-TE trade-off:</strong> Small sigma (σ=0.5) damages topology preservation (TE 0.30–0.80), while σ=1.0 maintains excellent TE (0.08–0.30) with marginal QE increase.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Learning rate has moderate impact:</strong> lr=1.0 outperforms lr=0.5 by 5–8% in QE, though lr=0.5 produces slightly better TE.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Best configuration:</strong> 40×40 grid with lr=1.0 and σ=1.0 achieved lowest QE (0.157) while maintaining excellent TE (0.092).</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Two-stage clustering:</strong> 40×40 with 1,600 neurons provides meaningful data compression (8 customers per neuron), ensuring K-Means on SOM weights produces different results than direct K-Means on raw data.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Decision:</strong> Use 40×40 with lr=1.0 and σ=1.0 for SOM visualization and two-stage clustering.</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miz94r2vtq8",
   "metadata": {},
   "source": [
    "### **7.5.2 SOM Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7o33la8oi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SOM with selected best parameters\n",
    "som_best = MiniSom(\n",
    "    x=selected_grid_size,\n",
    "    y=selected_grid_size,\n",
    "    input_len=len(feats),\n",
    "    sigma=selected_sigma,\n",
    "    learning_rate=selected_lr,\n",
    "    neighborhood_function='gaussian',\n",
    "    topology='hexagonal',\n",
    "    activation_distance='euclidean',\n",
    "    random_seed=1\n",
    ")\n",
    "\n",
    "# Initialize and train\n",
    "som_best.random_weights_init(som_data)\n",
    "som_best.train_batch(som_data, num_iteration=500 * selected_grid_size * selected_grid_size, verbose=False)\n",
    "\n",
    "print(f\"SOM trained with parameters: Grid={selected_grid_size}x{selected_grid_size}, LR={selected_lr}, σ={selected_sigma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component Planes\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(feats) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(feats):\n",
    "    weights = som_best.get_weights()[:, :, idx]\n",
    "    visualize_som_grid(som_best, weights, feature, ax=axes[idx])\n",
    "\n",
    "for idx in range(len(feats), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Component Planes: Feature Distribution across SOM Grid', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e07d5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Matrix - Unified Distance Matrix\n",
    "visualize_som_grid(som_best, som_best.distance_map(), 'U-Matrix: Unified Distance Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f2c4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hit Map - Customer distribution across SOM grid\n",
    "hitsmatrix = som_best.activation_response(som_data)\n",
    "visualize_som_grid(som_best, hitsmatrix, 'Hit Map: Customer Distribution')\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    'Metric': ['Total Customers', 'Avg per Unit', 'Max in Single Unit', 'Min in Single Unit'],\n",
    "    'Value': [f\"{hitsmatrix.sum():.0f}\", f\"{hitsmatrix.mean():.2f}\", f\"{hitsmatrix.max():.0f}\", f\"{hitsmatrix.min():.0f}\"]\n",
    "}))\n",
    "\n",
    "# Count units with 0 hits\n",
    "n_zero_hit_units = np.sum(hitsmatrix == 0)\n",
    "print(\"Number of units with 0 hits:\", n_zero_hit_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd59de2a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM Visualization Summary</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Visualized the trained 40×40 SOM using three complementary outputs: Component Planes (feature distributions), U-Matrix (cluster boundaries), and Hit Map (customer density). Each visualization serves a distinct purpose in understanding the SOM's learned structure.\n",
    "  </p>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\">Understand how demographic features are distributed across the SOM grid, identify natural cluster boundaries, and assess whether customer mappings are evenly distributed or concentrated in specific regions</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Component Planes:</strong> Display each feature's weight distribution across the SOM grid. <span style=\"color:#b2182b;\">Red</span> shades indicate higher Z-scores, <span style=\"color:#2166ac;\">blue</span> shades indicate lower Z-scores</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>U-Matrix:</strong> Shows average distance between each SOM unit and its neighbors. <span style=\"color:#b2182b;\">Red</span> regions indicate high distances (cluster boundaries), <span style=\"color:#2166ac;\">blue</span> regions indicate homogeneous areas (cluster centers)</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Hit Map:</strong> Displays how many customers are mapped to each SOM unit. <span style=\"color:#b2182b;\">Red</span> units have more customers, <span style=\"color:#2166ac;\">blue</span> units have fewer</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Education and Income show strong correlation:</strong> Both Education_Level_Num and Income_Bin_Num display similar gradients with <span style=\"color:#f4a582;\">yellow/beige</span> regions (higher values, Z=+1.5 to +2.0) in the upper-left area transitioning to <span style=\"color:#2166ac;\">blue</span> regions (lower values, Z=-1.0) in the lower portion, indicating these features co-vary and define a primary segmentation axis</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Gender shows clear binary segmentation:</strong> Gender_Encoded displays distinct <span style=\"color:#b2182b;\">red</span> (Z=+1, male) and <span style=\"color:#2166ac;\">blue</span> (Z=-1, female) regions distributed in a patchy pattern across the grid, representing a secondary segmentation dimension independent of the Education-Income axis</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Geographic features exhibit correlated patterns:</strong> Province_Encoded, City_Encoded, and Location_Code_Num show similar spatial distributions with <span style=\"color:#b2182b;\">red</span> regions (common areas) concentrated in the upper-left and <span style=\"color:#2166ac;\">blue</span> regions (rare areas) elsewhere, capturing related geographic information</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>FSA shows sparse high-value hotspots:</strong> FSA_Encoded is predominantly <span style=\"color:#2166ac;\">blue</span> (Z=-0 to -1.0) with isolated <span style=\"color:#b2182b;\">red</span> points reaching extreme values (Z=+2.5), indicating most customers have low FSA frequency with a small subset from high-frequency postal codes</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Marital status shows scattered patterns:</strong> Marital_Divorced displays uniformly scattered <span style=\"color:#b2182b;\">red</span> points across a predominantly <span style=\"color:#2166ac;\">blue</span> grid, while Marital_Married shows more regional variation but no clear cluster alignment</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>U-Matrix reveals diffuse cluster boundaries:</strong> The U-Matrix shows predominantly <span style=\"color:#92c5de;\">light blue</span> regions (low inter-neuron distances, 0.2-0.4) with scattered <span style=\"color:#f4a582;\">orange</span>/<span style=\"color:#b2182b;\">red</span> points (higher distances, 0.6-1.0) throughout, suggesting gradual transitions between customer segments rather than sharp cluster boundaries</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Hit Map reveals uneven coverage:</strong> With 13,038 total customers mapped (avg 8.15 per unit), the distribution shows concentration in certain regions (max 78 per unit) while <strong>657 units (41%) remain empty</strong>. This indicates the data naturally clusters in specific SOM regions, supporting the two-stage clustering approach where K-Means can identify these dense areas</li>\n",
    "  </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c42ea9",
   "metadata": {},
   "source": [
    "### **7.5.3 Emergent SOM Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9193a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emergent SOM for Two-Stage Clustering\n",
    "# Using 40x40 grid - optimal balance between granularity and neuron coverage\n",
    "\n",
    "som_emergent = som_best  # Use already trained 40x40 SOM from Grid Search\n",
    "emergent_grid_size = 40\n",
    "\n",
    "qe_emergent = som_emergent.quantization_error(som_data)\n",
    "te_emergent = som_emergent.topographic_error(som_data)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Grid Size', 'Total Units', 'Avg Customers/Unit', 'Quantization Error', 'Topographic Error'],\n",
    "    'Value': [f'{emergent_grid_size}x{emergent_grid_size}', f'{emergent_grid_size**2:,}', f'{len(som_data)/emergent_grid_size**2:.2f}', f'{qe_emergent:.4f}', f'{te_emergent:.4f}']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c22bde",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM for Two-Stage Clustering</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Using the 40×40 SOM (1,600 units) trained with optimal Grid Search parameters (lr=1.0, σ=1.0) as foundation for two-stage clustering. K-Means will cluster the neuron weight vectors to identify final customer segments.\n",
    "  </p>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Why 40×40 Grid for Two-Stage Clustering:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Meaningful data compression:</strong> With 13,038 customers across 1,600 neurons, each neuron represents ~8.15 customers on average. This compression ratio ensures the SOM provides genuine noise reduction and prototype learning, differentiating two-stage clustering from direct K-Means on raw data</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Robust neuron coverage:</strong> The moderate density of ~8 customers per neuron ensures weight vectors are stable representations of local customer profiles, not dominated by individual outliers</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Balanced granularity:</strong> 1,600 neurons provide sufficient resolution for K-Means to identify natural cluster boundaries while avoiding the over-granularity problem where very large grids (e.g., 50×50 or larger) would produce results nearly identical to direct clustering</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Quality Metrics:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Quantization Error: 0.1571</strong> - low error indicating neurons accurately represent the underlying data distribution while providing meaningful compression</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Topographic Error: 0.0915</strong> - only 9.2% of data points have non-adjacent 1st and 2nd BMUs, confirming excellent topology preservation for interpretable clustering results</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cj4ifcdzyz",
   "metadata": {},
   "source": [
    "### **7.5.4 Defining the number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kvkqsry1kg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten SOM weights for K-Means clustering\n",
    "som_weights_flat = som_emergent.get_weights().reshape(-1, len(feats))\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Description': ['SOM weights shape', 'Total neurons', 'Features per neuron'],\n",
    "    'Value': [str(som_weights_flat.shape), emergent_grid_size**2, len(feats)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c659c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Means on SOM weights across range of k values\n",
    "som_km_k_range = range(2, 14)\n",
    "som_km_metrics = {\n",
    "    'k': [],\n",
    "    'Inertia': [],\n",
    "    'Silhouette': [],\n",
    "    'Calinski-Harabasz': [],\n",
    "    'Davies-Bouldin': []\n",
    "}\n",
    "\n",
    "# Store fitted neuron labels for later use\n",
    "som_km_fitted_neuron_labels = {}\n",
    "\n",
    "# Helper function to map neuron labels to customers via BMU\n",
    "def get_customer_labels(som, data, neuron_labels, grid_size):\n",
    "    customer_labels = []\n",
    "    for sample in data:\n",
    "        bmu = som.winner(sample)\n",
    "        neuron_idx = bmu[0] * grid_size + bmu[1]\n",
    "        customer_labels.append(neuron_labels[neuron_idx])\n",
    "    return np.array(customer_labels)\n",
    "\n",
    "for k in som_km_k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=15, random_state=1, max_iter=300)\n",
    "    km_neuron_labels = kmeans.fit_predict(som_weights_flat)\n",
    "    \n",
    "    # Store neuron labels\n",
    "    som_km_fitted_neuron_labels[k] = km_neuron_labels\n",
    "    \n",
    "    # Map neuron labels to customers via BMU\n",
    "    customer_labels = get_customer_labels(som_emergent, som_data, km_neuron_labels, emergent_grid_size)\n",
    "    \n",
    "    # Metrics on customer data\n",
    "    metrics = evaluate_clustering_metrics(som_data, customer_labels)\n",
    "    \n",
    "    som_km_metrics['k'].append(k)\n",
    "    som_km_metrics['Inertia'].append(kmeans.inertia_)\n",
    "    som_km_metrics['Silhouette'].append(metrics['Silhouette Score'])\n",
    "    som_km_metrics['Calinski-Harabasz'].append(metrics['Calinski-Harabasz Index'])\n",
    "    som_km_metrics['Davies-Bouldin'].append(metrics['Davies-Bouldin Index'])\n",
    "\n",
    "som_km_metrics_df = pd.DataFrame(som_km_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7baa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "plot_elbow_method(\n",
    "    som_km_k_range,\n",
    "    som_km_metrics_df['Inertia'].tolist(),\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means: Elbow Method for Optimal k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5420ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering metrics comparison\n",
    "plot_clustering_metrics(\n",
    "    som_km_metrics_df,\n",
    "    som_km_k_range,\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means: Clustering Metrics Evaluation'\n",
    ")\n",
    "\n",
    "som_km_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48970344",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Optimal k Selection Summary (SOM + K-Means)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Evaluated k=2 to k=13 on the 1,600 SOM neuron weight vectors using multiple validation indices to identify the optimal number of clusters for two-stage clustering.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Determine k that balances cluster quality metrics with business interpretability for actionable customer segmentation</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Elbow Method (Inertia):</strong> Total within-cluster sum of squared distances on SOM weights - look for \"elbow\" where adding clusters yields diminishing returns</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Measures cluster cohesion and separation (range -1 to 1, higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster to within-cluster variance (higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Index:</strong> Average similarity between clusters (lower is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>k=3 - Statistical Optimum:</strong> Achieves the <strong>highest Calinski-Harabasz Index (3,064)</strong> across all k values and shows a clear <strong>elbow point</strong> where inertia reduction slows significantly. Silhouette (0.203) and Davies-Bouldin (1.740) are competitive. This aligns with the 3-cluster solutions from Hierarchical and K-Means clustering, enabling cross-method comparison</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>k=6 - Best Cluster Quality:</strong> Achieves <strong>highest Silhouette (0.214)</strong> among lower k values and <strong>best Davies-Bouldin (1.654)</strong>, indicating well-separated, compact clusters. This solution provides finer granularity while maintaining strong cluster cohesion</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Trade-off Analysis:</strong> k=3 maximizes between-cluster separation (highest CH), aligns with the elbow point, and offers simplicity for high-level strategic segmentation. k=6 optimizes cluster cohesion (best Silhouette, DBI), suitable for more detailed customer differentiation</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Decision for next step:</strong> Compare k=3 vs k=6 to determine the final clustering configuration based on cluster profiles and business interpretability</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tlor1pn3pfg",
   "metadata": {},
   "source": [
    "### **7.5.5 Comparison of k Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ffc6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate solutions based on clustering metrics analysis\n",
    "# k=3\n",
    "# k=6\n",
    "\n",
    "som_km_k_candidate_1 = 3\n",
    "som_km_k_candidate_2 = 6\n",
    "\n",
    "# Use pre-fitted neuron labels from 7.6.4\n",
    "som_km_labels_k1 = som_km_fitted_neuron_labels[som_km_k_candidate_1]\n",
    "som_km_labels_k2 = som_km_fitted_neuron_labels[som_km_k_candidate_2]\n",
    "\n",
    "# Map to customers \n",
    "customer_labels_k1 = get_customer_labels(som_emergent, som_data, som_km_labels_k1, emergent_grid_size)\n",
    "customer_labels_k2 = get_customer_labels(som_emergent, som_data, som_km_labels_k2, emergent_grid_size)\n",
    "\n",
    "# Create temporary DataFrames with cluster labels\n",
    "df_temp_k1 = pd.DataFrame(som_data, columns=feats)\n",
    "df_temp_k1['Cluster'] = customer_labels_k1\n",
    "\n",
    "df_temp_k2 = pd.DataFrame(som_data, columns=feats)\n",
    "df_temp_k2['Cluster'] = customer_labels_k2\n",
    "\n",
    "# Calculate cluster profiles (mean values per cluster)\n",
    "cluster_profiles_k1 = df_temp_k1.groupby('Cluster').mean()\n",
    "cluster_profiles_k2 = df_temp_k2.groupby('Cluster').mean()\n",
    "\n",
    "# Display both profiles for comparison\n",
    "print(f\"Cluster Profiles for k={som_km_k_candidate_1}:\")\n",
    "display(cluster_profiles_k1.round(3))\n",
    "\n",
    "print(f\"\\nCluster Profiles for k={som_km_k_candidate_2}:\")\n",
    "display(cluster_profiles_k2.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={som_km_k_candidate_1: customer_labels_k1, som_km_k_candidate_2: customer_labels_k2},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='SOM + K-Means: Cluster Size Comparison (k=3 vs k=6)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d07ad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k=3\n",
    "cluster_grid_k3 = som_km_labels_k1.reshape((emergent_grid_size, emergent_grid_size))\n",
    "visualize_som_grid(som_emergent, cluster_grid_k3.astype(float), f'SOM + K-Means Clustering (k={som_km_k_candidate_1})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31ad35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k=6\n",
    "cluster_grid_k6 = som_km_labels_k2.reshape((emergent_grid_size, emergent_grid_size))\n",
    "visualize_som_grid(som_emergent, cluster_grid_k6.astype(float), f'SOM + K-Means Clustering (k={som_km_k_candidate_2})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cce4af6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: k=3 selected</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K=3 emerges as the optimal solution, primarily driven by the <strong>clear elbow point</strong> in the inertia curve where additional clusters yield diminishing returns. This is further supported by the <strong>highest Calinski-Harabasz Index (3,064)</strong> across all k values, indicating optimal between-cluster separation. The SOM grid visualization shows three clearly defined, contiguous cluster regions with minimal fragmentation. The cluster size distribution is well-balanced (31.9%, 38.3%, 29.8%), ensuring all segments are substantial enough for actionable marketing strategies. While k=6 achieves better Silhouette (0.214 vs 0.203) and Davies-Bouldin (1.654 vs 1.740), the elbow criterion clearly favors k=3, and k=6 produces smaller clusters (smallest at 8.7%) with more fragmented spatial distribution. K=3 aligns with the solutions from Hierarchical and K-Means clustering, enabling direct cross-method comparison.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eilzy3wbjvh",
   "metadata": {},
   "source": [
    "### **7.5.6 Final SOM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed950d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select k=3 and use pre-fitted labels\n",
    "selected_k = 3\n",
    "df_demographic_a_scaled['Cluster_SOM_KMeans'] = customer_labels_k1\n",
    "\n",
    "# Calculate final metrics\n",
    "som_labels_final = df_demographic_a_scaled['Cluster_SOM_KMeans'].values\n",
    "som_final_metrics = evaluate_clustering_metrics(df_demographic_a_scaled[feats], som_labels_final)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "demo_clustering_results['SOM + K-Means'] = {\n",
    "    'k': selected_k,\n",
    "    'Silhouette': som_final_metrics['Silhouette Score'],\n",
    "    'Calinski-Harabasz': som_final_metrics['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': som_final_metrics['Davies-Bouldin Index'],\n",
    "    'R2': get_rsq(df_demographic_a_scaled[feats + ['Cluster_SOM_KMeans']], feats, 'Cluster_SOM_KMeans'),\n",
    "    'labels': som_labels_final\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qlm1qq4mf1",
   "metadata": {},
   "source": [
    "### **7.5.7 SOM Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1595a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Profile Heatmap - Z-scores of demographic features per cluster (SOM)\n",
    "# Reuse cluster_profiles\n",
    "cluster_profiles_som = cluster_profiles_k1\n",
    "som_population_mean = df_demographic_a_scaled[feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    cluster_profiles_som,\n",
    "    som_population_mean,\n",
    "    GROUP80_palette_continuous,\n",
    "    title='SOM + K-Means Clustering: Demographic Profiles (n=3)\\nStandardized Z-Scores per Cluster'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234ded3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    df_demographic_a_scaled['Cluster_SOM_KMeans'].values,\n",
    "    selected_k,\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means Clustering - Final Cluster Sizes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d61fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "som_feature_variance = cluster_profiles_som.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    som_feature_variance,\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means Clustering: Feature Importance Analysis\\nWhich Demographics Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance as DataFrame\n",
    "som_feature_importance_df = pd.DataFrame({\n",
    "    'Feature': som_feature_variance.index,\n",
    "    'Variance': som_feature_variance.values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "display(som_feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b615c5a0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM + K-Means Clustering Profiling Summary (k=3)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed demographic characteristics of the final 3 SOM + K-Means clusters to identify distinct customer segments. The two-stage approach produces segmentation driven by Education, Income, and Geographic location, with Gender showing no differentiation.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (31.9%, n=4,156) - Common Regions Higher-Income:</strong> Common cities (Z=+1.03), common provinces (Z=+0.52), common FSA (Z=+0.59), higher education (Z=+0.58), higher income (Z=+0.50), married (Z=+0.66).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (38.3%, n=4,997) - Rare Regions Higher-Income:</strong> Rare cities (Z=-0.85), rare provinces (Z=-0.45), rare FSA (Z=-0.50), higher education (Z=+0.57), higher income (Z=+0.49), married (Z=+0.67).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (29.8%, n=3,885) - Lower-Education Lower-Income:</strong> Average geographic distribution, significantly lower education (Z=-1.35), significantly lower income (Z=-1.17), less married (Z=+0.38).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Segmentation Driver - Education (Variance: 1.24):</strong> Education level emerges as the strongest differentiator, isolating Cluster 2 with significantly lower education.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Secondary Drivers - Income (0.92) & City (0.88):</strong> Income correlates strongly with Education; City frequency differentiates Clusters 0 and 1 (common vs rare regions).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Gender Independence (Variance: 0.0003):</strong> Gender shows virtually no differentiation across clusters, indicating the SOM topology does not segment by gender.</li>\n",
    "    </ul> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906a4b5f",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='8'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>8. Behavorial Clustering</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59634335",
   "metadata": {},
   "source": [
    "## **8.0 Multivariate Outlier Detection (DBSCAN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff08560",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">DBSCAN for Multivariate Outlier Detection</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a <strong>density-based algorithm</strong> that groups points in high-density regions while explicitly identifying outliers as noise. Unlike univariate methods (IQR, Z-score), DBSCAN detects <strong>multivariate outliers</strong>, meaning customers whose combination of behavioral features is anomalous even if individual features appear normal. Points labeled as noise (cluster = -1) are natural outlier candidates that can distort centroid-based clustering algorithms and should be removed before subsequent analysis.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Pick a Point:</strong> Start with an arbitrary <strong>unvisited</strong> point and mark it as visited.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Check its Neighborhood (ε-neighborhood):</strong> Retrieve all points within distance <strong>ε</strong> of the selected point and count them.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Core vs. Border/Noise Decision:</strong> \n",
    "            If the neighborhood size is <strong>≥ MinPts</strong>, the point is a <strong>core point</strong> and a new cluster is started. \n",
    "            If it is <strong>< MinPts</strong>, the point is temporarily labeled as <strong>noise</strong> (it may later become a <strong>border point</strong> if found within ε of a core point).\n",
    "        </li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Expand the Cluster:</strong> For a core point, add all ε-neighbors to the cluster. Maintain a \"seed set\" (queue) of neighbors to process.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Recursive Growth via Core Neighbors:</strong> For each neighbor in the seed set, if it is unvisited, mark it visited and compute its ε-neighborhood. \n",
    "            If that neighbor is also a <strong>core point</strong>, add its neighbors to the seed set. This continues until no new points can be added.\n",
    "        </li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Outlier Identification:</strong> Points not assigned to any cluster remain labeled as <strong>noise</strong> (cluster = -1). These are the multivariate outliers.</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Why DBSCAN for Outlier Detection:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Multivariate sensitivity:</strong> Detects customers with unusual feature combinations (e.g., high distance variability but zero redemption) that univariate methods miss</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>No distribution assumptions:</strong> Does not assume normality, making it robust for behavioral data with skewed distributions</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Automatic outlier labeling:</strong> Noise points are natural outlier candidates without requiring manual threshold setting</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>ε Selection via k-distance Graph:</strong> Compute the k-distance curve with <strong>k=8</strong> (rule of thumb: <strong>2 × n_features</strong>, here 4 features) and identify the elbow region for ε candidates</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Parameter Grid Evaluation:</strong> Test combinations of ε and MinPts, tracking <strong>noise count</strong> and <strong>noise percentage</strong> to achieve target outlier rate (typically 1-5%)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Outlier Identification:</strong> Select parameters that identify a meaningful outlier group, label noise points as multivariate outliers</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Clean Dataset:</strong> Remove identified outliers from the behavioral dataset and proceed with clustering algorithms (8.1-8.5) on the cleaned data</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a921d36",
   "metadata": {},
   "source": [
    "### **8.0.1 Selecting ε and MinPts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060178c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-distance graph for ε selection\n",
    "# k = 2 × n_features (4 behavioral features -> k=8)\n",
    "behavioral_feats = [ 'distance_variability', 'companion_flight_ratio', \n",
    "                    'flight_regularity', 'redemption_frequency']\n",
    "\n",
    "k = 8\n",
    "neigh = NearestNeighbors(n_neighbors=k)\n",
    "neigh.fit(df_behavioral_a_scaled[behavioral_feats])\n",
    "distances, _ = neigh.kneighbors(df_behavioral_a_scaled[behavioral_feats])\n",
    "k_distances = np.sort(distances[:, -1])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(k_distances, color=CUSTOM_HEX[0], linewidth=1.5)\n",
    "ax.axhline(y=1, color='red', linestyle='--', linewidth=1.5, label='ε = 1 Potential eps (elbow)')\n",
    "ax.set_xlabel('Sorted Points', fontweight='bold', fontsize=11)\n",
    "ax.set_ylabel(f'{k}-NN Distance', fontweight='bold', fontsize=11)\n",
    "ax.set_title('DBSCAN Outlier Detection - k-Distance Graph', fontweight='bold', fontsize=13, pad=15)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e47c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid evaluation\n",
    "# The curve is fairly smooth until 0.8-1.2, then it starts rising sharply (and explodes toward 4 at the very end)\n",
    "# So a first ε candidate (with MinPts round about 18) is around 0.8 - 1.2 — i.e., just before the steep ramp begins\n",
    "\n",
    "eps_values = [0.8, 0.9, 1.0, 1.1, 1.2]\n",
    "minpts_values = [7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "dbscan_outlier_results = []\n",
    "\n",
    "for eps in eps_values:\n",
    "    for minpts in minpts_values:\n",
    "        dbscan_model = DBSCAN(eps=eps, min_samples=minpts, n_jobs=-1)\n",
    "        labels = dbscan_model.fit_predict(df_behavioral_a_scaled[behavioral_feats])\n",
    "        n_noise = np.sum(labels == -1)\n",
    "        noise_pct = (n_noise / len(labels)) * 100\n",
    "        dbscan_outlier_results.append({\n",
    "            \"eps\": eps, \n",
    "            \"min_samples\": minpts, \n",
    "            \"n_outliers\": n_noise,\n",
    "            \"outlier_pct\": round(noise_pct, 2)\n",
    "        })\n",
    "\n",
    "dbscan_outlier_results_df = pd.DataFrame(dbscan_outlier_results)\n",
    "dbscan_outlier_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd32590",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">DBSCAN Parameter Selection Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Determined ε and MinPts for multivariate outlier detection using a k-distance plot to anchor ε at the elbow, then validated via grid search covering 5 ε values (0.8 to 1.2) and 9 MinPts values (7 to 15), totaling 45 parameter combinations.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>ε selection:</strong> k-distance graph (k=8) shows clear elbow around ε=1.0, representing the natural density break in the 4-dimensional behavioral feature space.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>MinPts selection:</strong> Following the rule of thumb MinPts = 2 × n_features, MinPts=8 ensures robust local density estimation for 4 behavioral features.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Grid search pattern:</strong> Lower ε values (0.8) produced 2.3% to 3.5% outliers, while higher ε values (1.2) dropped below 0.8%. The ε=1.0 range yields 1.1% to 1.6% outliers across MinPts values.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Selected parameters:</strong> ε=1.0, MinPts=8 identifies <strong>153 outliers (1.17%)</strong>, representing customers with unusual behavioral patterns in sparsely populated regions of the feature space.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7d536b",
   "metadata": {},
   "source": [
    "### **8.0.2 Outlier Identification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f15417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final DBSCAN parameters for outlier detection\n",
    "chosen_eps_outlier = 1.0\n",
    "chosen_minpts_outlier = 8\n",
    "\n",
    "# Fit DBSCAN and identify outliers (label = -1)\n",
    "dbscan_outlier = DBSCAN(eps=chosen_eps_outlier, min_samples=chosen_minpts_outlier, n_jobs=-1)\n",
    "outlier_labels = dbscan_outlier.fit_predict(df_behavioral_a_scaled[behavioral_feats])\n",
    "\n",
    "outlier_mask = outlier_labels == -1\n",
    "behavioral_outlier_indices = df_behavioral_a_scaled[outlier_mask].index.tolist()\n",
    "\n",
    "# Create clean behavioral dataset\n",
    "df_behavioral_clean = df_behavioral_a_scaled[~outlier_mask].copy()\n",
    "df_behavioral_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545038ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Visualization of outliers in PCA space\n",
    "\n",
    "pca_3d = PCA(n_components=3)\n",
    "behavioral_pca_3d = pca_3d.fit_transform(df_behavioral_a_scaled[behavioral_feats])\n",
    "\n",
    "fig = plt.figure(figsize=(12, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(behavioral_pca_3d[~outlier_mask, 0], behavioral_pca_3d[~outlier_mask, 1], behavioral_pca_3d[~outlier_mask, 2],\n",
    "           c=CUSTOM_HEX[2], alpha=0.15, s=8, label=f'Clean ({(~outlier_mask).sum():,})')\n",
    "ax.scatter(behavioral_pca_3d[outlier_mask, 0], behavioral_pca_3d[outlier_mask, 1], behavioral_pca_3d[outlier_mask, 2],\n",
    "           c='red', alpha=0.9, s=30, marker='x', label=f'Outliers ({outlier_mask.sum():,})')\n",
    "\n",
    "ax.set_xlabel(f'PC1 ({pca_3d.explained_variance_ratio_[0]:.1%})', fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({pca_3d.explained_variance_ratio_[1]:.1%})', fontweight='bold')\n",
    "ax.set_zlabel(f'PC3 ({pca_3d.explained_variance_ratio_[2]:.1%})', fontweight='bold')\n",
    "ax.set_title('DBSCAN Outlier Detection - 3D PCA Projection', fontweight='bold', fontsize=13, pad=15)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833bf92e",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">DBSCAN Outlier Identification Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Applied DBSCAN (ε=1.0, MinPts=8) to identify multivariate outliers in the 4-dimensional behavioral feature space. The 3D PCA projection confirms that outliers (red) are located at the periphery or in sparsely populated regions, validating the density-based detection.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Results:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Outliers removed:</strong> 153 customers (1.17%)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Clean dataset:</strong> 12,924 customers</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Next Steps:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Behavioral clustering (8.1-8.5) proceeds on <strong>df_behavioral_clean</strong>. Outlier indices are stored in <strong>behavioral_outlier_indices</strong> for post-hoc analysis/profiling.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4841f7c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a4fac4",
   "metadata": {},
   "source": [
    "## **8.1 Hierarchical Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a178d7af",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Hierarchical Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Hierarchical clustering is an <strong>agglomerative bottom-up algorithm</strong> that builds a tree-like structure (dendrogram) by iteratively merging the closest data points or clusters. Unlike K-Means, it does not require pre-specifying the number of clusters and provides a complete hierarchical view of data relationships.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialization:</strong> Start with each data point as its own cluster (n clusters for n points)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Distance Calculation:</strong> Compute pairwise distances between all clusters using a linkage criterion</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Merge Step:</strong> Iteratively merge the two closest clusters into one larger cluster</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Repeat:</strong> Continue until all points are merged into a single cluster, forming a hierarchical tree (dendrogram)</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Linkage Method Selection:</strong> Test Ward, Complete, Average, and Single linkage methods using CCC (Cophenetic Correlation Coefficient) and R² (Variance Explained)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Optimal k Selection:</strong> For the best linkage method (Ward), identify optimal k using Silhouette, Calinski-Harabasz, Davies-Bouldin indices and dendrogram visual inspection</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Solution Comparison:</strong> Compare k=2 vs k=6 solutions by examining cluster size distributions and feature profiles</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model & Profiling:</strong> Fit final Ward linkage model with k=6 and analyze behavioral characteristics of each cluster</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bd4645",
   "metadata": {},
   "source": [
    "### **8.1.1 Finding the best Linkage Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare linkage methods using two complementary metrics:\n",
    "# 1. CCC (Cophenetic Correlation Coefficient)\n",
    "# 2. R²\n",
    "\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single']\n",
    "\n",
    "# Use plot_linkage_comparison\n",
    "ccc_df_behav, r2_results_all_behav = plot_linkage_comparison(\n",
    "    df=df_behavioral_clean,\n",
    "    linkage_methods=linkage_methods,\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='Linkage Method Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266e96d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Linkage Method Selection Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Compared four linkage methods (Ward, Complete, Average, Single) using dendrogram quality and clustering performance metrics to identify the optimal approach for hierarchical clustering of behavioral features.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Determine which linkage criterion best preserves hierarchical structure while maximizing clustering quality</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Linkage Methods Tested:</strong> Ward (minimizes within-cluster variance), Complete (maximum distance), Average (mean distance), Single (minimum distance)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>CCC (Cophenetic Correlation Coefficient):</strong> Measures how faithfully the dendrogram preserves pairwise distances (higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>R² (Variance Explained):</strong> Proportion of variance explained by clustering across k=2 to 10 (higher is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>CCC Results:</strong> Average (0.558) achieves highest dendrogram preservation, followed by Single (0.485), Complete (0.388), and Ward (0.354)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>R² Results:</strong> Ward consistently explains the most variance across all k values (reaching 0.52 at k=10), outperforming Average (0.45), Complete (0.11), and Single (0.01). Note that Ward has an inherent R² advantage because its optimization criterion (minimizing within-cluster variance) directly maximizes R²</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Decision:</strong> Selected Ward linkage. Although Ward's R² advantage is mathematically built-in, this is precisely what we want for customer segmentation: compact, well-separated clusters suitable for actionable marketing personas. DBSCAN has already removed multivariate outliers, mitigating Ward's sensitivity to extreme values</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35839f12",
   "metadata": {},
   "source": [
    "### **8.1.2 Defining the number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine optimal k using multiple evaluation metrics\n",
    "# Ward linkage selected\n",
    "\n",
    "hc_k_range_behav = range(2, 11)\n",
    "hc_metrics_behav = {\n",
    "    'k': [],\n",
    "    'Silhouette': [],\n",
    "    'Calinski-Harabasz': [],\n",
    "    'Davies-Bouldin': []\n",
    "}\n",
    "\n",
    "for k in hc_k_range_behav:\n",
    "    hc = AgglomerativeClustering(n_clusters=k, linkage='ward', metric='euclidean')\n",
    "    labels = hc.fit_predict(df_behavioral_clean)\n",
    "    \n",
    "    metrics = evaluate_clustering_metrics(df_behavioral_clean, labels)\n",
    "    \n",
    "    hc_metrics_behav['k'].append(k)\n",
    "    hc_metrics_behav['Silhouette'].append(metrics['Silhouette Score'])\n",
    "    hc_metrics_behav['Calinski-Harabasz'].append(metrics['Calinski-Harabasz Index'])\n",
    "    hc_metrics_behav['Davies-Bouldin'].append(metrics['Davies-Bouldin Index'])\n",
    "    \n",
    "\n",
    "# Create metrics DataFrame\n",
    "hc_metrics_df_behav = pd.DataFrame(hc_metrics_behav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133b797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering metrics\n",
    "plot_clustering_metrics(\n",
    "    hc_metrics_df_behav,\n",
    "    hc_k_range_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering: Optimal k Evaluation (Ward Linkage)'\n",
    ")\n",
    "\n",
    "# Display metrics table\n",
    "hc_metrics_df_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995c78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dendrogram visualization for visual confirmation of cluster structure\n",
    "\n",
    "linkage_matrix_behav = linkage(df_behavioral_clean, method='ward', metric='euclidean')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "dendrogram(\n",
    "    linkage_matrix_behav,\n",
    "    ax=ax,\n",
    "    truncate_mode='lastp',\n",
    "    p=30,\n",
    "    leaf_font_size=10,\n",
    "    show_leaf_counts=True,\n",
    "    color_threshold=0.7*max(linkage_matrix_behav[:,2])\n",
    ")\n",
    "\n",
    "ax.set_title('Hierarchical Clustering Dendrogram (Ward Linkage)\\nVisual Confirmation of Natural Grouping Structure', \n",
    "             fontweight='bold', fontsize=14, pad=15)\n",
    "ax.set_xlabel('Sample Index or Cluster Size', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('Euclidean Distance', fontsize=11, fontweight='bold')\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e07f4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Optimal k Selection Summary (Hierarchical Clustering)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Multi-metric approach combining internal validation indices and dendrogram visual inspection to determine optimal k for hierarchical clustering with Ward linkage.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Identify the number of clusters (k) that maximizes cluster quality while maintaining business interpretability</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Measures how similar data points are to their own cluster vs. neighboring clusters (range -1 to 1, higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster to within-cluster variance (higher values indicate better-defined clusters)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Index:</strong> Average similarity between each cluster and its most similar cluster (lower values indicate better separation)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Dendrogram Analysis:</strong> Visual inspection of hierarchical tree structure to identify natural cluster boundaries</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Decreases steadily from k=2 (0.169) to k=10 (0.086), with k=2 achieving the highest score</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Peak:</strong> Maximum at k=2 (2370), then declining steadily; k=3 (2168) and k=4 (2112) still maintain reasonable between-cluster separation</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Minimum:</strong> Best separation at k=6 (1.66), followed by k=5 (1.70); k=2 shows DBI of 1.94, k=3 shows worst DBI (2.03)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate Selection:</strong> k=2 and k=6 emerge as candidates. k=2 achieves optimal Silhouette (0.169) and CH (2370), corresponding to the primary dendrogram split at Euclidean distance 120. k=6 offers the best cluster separation (DBI 1.66) with Silhouette (0.099) and CH (1833), providing finer behavioral granularity for marketing personas</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e0590",
   "metadata": {},
   "source": [
    "### **8.1.3 Comparison of Clustering Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e727998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate solutions based on clustering metrics analysis\n",
    "# k=2\n",
    "# k=6\n",
    "\n",
    "hc_k_candidate_1_behav = 2\n",
    "hc_k_candidate_2_behav = 6\n",
    "\n",
    "# Fit both candidate solutions\n",
    "hc_k1_behav = AgglomerativeClustering(n_clusters=hc_k_candidate_1_behav, linkage='ward', metric='euclidean')\n",
    "hc_k2_behav = AgglomerativeClustering(n_clusters=hc_k_candidate_2_behav, linkage='ward', metric='euclidean')\n",
    "\n",
    "hc_labels_k1_behav = hc_k1_behav.fit_predict(df_behavioral_clean)\n",
    "hc_labels_k2_behav = hc_k2_behav.fit_predict(df_behavioral_clean)\n",
    "\n",
    "# Create temporary DataFrames with cluster labels\n",
    "df_temp_k1 = df_behavioral_clean.copy()\n",
    "df_temp_k1['Cluster'] = hc_labels_k1_behav\n",
    "\n",
    "df_temp_k2 = df_behavioral_clean.copy()\n",
    "df_temp_k2['Cluster'] = hc_labels_k2_behav\n",
    "\n",
    "# Calculate cluster profiles (mean values per cluster)\n",
    "cluster_profiles_k1 = df_temp_k1.groupby('Cluster').mean()\n",
    "cluster_profiles_k2 = df_temp_k2.groupby('Cluster').mean()\n",
    "\n",
    "# Display both profiles for comparison\n",
    "print(f\"\\nCluster Profiles for k={hc_k_candidate_1_behav}:\")\n",
    "display(cluster_profiles_k1.round(3))\n",
    "\n",
    "print(f\"\\n\\nCluster Profiles for k={hc_k_candidate_2_behav}:\")\n",
    "display(cluster_profiles_k2.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={hc_k_candidate_1_behav: hc_labels_k1_behav, hc_k_candidate_2_behav: hc_labels_k2_behav},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering: Cluster Size Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94e754b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: k=6 selected</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K=6 emerges as the optimal solution, primarily driven by the <strong>lowest Davies-Bouldin Index (1.66)</strong> across all k values, indicating best cluster separation. While k=2 achieves higher Silhouette (0.169 vs 0.099) and Calinski-Harabasz (2370 vs 1833), it only distinguishes two broad segments (71.3% vs 28.7%) without actionable granularity. The k=6 solution provides six well-balanced clusters (ranging from 9.6% to 31.4%), each with a distinct dominant behavioral feature, ensuring all segments are substantial enough for targeted marketing strategies. The dendrogram structure supports this split with clear secondary branching below the primary divide.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a785f81",
   "metadata": {},
   "source": [
    "### **8.1.4 Final Hierarchical Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9b0310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Hierarchical Clustering Solution\n",
    "# Using k=6 from comparison analysis\n",
    "\n",
    "hc_final_k_behav = hc_k_candidate_2_behav  # k=6\n",
    "\n",
    "# Reuse labels from comparison step\n",
    "hc_labels_final_behav = hc_labels_k2_behav\n",
    "\n",
    "# Create labeled dataset\n",
    "df_behavioral_clean_labeled_hc = df_behavioral_clean.copy()\n",
    "df_behavioral_clean_labeled_hc['Cluster'] = hc_labels_final_behav\n",
    "\n",
    "# Calculate final metrics\n",
    "hc_final_metrics_behav = evaluate_clustering_metrics(df_behavioral_clean, hc_labels_final_behav)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "if 'behavioral_clustering_results' not in dir():\n",
    "    behavioral_clustering_results = {}\n",
    "\n",
    "behavioral_clustering_results['Hierarchical'] = {\n",
    "    'k': hc_final_k_behav,\n",
    "    'Silhouette': hc_final_metrics_behav['Silhouette Score'],\n",
    "    'Calinski-Harabasz': hc_final_metrics_behav['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': hc_final_metrics_behav['Davies-Bouldin Index'],\n",
    "    'R2': get_rsq(df_behavioral_clean_labeled_hc, df_behavioral_clean.columns.tolist(), 'Cluster'),\n",
    "    'labels': hc_labels_final_behav\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759d8101",
   "metadata": {},
   "source": [
    "### **8.1.5 Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c28814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cluster Profiles Heatmap - Z-scores of behavioral features per cluster\n",
    "behavioral_feats = df_behavioral_clean.columns.tolist()\n",
    "hc_cluster_profiles_behav = df_behavioral_clean_labeled_hc.groupby('Cluster')[behavioral_feats].mean()\n",
    "hc_population_mean_behav = df_behavioral_clean[behavioral_feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    hc_cluster_profiles_behav, \n",
    "    hc_population_mean_behav, \n",
    "    GROUP80_palette_continuous,\n",
    "    title='Hierarchical Clustering: Behavioral Profiles (k=6) \\nStandardized Z-Scores per Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fe8b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    hc_labels_final_behav, \n",
    "    hc_final_k_behav, \n",
    "    CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering - Final Cluster Sizes'\n",
    ")\n",
    "\n",
    "# Display cluster size statistics\n",
    "cluster_sizes_hc_behav = pd.Series(hc_labels_final_behav).value_counts().sort_index()\n",
    "cluster_dist_df_hc_behav = pd.DataFrame({\n",
    "    'Cluster': cluster_sizes_hc_behav.index,\n",
    "    'Count': cluster_sizes_hc_behav.values,\n",
    "    'Percentage': (cluster_sizes_hc_behav.values / len(hc_labels_final_behav) * 100).round(2)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "# Features with high variance differentiate clusters most effectively\n",
    "\n",
    "hc_feature_variance_behav = hc_cluster_profiles_behav.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    hc_feature_variance_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='Hierarchical Clustering: Feature Importance Analysis\\nWhich Features Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance ranking\n",
    "hc_feature_importance_df_behav = pd.DataFrame({\n",
    "    'Feature': hc_feature_variance_behav.index,\n",
    "    'Variance': hc_feature_variance_behav.values.round(4),\n",
    "})\n",
    "\n",
    "hc_feature_importance_df_behav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb956f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Hierarchical Clustering Profiling Summary (k=6)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed behavioral characteristics of the final 6 hierarchical clusters to identify distinct customer segments. Ward linkage produces six well-separated personas, each dominated by a distinct behavioral pattern.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Cluster Profiles:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (31.4%, n=4,043) - Sporadic Mainstream:</strong> Irregular flight patterns (flight_regularity Z=+0.75), average across other features. Largest segment representing occasional travelers.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (18.2%, n=2,347) - Engaged Social Redeemer:</strong> Very high redemption activity (Z=+1.27), travels with companions (Z=+0.65). Most engaged loyalty program segment.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (9.6%, n=1,240) - Ultra-Routine Solo:</strong> Minimal destination variability (Z=-1.09), strongly solo (Z=-1.00), passive redeemer (Z=-0.73). Fixed-route business travelers.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 3 (17.4%, n=2,236) - Solo Explorer:</strong> High distance variability (Z=+1.10), travels alone (Z=-0.64), passive redeemer (Z=-0.54). Adventure-seeking solo travelers.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 4 (10.5%, n=1,353) - Regular Family Traveler:</strong> Very high companion ratio (Z=+1.25), consistent schedule (Z=-0.48), average redemption. Family vacation segment.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 5 (12.9%, n=1,666) - Ultra-Regular Commuter:</strong> Very consistent monthly patterns (Z=-1.04), passive redeemer (Z=-0.41). Predictable business commuters.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Importance (Variance across clusters):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>companion_flight_ratio (0.69):</strong> Primary driver distinguishing social vs. solo travelers</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>distance_variability (0.55):</strong> Separates explorers from routine travelers</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>redemption_frequency (0.51):</strong> Identifies engaged vs. passive loyalty members</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>flight_regularity (0.35):</strong> Distinguishes sporadic vs. consistent flyers</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bcccaa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f1f37",
   "metadata": {},
   "source": [
    "## **8.2 K-Means Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae5bdf0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">K-Means Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K-Means is an <strong>iterative partitioning algorithm</strong> that assigns data points to k clusters by minimizing within-cluster variance (Sum of Squared Errors). Unlike hierarchical clustering, it requires pre-specifying k and uses a centroid-based approach to create spherical, compact clusters.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Choose Seeds:</strong> Select k initial centroids (using k-means++ for better starting positions)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Assignment:</strong> Associate each data point with the nearest seed/centroid based on Euclidean distance</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Update Centroids:</strong> Calculate the centroids of the formed clusters as the mean of all assigned points</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Iterate:</strong> Go back to step 2 and repeat until centroids cease to be recentered (convergence)</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Optimal k Selection:</strong> Test k=2-13 using Elbow Method (Inertia/SSE), Silhouette Score, Calinski-Harabasz Index, and Davies-Bouldin Index</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Solution Comparison:</strong> Compare k=4 vs k=5 solutions by examining cluster sizes and feature profiles</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model & Profiling:</strong> Fit final K-Means model with k=5 (k-means++ initialization) and analyze behavioral characteristics of each cluster</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632effcd",
   "metadata": {},
   "source": [
    "### **8.2.1 Defining the number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d079f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Means clustering across range of k values\n",
    "# Using k-means++ initialization and multiple runs for stability\n",
    "\n",
    "km_k_range_behav = range(2, 14)\n",
    "km_metrics_behav = {\n",
    "    'k': [],\n",
    "    'Inertia': [],\n",
    "    'Silhouette': [],\n",
    "    'Calinski-Harabasz': [],\n",
    "    'Davies-Bouldin': []\n",
    "}\n",
    "\n",
    "# Store labels and silhouette samples for visualization\n",
    "km_fitted_labels_behav = {}\n",
    "km_silhouette_samples_behav = {}\n",
    "\n",
    "for k in km_k_range_behav:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=15, random_state=42, max_iter=300)\n",
    "    km_labels = kmeans.fit_predict(df_behavioral_clean)\n",
    "    \n",
    "    # Store labels and silhouette samples\n",
    "    km_fitted_labels_behav[k] = km_labels\n",
    "    km_silhouette_samples_behav[k] = silhouette_samples(df_behavioral_clean, km_labels)\n",
    "    \n",
    "    metrics = evaluate_clustering_metrics(df_behavioral_clean, km_labels)\n",
    "    \n",
    "    km_metrics_behav['k'].append(k)\n",
    "    km_metrics_behav['Inertia'].append(kmeans.inertia_)\n",
    "    km_metrics_behav['Silhouette'].append(metrics['Silhouette Score'])\n",
    "    km_metrics_behav['Calinski-Harabasz'].append(metrics['Calinski-Harabasz Index'])\n",
    "    km_metrics_behav['Davies-Bouldin'].append(metrics['Davies-Bouldin Index'])\n",
    "\n",
    "km_metrics_df_behav = pd.DataFrame(km_metrics_behav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76595c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Elbow Method\n",
    "plot_elbow_method(\n",
    "    km_k_range_behav,\n",
    "    km_metrics_df_behav['Inertia'].tolist(),\n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Elbow Method: Optimal k Selection'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be6a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette Analysis\n",
    "\n",
    "for nclus in km_k_range_behav:\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    km_labels = km_fitted_labels_behav[nclus]\n",
    "    sample_silhouette_values = km_silhouette_samples_behav[nclus]\n",
    "    silhouette_avg = km_metrics_df_behav[km_metrics_df_behav['k'] == nclus]['Silhouette'].values[0]\n",
    "    \n",
    "    y_lower = 10\n",
    "    for i in range(nclus):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[km_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "        \n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "        \n",
    "        color = CUSTOM_HEX[i % len(CUSTOM_HEX)]\n",
    "        ax.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                         0, ith_cluster_silhouette_values,\n",
    "                         facecolor=color, edgecolor=color, alpha=0.7)\n",
    "        \n",
    "        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i), fontweight='bold', fontsize=11)\n",
    "        y_lower = y_upper + 10\n",
    "    \n",
    "    ax.set_title(f\"K-Means Silhouette Analysis for k={nclus}\\nAverage Silhouette Score: {silhouette_avg:.4f}\", \n",
    "                 fontsize=14, fontweight='bold', pad=15)\n",
    "    ax.set_xlabel(\"Silhouette Coefficient Values\", fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(\"Cluster\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=2.5, \n",
    "               label=f'Average: {silhouette_avg:.4f}')\n",
    "    \n",
    "    xmin = max(-0.3, np.round(sample_silhouette_values.min() - 0.1, 2))\n",
    "    xmax = min(1.0, np.round(sample_silhouette_values.max() + 0.1, 2))\n",
    "    ax.set_xlim([xmin, xmax])\n",
    "    ax.set_ylim([0, len(df_behavioral_clean) + (nclus + 1) * 10])\n",
    "    \n",
    "    ax.set_yticks([])\n",
    "    ax.set_xticks(np.arange(xmin, xmax + 0.1, 0.1))\n",
    "    ax.legend(loc='upper right', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f852d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering metrics\n",
    "plot_clustering_metrics(\n",
    "    km_metrics_df_behav,\n",
    "    km_k_range_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Clustering: Optimal k Evaluation'\n",
    ")\n",
    "\n",
    "# Display metrics table\n",
    "km_metrics_df_behav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eba7383",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Optimal k Selection Summary (K-Means Clustering)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Evaluated k=2 to k=13 using multiple validation indices to identify the optimal number of clusters for K-Means partitioning of airline customer behavioral data.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Determine k that balances cluster quality metrics with business interpretability for actionable customer segmentation</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Elbow Method (Inertia):</strong> Total within-cluster sum of squared distances - look for \"elbow\" where adding clusters yields diminishing returns</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Measures cluster cohesion and separation (range -1 to 1, higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster to within-cluster variance (higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Index:</strong> Average similarity between clusters (lower is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Elbow Analysis:</strong> Inertia curve shows continuous decline from 38,612 (k=2) to 17,127 (k=13). The rate of decrease slows notably after k=5, suggesting an elbow region around k=4 to k=5</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Peaks at k=2 (0.194), with k=5 (0.186) and k=4 (0.185) achieving the next best scores before declining at k=6 (0.172)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Index:</strong> Improves steadily, with k=5 (1.36) outperforming k=4 (1.45), indicating better cluster separation at k=5</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate Selection:</strong> k=4 and k=5 emerge as candidates. Both achieve similar Silhouette scores (0.185 vs 0.186) while k=5 offers superior cluster separation (DBI 1.36 vs 1.45) at minimal cost to cohesion</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e746e96b",
   "metadata": {},
   "source": [
    "### **8.2.2 Comparison of Clustering Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf47d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate solutions based on clustering metrics analysis\n",
    "# k=4\n",
    "# k=5\n",
    "\n",
    "km_k_candidate_1_behav = 4\n",
    "km_k_candidate_2_behav = 5\n",
    "\n",
    "# Use pre-fitted labels\n",
    "km_labels_k1_behav = km_fitted_labels_behav[km_k_candidate_1_behav]\n",
    "km_labels_k2_behav = km_fitted_labels_behav[km_k_candidate_2_behav]\n",
    "\n",
    "# Create temporary DataFrames with cluster labels\n",
    "df_temp_k1 = df_behavioral_clean.copy()\n",
    "df_temp_k1['Cluster'] = km_labels_k1_behav\n",
    "\n",
    "df_temp_k2 = df_behavioral_clean.copy()\n",
    "df_temp_k2['Cluster'] = km_labels_k2_behav\n",
    "\n",
    "# Calculate cluster profiles (mean values per cluster)\n",
    "cluster_profiles_k1 = df_temp_k1.groupby('Cluster').mean()\n",
    "cluster_profiles_k2 = df_temp_k2.groupby('Cluster').mean()\n",
    "\n",
    "# Display both profiles for comparison\n",
    "print(f\"\\nCluster Profiles for k={km_k_candidate_1_behav}:\")\n",
    "display(cluster_profiles_k1.round(3))\n",
    "\n",
    "print(f\"\\n\\nCluster Profiles for k={km_k_candidate_2_behav}:\")\n",
    "display(cluster_profiles_k2.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={km_k_candidate_1_behav: km_labels_k1_behav, km_k_candidate_2_behav: km_labels_k2_behav},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='K-Means Clustering: Cluster Size Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe0ab6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: k=5 selected</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K=5 achieves best Davies-Bouldin Index (1.36) with comparable Silhouette to k=4 (0.186 vs 0.185). Balanced cluster sizes (17.6% to 23.6%) with distinct profiles across all four behavioral features. K=4 merges the high-redemption segment with sporadic flyers, losing a key marketing target. K=5 isolates the \"Engaged Social Redeemer\" (19.7%, redemption Z=+1.40) as a distinct actionable segment.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdc0b8a",
   "metadata": {},
   "source": [
    "### **8.2.3 Final K-Means Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2a2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final K-Means Clustering Solution\n",
    "# Using k=5 from comparison analysis\n",
    "\n",
    "km_final_k_behav = km_k_candidate_2_behav  # k=5\n",
    "\n",
    "# Reuse labels from comparison step\n",
    "km_labels_final_behav = km_labels_k2_behav\n",
    "\n",
    "# Create labeled dataset\n",
    "df_behavioral_clean_labeled_km = df_behavioral_clean.copy()\n",
    "df_behavioral_clean_labeled_km['Cluster'] = km_labels_final_behav\n",
    "\n",
    "# Calculate final metrics\n",
    "km_final_metrics_behav = evaluate_clustering_metrics(df_behavioral_clean, km_labels_final_behav)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "behavioral_clustering_results['K-Means'] = {\n",
    "    'k': km_final_k_behav,\n",
    "    'Silhouette': km_final_metrics_behav['Silhouette Score'],\n",
    "    'Calinski-Harabasz': km_final_metrics_behav['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': km_final_metrics_behav['Davies-Bouldin Index'],\n",
    "    'R2': get_rsq(df_behavioral_clean_labeled_km, df_behavioral_clean.columns.tolist(), 'Cluster'),\n",
    "    'labels': km_labels_final_behav\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3f04dd",
   "metadata": {},
   "source": [
    "### **8.2.4 Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e14530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cluster Profiles Heatmap - Z-scores of behavioral features per cluster\n",
    "km_cluster_profiles_behav = df_behavioral_clean_labeled_km.groupby('Cluster')[behavioral_feats].mean()\n",
    "km_population_mean_behav = df_behavioral_clean[behavioral_feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    km_cluster_profiles_behav, \n",
    "    km_population_mean_behav, \n",
    "    GROUP80_palette_continuous,\n",
    "    title='K-Means Clustering: Behavioral Profiles(k=5)\\nStandardized Z-Scores per Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612a3185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    km_labels_final_behav, \n",
    "    km_final_k_behav, \n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Clustering - Final Cluster Sizes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2094ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "km_feature_variance_behav = km_cluster_profiles_behav.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    km_feature_variance_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='K-Means Clustering: Feature Importance Analysis\\nWhich Features Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance ranking\n",
    "km_feature_importance_df_behav = pd.DataFrame({\n",
    "    'Feature': km_feature_variance_behav.index,\n",
    "    'Variance': km_feature_variance_behav.values.round(4)\n",
    "})\n",
    "\n",
    "km_feature_importance_df_behav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de9381b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">K-Means Clustering Profiling Summary (k=5)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed behavioral characteristics of the final 5 K-Means clusters to identify distinct customer segments. Redemption frequency, companion ratio, and flight regularity emerge as primary differentiators.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (17.6%, n=2,264) - Family Travelers:</strong> Very high companion ratio (Z=+1.18), below-average regularity (Z=-0.60), average redemption. Family vacation segment traveling together.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (21.4%, n=2,760) - Business Commuters:</strong> High flight regularity (Z=+0.89), low distance variability (Z=-0.59), passive redeemer (Z=-0.38). Predictable business travelers.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (17.7%, n=2,285) - Disengaged Solo:</strong> Very low regularity (Z=-0.91), solo traveler (Z=-0.81), passive redeemer (Z=-0.59). Re-engagement target segment.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 3 (23.6%, n=3,040) - Explorers:</strong> Very high distance variability (Z=+1.11), travels alone (Z=-0.38), average regularity. Adventure-seeking travelers visiting diverse destinations.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 4 (19.7%, n=2,536) - Engaged Loyalists:</strong> Very high redemption (Z=+1.40), travels with companions (Z=+0.37), somewhat regular (Z=+0.38). Most engaged loyalty segment.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Segmentation Driver - Redemption Frequency (Variance: 0.64):</strong> Redemption creates the clearest separation, with Cluster 4 showing significantly higher engagement.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Secondary Drivers - Companion Ratio (0.60) & Flight Regularity (0.53):</strong> Companion ratio distinguishes family vs. solo travelers, while regularity separates commuters from sporadic flyers.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Distance Variability (Variance: 0.48):</strong> Differentiates explorers (Cluster 3) from routine travelers.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07573ad4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e62363c",
   "metadata": {},
   "source": [
    "## **8.3 Mean Shift Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1881d0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Mean Shift Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Mean Shift is a <strong>density-based mode-seeking algorithm</strong> that identifies clusters by shifting a sliding window toward regions of highest point density. Unlike K-Means, it does not require pre-specifying k and can discover clusters of arbitrary shape by following the gradient of the underlying density estimate.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialize a Sliding Window:</strong> Begin with a circular kernel window centered at a point <strong>C</strong> (randomly selected or one per data point) with radius <strong>r</strong> (bandwidth)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Shift Toward Higher Density:</strong> At each iteration, compute the mean of all points inside the window and shift the center <strong>C</strong> to this mean, gradually moving toward higher-density regions</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Convergence:</strong> Repeat the shift step until the movement of the window center becomes negligible (the center has converged to a mode)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Merge Modes and Assign Clusters:</strong> Run the process from many initial centers, then merge converged centers that are within a tolerance distance. Assign each data point to the cluster of the nearest converged center. If sliding windows overlap, the densest mode (window containing the most points) is preserved and points are grouped accordingly</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Selecting the Best Bandwidth:</strong> Test bandwidth quantiles from 0.018 to 0.027 using <strong>estimate_bandwidth()</strong>, then fit Mean Shift for each bandwidth and track <strong>n_clusters</strong>, <strong>R²</strong>, and <strong>Silhouette</strong></li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Evaluation of Mean Shift Solutions:</strong> Compare q=0.027 (2 clusters) vs q=0.025 (3 clusters) by examining cluster sizes and feature profiles</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Mean Shift Clustering Solution:</strong> Select q=0.025 (bandwidth=1.07, 3 clusters)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Mean Shift Cluster Profiling:</strong> Profile the final solution using cluster profile heatmap, cluster size distribution, and feature importance</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab797c0a",
   "metadata": {},
   "source": [
    "### **8.3.1 Selecting the best Bandwidth**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340f8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate bandwidth values\n",
    "bandwidth_quantiles_behav = [0.018, 0.02, 0.021, 0.0215, 0.023, 0.024, 0.025, 0.026, 0.027]\n",
    "ms_results_behav = []\n",
    "\n",
    "X_ms_behav = df_behavioral_clean[behavioral_feats]\n",
    "\n",
    "for q in bandwidth_quantiles_behav:\n",
    "    bw = estimate_bandwidth(X_ms_behav, quantile=q, random_state=1)\n",
    "    ms = MeanShift(bandwidth=bw, bin_seeding=True, n_jobs=-1)\n",
    "    labels = ms.fit_predict(X_ms_behav)\n",
    "\n",
    "    n_clusters = len(np.unique(labels))\n",
    "\n",
    "    # R²\n",
    "    df_tmp = X_ms_behav.copy()\n",
    "    df_tmp[\"labels\"] = labels\n",
    "    r2 = get_rsq(df_tmp, behavioral_feats, \"labels\")\n",
    "\n",
    "    # Silhouette (only defined if >= 2 clusters)\n",
    "    sil = silhouette_score(X_ms_behav, labels) if n_clusters >= 2 else np.nan\n",
    "\n",
    "    ms_results_behav.append({\n",
    "        \"quantile\": q,\n",
    "        \"bandwidth\": float(bw),\n",
    "        \"n_clusters\": int(n_clusters),\n",
    "        \"R2\": float(r2),\n",
    "        \"Silhouette\": float(sil) if not np.isnan(sil) else np.nan\n",
    "    })\n",
    "\n",
    "ms_results_df_behav = pd.DataFrame(ms_results_behav).sort_values(\"quantile\", ascending=False)\n",
    "ms_results_df_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_meanshift_quantile_vs_clusters(ms_results_df_behav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e635f3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Mean Shift Bandwidth Selection Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Determined the Mean Shift kernel bandwidth by testing a targeted set of bandwidth quantiles. For each quantile, bandwidth was estimated via <strong>estimate_bandwidth()</strong>, Mean Shift was fitted, and the solution was evaluated using <strong>cluster count</strong>, <strong>R²</strong>, and <strong>Silhouette</strong> to balance segmentation granularity and separation quality.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Select a bandwidth regime that yields interpretable, stable clusters with good separation, while avoiding over-fragmentation (many micro-clusters) or collapse into too few clusters</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Quantile scan (bandwidth selection):</strong> Evaluated quantiles from 0.018 to 0.027 to observe how cluster count changes as bandwidth varies.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Evaluation criteria:</strong> Compared <strong>n_clusters</strong> (solution granularity), <strong>Silhouette</strong> (cluster separation) and <strong>R²</strong> (variance explained).</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster count pattern:</strong> Higher quantiles (q=0.027) produce 2 clusters, q=0.025-0.026 yield 3 clusters, q=0.021-0.024 produce 5-6 clusters, and lower quantiles (q=0.018) fragment into 16 micro-clusters.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate A (q=0.027):</strong> 2 clusters with highest Silhouette (0.241) but insufficient granularity for actionable segmentation.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate B (q=0.025):</strong> 3 clusters with second-highest Silhouette (0.147), R²=0.072, bandwidth=1.068. Provides meaningful granularity while maintaining good cluster separation.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Decision for next step:</strong> Select q=0.025 (3 clusters) as candidate for comparison, balancing interpretability with cluster quality metrics.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821a201",
   "metadata": {},
   "source": [
    "### **8.3.2 Evaluation of Mean Shift Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab65af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate Mean Shift solutions based on previous evaluation results\n",
    "# Candidate A: quantile=0.027 -> 2 Clusters\n",
    "# Candidate B: quantile=0.025 -> 3 Clusters\n",
    "\n",
    "ms_q_1_behav = 0.027\n",
    "ms_q_2_behav = 0.025\n",
    "\n",
    "X_ms_behav = df_behavioral_clean[behavioral_feats]\n",
    "\n",
    "# Get bandwidths from the already computed results table\n",
    "ms_bw_1_behav = float(ms_results_df_behav.loc[ms_results_df_behav[\"quantile\"] == ms_q_1_behav, \"bandwidth\"].iloc[0])\n",
    "ms_bw_2_behav = float(ms_results_df_behav.loc[ms_results_df_behav[\"quantile\"] == ms_q_2_behav, \"bandwidth\"].iloc[0])\n",
    "\n",
    "# Fit both candidate solutions\n",
    "ms_cand_1_behav = MeanShift(bandwidth=ms_bw_1_behav, bin_seeding=True, n_jobs=-1)\n",
    "ms_cand_2_behav = MeanShift(bandwidth=ms_bw_2_behav, bin_seeding=True, n_jobs=-1)\n",
    "\n",
    "ms_labels_1_behav = ms_cand_1_behav.fit_predict(X_ms_behav)\n",
    "ms_labels_2_behav = ms_cand_2_behav.fit_predict(X_ms_behav)\n",
    "\n",
    "# Cluster profiles\n",
    "df_temp_ms1 = df_behavioral_clean.copy()\n",
    "df_temp_ms1[\"Cluster\"] = ms_labels_1_behav\n",
    "cluster_profiles_ms1_behav = df_temp_ms1.groupby(\"Cluster\")[behavioral_feats].mean()\n",
    "\n",
    "df_temp_ms2 = df_behavioral_clean.copy()\n",
    "df_temp_ms2[\"Cluster\"] = ms_labels_2_behav\n",
    "cluster_profiles_ms2_behav = df_temp_ms2.groupby(\"Cluster\")[behavioral_feats].mean()\n",
    "\n",
    "display(cluster_profiles_ms1_behav.round(3))\n",
    "display(cluster_profiles_ms2_behav.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={\n",
    "        f\"q={ms_q_1_behav} (bw={ms_bw_1_behav:.2f})\": ms_labels_1_behav,\n",
    "        f\"q={ms_q_2_behav} (bw={ms_bw_2_behav:.2f})\": ms_labels_2_behav,\n",
    "    },\n",
    "    palette=CUSTOM_HEX,\n",
    "    title=\"Mean Shift Clustering: Cluster Size Comparison (Quantile & Bandwidth)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6927f7cd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: q=0.025 selected (3 clusters), but limited practical value</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Both solutions produce highly imbalanced clusters: q=0.027 yields one dominant cluster (97.3%) plus one micro-cluster (2.7%), while q=0.025 yields one dominant cluster (95.3%) plus two micro-clusters (2.1%, 2.7%). Mean Shift identifies density modes but the behavioral feature space has insufficient natural density separation for actionable segmentation. The algorithm finds one large mainstream mode with small outlier groups rather than distinct behavioral personas.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e7c94",
   "metadata": {},
   "source": [
    "### **8.3.3 Final Mean Shift Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add45c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Mean Shift Clustering Solution\n",
    "# Using quantile from comparison analysis\n",
    "\n",
    "chosen_quantile_ms_behav = ms_q_2_behav  # 0.025\n",
    "chosen_bandwidth_ms_behav = ms_bw_2_behav\n",
    "\n",
    "# Reuse labels from comparison step\n",
    "ms_labels_final_behav = ms_labels_2_behav\n",
    "\n",
    "df_behavioral_clean[\"ms_cluster\"] = ms_labels_final_behav\n",
    "\n",
    "# Calculate final metrics\n",
    "ms_final_metrics_behav = evaluate_clustering_metrics(df_behavioral_clean[behavioral_feats], ms_labels_final_behav)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "behavioral_clustering_results['Mean Shift'] = {\n",
    "    'k': len(np.unique(ms_labels_final_behav)),\n",
    "    'Silhouette': ms_final_metrics_behav['Silhouette Score'],\n",
    "    'Calinski-Harabasz': ms_final_metrics_behav['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': ms_final_metrics_behav['Davies-Bouldin Index'],\n",
    "    'R2': ms_results_df_behav[ms_results_df_behav['quantile'] == chosen_quantile_ms_behav]['R2'].values[0],\n",
    "    'labels': ms_labels_final_behav\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79d612",
   "metadata": {},
   "source": [
    "### **8.3.4 Mean Shift Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Profile Heatmap - Z-scores of behavioral features per cluster (Mean Shift)\n",
    "ms_cluster_profiles_behav = (df_behavioral_clean\n",
    "                       .groupby(\"ms_cluster\")[behavioral_feats]\n",
    "                       .mean())\n",
    "ms_population_mean_behav = df_behavioral_clean[behavioral_feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    ms_cluster_profiles_behav,\n",
    "    ms_population_mean_behav,\n",
    "    GROUP80_palette_continuous,\n",
    "    title=\"Mean Shift - Cluster Profiles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster sizes\n",
    "nclus_ms_behav = len(np.unique(ms_labels_final_behav))\n",
    "plot_cluster_sizes(\n",
    "    ms_labels_final_behav,\n",
    "    nclus_ms_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title=\"Mean Shift - Final Cluster Sizes\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "ms_feature_variance_behav = ms_cluster_profiles_behav.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    ms_feature_variance_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title=\"Mean Shift Clustering: Feature Importance Analysis\\nWhich Features Differentiate Clusters?\"\n",
    ")\n",
    "\n",
    "# Display feature importance ranking\n",
    "ms_feature_importance_df_behav = pd.DataFrame({\n",
    "    \"Feature\": ms_feature_variance_behav.index,\n",
    "    \"Variance\": ms_feature_variance_behav.values.round(4),\n",
    "})\n",
    "\n",
    "ms_feature_importance_df_behav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b65b8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Mean Shift Profiling Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Profiled the final Mean Shift solution (q=0.025, 3 clusters). Highly imbalanced: one dominant mainstream cluster captures 95.3% of customers.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (95.3%, n=12,273) - Mainstream:</strong> Near-average across all features. Represents the dense core of the behavioral feature space.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (2.1%, n=266) - Extreme Explorer:</strong> Very high distance variability (Z=+1.84), solo (Z=-1.05), regular schedule (Z=-1.19).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (2.7%, n=346) - Extreme Sporadic Routine:</strong> Very low distance variability (Z=-1.67), highly irregular (Z=+1.33).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Driver - Distance Variability (Variance: 3.08):</strong> Mean Shift separates only extreme outliers on distance patterns, failing to segment the mainstream population meaningfully.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c448877",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac61cacb",
   "metadata": {},
   "source": [
    "## **8.4 Gaussian Mixture Models (GMM)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7639b5ae",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Gaussian Mixture Model (GMM) Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        GMM is a <strong>probabilistic clustering algorithm</strong> that models data as a mixture of k Gaussian distributions. Unlike hard clustering (K-Means, Hierarchical), GMM provides soft assignments where each point has a probability of belonging to each cluster, enabling uncertainty quantification and overlapping segments.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialization:</strong> Randomly initialize k Gaussian components (each with mean, covariance, and mixing weight)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Expectation Step (E-step):</strong> Calculate probability that each data point belongs to each Gaussian component using current parameters</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Maximization Step (M-step):</strong> Update component parameters (means, covariances, weights) to maximize the likelihood of the data given current assignments</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Convergence:</strong> Iterate E-step and M-step until parameters stabilize (EM algorithm converges to local maximum)</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Covariance Type Selection:</strong> Compare full, tied, diag, and spherical covariance structures using BIC and AIC to identify the best fitting model</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Component Selection:</strong> For diag covariance, test n=2-10 components and evaluate using BIC, AIC, Silhouette, and R²</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Candidate Comparison:</strong> Compare n=3 vs n=4 components using cluster profiles, assignment uncertainty, and cluster size distribution</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model and Profiling:</strong> Selected n=3 components with diag covariance for better assignment certainty (12.6% uncertain vs 29.8%), producing three segments (48.1%, 19.3%, 32.6%) differentiated primarily by redemption frequency</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c3dc42",
   "metadata": {},
   "source": [
    "### **8.4.1 Selecting covariance_type & n_components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21377a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candidates\n",
    "gmm_n_components_behav = list(range(2, 11))\n",
    "gmm_cov_types_behav = [\"full\", \"tied\", \"diag\", \"spherical\"]\n",
    "\n",
    "X_gmm_behav = df_behavioral_clean[behavioral_feats].copy()\n",
    "\n",
    "gmm_results_df_behav = evaluate_gmm_grid(\n",
    "    X=X_gmm_behav,\n",
    "    feats=behavioral_feats,\n",
    "    n_components_list=gmm_n_components_behav,\n",
    "    covariance_types=gmm_cov_types_behav,\n",
    "    n_init=10,\n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "gmm_results_df_sorted_behav = (\n",
    "    gmm_results_df_behav\n",
    "    .sort_values([\"BIC\", \"AIC\"], ascending=[True, True])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "gmm_results_df_sorted_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f88242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) pick covariance_type\n",
    "plot_gmm_covtype_bic_aic(gmm_results_df_behav, gmm_cov_types_behav, gmm_n_components_behav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0319bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Find best n_components for chosen covariance_type\n",
    "chosen_covariance_type_gmm_behav = \"diag\" # based on previous plot\n",
    "plot_gmm_n_selection_for_covtype(gmm_results_df_behav, chosen_covariance_type_gmm_behav, gmm_n_components_behav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8954efa3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">\n",
    "        GMM Parameter Selection Summary\n",
    "    </h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Evaluated Gaussian Mixture Models by running a grid over <strong>n_components</strong> (2–10) and\n",
    "        <strong>covariance_type</strong> (<strong>full</strong>, <strong>tied</strong>, <strong>diag</strong>, <strong>spherical</strong>)\n",
    "        using <strong>init_params=\"kmeans\"</strong> to stabilize initialization. Each configuration was scored with\n",
    "        <strong>BIC</strong> and <strong>AIC</strong> (model selection), plus <strong>R²</strong> (variance explained) and\n",
    "        <strong>Silhouette</strong> (cluster separation sanity check).\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">\n",
    "            Select a <strong>covariance structure</strong> that best fits the data distribution, then choose\n",
    "            <strong>n_components</strong> that balances model fit (BIC/AIC), separation (Silhouette), and interpretability.\n",
    "        </li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">\n",
    "            <strong>Step 1 – covariance_type selection:</strong>\n",
    "            Compared BIC &amp; AIC curves across all covariance types. The best covariance_type achieves the\n",
    "            <strong>lowest BIC/AIC</strong> consistently across n.\n",
    "        </li>\n",
    "        <li style=\"margin-right: 20px;\">\n",
    "            <strong>Step 2 – choose n_components within the winning covariance_type:</strong>\n",
    "            For the selected covariance_type, inspected BIC/AIC, Silhouette, and R².\n",
    "        </li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Best covariance_type:</strong> diag (diagonal) achieves lowest BIC/AIC consistently across n_components, with BIC decreasing from 141,446 (n=2) to 70,531 (n=10)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate A (n=3):</strong> BIC 112,337, Silhouette 0.118 (highest among diag), R² 0.234. Best cluster separation.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Candidate B (n=4):</strong> BIC 112,165, Silhouette 0.110, R² 0.295. Better model fit and variance explained.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Decision:</strong> Compare n=3 vs n=4 for diag covariance. Higher n values (5+) achieve lower BIC but Silhouette drops below 0.06, indicating poor cluster separation.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99077aad",
   "metadata": {},
   "source": [
    "### **8.4.2 Evaluation of GMM Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f32d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=3\n",
    "# n=4\n",
    "\n",
    "# Fit only the selected candidates\n",
    "gmm_3_behav = GaussianMixture(n_components=3, covariance_type='diag', n_init=10, init_params='kmeans', random_state=1)\n",
    "gmm_4_behav = GaussianMixture(n_components=4, covariance_type='diag', n_init=10, init_params='kmeans', random_state=1)\n",
    "\n",
    "gmm_labels_3_behav = gmm_3_behav.fit_predict(df_behavioral_clean[behavioral_feats])\n",
    "gmm_labels_4_behav = gmm_4_behav.fit_predict(df_behavioral_clean[behavioral_feats])\n",
    "\n",
    "# Calculate cluster means\n",
    "df_temp_3 = df_behavioral_clean[behavioral_feats].copy()\n",
    "df_temp_3['Cluster'] = gmm_labels_3_behav\n",
    "cluster_means_3_behav = df_temp_3.groupby('Cluster').mean()\n",
    "\n",
    "df_temp_4 = df_behavioral_clean[behavioral_feats].copy()\n",
    "df_temp_4['Cluster'] = gmm_labels_4_behav\n",
    "cluster_means_4_behav = df_temp_4.groupby('Cluster').mean()\n",
    "\n",
    "display(cluster_means_3_behav.round(3))\n",
    "display(cluster_means_4_behav.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={'GMM (n=3)': gmm_labels_3_behav, 'GMM (n=4)': gmm_labels_4_behav},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='GMM Clustering: Cluster Size Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242dfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncertainty Analysis: Assignment probability < 70%\n",
    "uncertainty_threshold = 0.7\n",
    "\n",
    "gmm_max_probs_3_behav = gmm_3_behav.predict_proba(df_behavioral_clean[behavioral_feats]).max(axis=1)\n",
    "gmm_max_probs_4_behav = gmm_4_behav.predict_proba(df_behavioral_clean[behavioral_feats]).max(axis=1)\n",
    "\n",
    "uncertain_pct_3_behav = (gmm_max_probs_3_behav < uncertainty_threshold).sum() / len(gmm_max_probs_3_behav) * 100\n",
    "uncertain_pct_4_behav = (gmm_max_probs_4_behav < uncertainty_threshold).sum() / len(gmm_max_probs_4_behav) * 100\n",
    "\n",
    "uncertainty_summary_behav = pd.DataFrame({\n",
    "    'n_components': [3, 4],\n",
    "    'uncertain_pct': [uncertain_pct_3_behav, uncertain_pct_4_behav],\n",
    "    'mean_prob': [gmm_max_probs_3_behav.mean(), gmm_max_probs_4_behav.mean()],\n",
    "    'min_prob': [gmm_max_probs_3_behav.min(), gmm_max_probs_4_behav.min()]\n",
    "})\n",
    "\n",
    "display(uncertainty_summary_behav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c989e44",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: n=3 components selected with diag covariance</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        N=3 achieves substantially better assignment certainty (12.6% uncertain vs 29.8% for n=4) and higher mean probability (0.881 vs 0.798). It captures the key behavioral segments: passive redeemers (Cluster 1: redemption Z=-1.31), engaged social travelers (Cluster 2: redemption Z=+1.11, companion Z=+0.40), and mainstream (Cluster 0). N=4 fragments into a small cluster (10.3%) without meaningful improvement. Balanced sizes (48.1%, 19.3%, 32.6%) ensure actionable segments.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96148491",
   "metadata": {},
   "source": [
    "### **8.4.3 Final GMM Clustering Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final parameters based on evaluation previous section\n",
    "# Selected: n_components=3, covariance_type='diag'\n",
    "chosen_n_components_gmm_behav = 3\n",
    "\n",
    "gmm_labels_final_behav = gmm_labels_3_behav\n",
    "gmm_final_behav = gmm_3_behav\n",
    "\n",
    "df_behavioral_clean['gmm_cluster'] = gmm_labels_final_behav\n",
    "\n",
    "# Calculate final metrics\n",
    "gmm_final_metrics_behav = evaluate_clustering_metrics(df_behavioral_clean[behavioral_feats], gmm_labels_final_behav)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "behavioral_clustering_results['GMM'] = {\n",
    "    'k': chosen_n_components_gmm_behav,\n",
    "    'Silhouette': gmm_final_metrics_behav['Silhouette Score'],\n",
    "    'Calinski-Harabasz': gmm_final_metrics_behav['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': gmm_final_metrics_behav['Davies-Bouldin Index'],\n",
    "    'R2': gmm_results_df_behav[(gmm_results_df_behav['n_components'] == chosen_n_components_gmm_behav) & (gmm_results_df_behav['covariance_type'] == 'diag')]['R2'].values[0],\n",
    "    'labels': gmm_labels_final_behav\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcae5159",
   "metadata": {},
   "source": [
    "### **8.4.4 GMM Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb06bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Profile Heatmap - Z-scores of behavioral features per cluster (GMM)\n",
    "df_temp_gmm_behav = df_behavioral_clean[behavioral_feats].copy()\n",
    "df_temp_gmm_behav['Cluster'] = gmm_labels_final_behav\n",
    "cluster_profiles_gmm_behav = df_temp_gmm_behav.groupby('Cluster').mean()\n",
    "gmm_population_mean_behav = df_behavioral_clean[behavioral_feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    cluster_profiles_gmm_behav,\n",
    "    gmm_population_mean_behav,\n",
    "    GROUP80_palette_continuous,\n",
    "    title='GMM Clustering: Behavioral Profiles (n=3)\\nStandardized Z-Scores per Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1461fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    gmm_labels_final_behav,\n",
    "    chosen_n_components_gmm_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='GMM Clustering - Final Cluster Sizes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "gmm_feature_variance_behav = cluster_profiles_gmm_behav.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    gmm_feature_variance_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='GMM Clustering: Feature Importance Analysis\\nWhich Features Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance as DataFrame\n",
    "gmm_feature_importance_df_behav = pd.DataFrame({\n",
    "    'Feature': gmm_feature_variance_behav.index,\n",
    "    'Variance': gmm_feature_variance_behav.values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "display(gmm_feature_importance_df_behav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470deac",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">GMM Clustering Profiling Summary (n=3)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed behavioral characteristics of the final 3 GMM clusters (diag covariance) to identify probabilistic customer segments. Redemption frequency emerges as the dominant differentiator.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (48.1%, n=6,203) - Mainstream Travelers:</strong> Average across all features, slightly lower redemption (Z=-0.24). Largest segment representing typical customers.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (19.3%, n=2,484) - Passive Redeemers:</strong> Very low redemption activity (Z=-1.31), solo travelers (companion Z=-0.33), irregular flight patterns (Z=-0.22).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (32.6%, n=4,198) - Engaged Social Travelers:</strong> High redemption frequency (Z=+1.11), travels with companions (Z=+0.40), regular flight patterns (Z=+0.24).</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Driver - Redemption Frequency (Variance: 1.47):</strong> Redemption behavior creates the strongest separation between clusters.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Limited Multi-Feature Differentiation:</strong> Secondary features show minimal variance (companion: 0.15, regularity: 0.05, distance: 0.01). The GMM solution essentially segments by redemption behavior alone, with other behavioral dimensions contributing little to cluster separation.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a41771",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed657dc",
   "metadata": {},
   "source": [
    "## **8.5 Self Organizing Maps**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3936d40",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e1e1e1ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #212121, #313131, #595959, #909090) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #000000ff; font-weight: bold;\">Self-Organizing Maps (SOM) + K-Means Two-Stage Clustering Methodology</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Self-Organizing Maps are <strong>unsupervised neural networks</strong> that project high-dimensional data onto a low-dimensional grid while preserving topological relationships. Each neuron represents a weight vector in the input space, and during training, neurons are \"pulled\" toward data patterns, dragging their neighbors along. The two-stage approach combines SOM's dimensionality reduction with K-Means clustering on the learned neuron weights.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Algorithm:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Initialization:</strong> Randomly initialize neuron weight vectors and set neighborhood radius (σ) and learning rate (α)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>BMU Selection:</strong> For each input pattern, find the Best Matching Unit (BMU) - the neuron with minimum Euclidean distance to the input</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Weight Update:</strong> Update the BMU and its neighbors: w(new) = w(old) + α[x - w(old)], with neighborhood function controlling update strength</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Parameter Decay:</strong> Gradually reduce learning rate and neighborhood radius over iterations</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Two-Stage Clustering:</strong> Apply K-Means to the trained SOM weight vectors, then map customers to clusters via their BMUs</li>\n",
    "    </ol>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Key Quality Metrics:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Quantization Error (QE):</strong> Average distance between data points and their BMUs - measures data representation accuracy (lower is better)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Topographic Error (TE):</strong> Proportion of data points where 1st and 2nd BMUs are not adjacent - measures topology preservation (lower is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #313131; font-weight: bold;\">Workflow for This Analysis:</h4>\n",
    "    <ol style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Parameter Grid Search:</strong> Test 18 combinations of grid sizes (10x10, 20x20, 40x40), learning rates (0.5, 0.75, 1.0), and sigma values (0.5, 1.0) to optimize QE/TE trade-off</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>SOM Visualization:</strong> Analyze trained SOM using Component Planes (feature distributions), U-Matrix (cluster boundaries), and Hit Map (customer density)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Two-Stage Clustering:</strong> Apply K-Means to 1,600 neuron weight vectors (40x40 grid with lr=0.5, σ=1.0), evaluate k=2-13 using Silhouette, Calinski-Harabasz, and Davies-Bouldin indices</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Solution Comparison:</strong> Compare k=4 vs k=5 solutions by examining SOM grid visualizations, cluster sizes, and feature profiles</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Final Model and Profiling:</strong> Select k=5 for better cluster separation (DBI 1.38) and more actionable marketing segments. Results highly consistent with standalone K-Means, validating behavioral patterns</li>\n",
    "    </ol>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724c102",
   "metadata": {},
   "source": [
    "### **8.5.1 SOM Parameter Grid Search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d34bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid for SOM optimization\n",
    "grid_sizes_behav = [10, 20, 40]\n",
    "learning_rates_behav = [0.5, 0.75, 1.0]\n",
    "sigma_values_behav = [0.5, 1.0]\n",
    "\n",
    "# Prepare scaled data for SOM training\n",
    "som_data_behav = df_behavioral_clean[behavioral_feats].values\n",
    "\n",
    "\n",
    "'''# Store grid search results\n",
    "grid_search_results_behav = []\n",
    "\n",
    "# Perform grid search\n",
    "for grid_size in grid_sizes_behav:\n",
    "    for lr in learning_rates_behav:\n",
    "        for sigma in sigma_values_behav:\n",
    "            # Initialize SOM with current parameters\n",
    "            som = MiniSom(\n",
    "                x=grid_size,\n",
    "                y=grid_size,\n",
    "                input_len=len(behavioral_feats),\n",
    "                sigma=sigma,\n",
    "                learning_rate=lr,\n",
    "                neighborhood_function='gaussian',\n",
    "                topology='hexagonal',\n",
    "                activation_distance='euclidean',\n",
    "                random_seed=1\n",
    "            )\n",
    "            \n",
    "            # Initialize weights randomly from data\n",
    "            som.random_weights_init(som_data_behav)\n",
    "            \n",
    "            # Train SOM\n",
    "            # Scale iterations with map size (500 per neuron): larger grids have more neurons that need sufficient updates to converge properly\n",
    "            num_iterations = 500 * grid_size * grid_size\n",
    "            som.train_batch(som_data_behav, num_iteration=num_iterations, verbose=False)\n",
    "            \n",
    "            # Calculate quality metrics\n",
    "            qe = som.quantization_error(som_data_behav)\n",
    "            te = som.topographic_error(som_data_behav)\n",
    "            \n",
    "            # Store results\n",
    "            grid_search_results_behav.append({\n",
    "                'grid_size': f'{grid_size}x{grid_size}',\n",
    "                'learning_rate': lr,\n",
    "                'sigma': sigma,\n",
    "                'units': grid_size * grid_size,\n",
    "                'quantization_error': qe,\n",
    "                'topographic_error': te\n",
    "            })\n",
    "\n",
    "# Convert results to DataFrame\n",
    "grid_search_df_behav = pd.DataFrame(grid_search_results_behav)\n",
    "grid_search_df_behav = grid_search_df_behav.sort_values(['quantization_error', 'topographic_error']).reset_index(drop=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d414b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''grid_search_df_behav.to_csv('data/output_data/som_grid_search_results_behav.csv', index=False)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077854e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_df_behav = pd.read_csv('data/output_data/som_grid_search_results_behav.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4709a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize grid search results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Quantization Error by Grid Size\n",
    "for lr in learning_rates_behav:\n",
    "    for sigma in sigma_values_behav:\n",
    "        subset = grid_search_df_behav[(grid_search_df_behav['learning_rate'] == lr) & (grid_search_df_behav['sigma'] == sigma)]\n",
    "        axes[0].plot(subset['units'], subset['quantization_error'], \n",
    "                    marker='o', alpha=0.6, label=f'LR={lr}, σ={sigma}')\n",
    "\n",
    "axes[0].set_xlabel('Number of SOM Units', fontsize=11, fontweight='bold')\n",
    "axes[0].set_ylabel('Quantization Error (QE)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_title('SOM Grid Search: Quantization Error', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(False)\n",
    "\n",
    "# Plot 2: Topographic Error by Grid Size\n",
    "for lr in learning_rates_behav:\n",
    "    for sigma in sigma_values_behav:\n",
    "        subset = grid_search_df_behav[(grid_search_df_behav['learning_rate'] == lr) & (grid_search_df_behav['sigma'] == sigma)]\n",
    "        axes[1].plot(subset['units'], subset['topographic_error'], \n",
    "                    marker='o', alpha=0.6, label=f'LR={lr}, σ={sigma}')\n",
    "\n",
    "axes[1].set_xlabel('Number of SOM Units', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Topographic Error (TE)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_title('SOM Grid Search: Topographic Error', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(False)\n",
    "axes[1].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display best parameter combinations\n",
    "grid_search_df_behav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba308112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best parameters based on lowest QE and TE\n",
    "best_params_behav = grid_search_df_behav.iloc[3]\n",
    "\n",
    "# Extract parameters for later use\n",
    "selected_grid_size_behav = int(best_params_behav['grid_size'].split('x')[0])\n",
    "selected_lr_behav = best_params_behav['learning_rate']\n",
    "selected_sigma_behav = best_params_behav['sigma']\n",
    "\n",
    "# Display best parameters\n",
    "best_params_display_behav = pd.DataFrame({\n",
    "    'Parameter': ['Grid Size', 'Learning Rate', 'Sigma', 'Total Units', 'Quantization Error', 'Topographic Error'],\n",
    "    'Value': [\n",
    "        best_params_behav['grid_size'],\n",
    "        best_params_behav['learning_rate'],\n",
    "        best_params_behav['sigma'],\n",
    "        best_params_behav['units'],\n",
    "        f\"{best_params_behav['quantization_error']:.4f}\",\n",
    "        f\"{best_params_behav['topographic_error']:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "best_params_display_behav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7c4f2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM Parameter Grid Search Summary</h3>\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Performed systematic grid search over 18 parameter combinations to identify optimal SOM configuration for behavioral clustering. Evaluated 3 grid sizes (10x10, 20x20, 40x40), 3 learning rates (0.5, 0.75, 1.0), and 2 sigma values (0.5, 1.0), with iterations scaled proportionally to grid size (500 x number of neurons) to ensure convergence.\n",
    "  </p>\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\">Find optimal SOM parameters that balance low Quantization Error (accurate data representation) with low Topographic Error (preserved neighborhood topology) for subsequent two-stage clustering</li>\n",
    "  </ul>\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Quality metrics:</strong> Quantization Error (QE) measures average distance between data points and their Best Matching Units (lower = better data representation). Topographic Error (TE) measures proportion of data points where first and second BMUs are not adjacent (lower = better topology preservation)</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Iteration scaling:</strong> Training iterations set to 500 x grid_size² (e.g., 50,000 for 10x10, 200,000 for 20x20, 800,000 for 40x40) following the standard heuristic of 500 iterations per neuron to ensure proper convergence across all grid sizes</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Parameter selection rationale:</strong> For two-stage clustering (SOM + K-Means), moderate grid sizes are preferred to ensure meaningful data compression. While larger grids minimize QE, they reduce the SOM's ability to aggregate similar customers into prototype neurons, diminishing the benefit of the two-stage approach</li>\n",
    "  </ul>\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Grid size dominates QE:</strong> Larger grids yield substantially lower QE. 10x10 produced QE 0.68-0.71, 20x20 achieved 0.44-0.47, and 40x40 reached 0.25-0.26.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Sigma controls QE-TE trade-off:</strong> Small sigma (σ=0.5) severely damages topology preservation (TE 0.76-0.97), while σ=1.0 maintains acceptable TE (0.39-0.77) with marginal QE increase.</li> \n",
    "    <li style=\"margin-right: 20px;\"><strong>Learning rate has moderate impact:</strong> lr=0.5 produces slightly better TE than lr=1.0 at σ=1.0, while QE remains similar across learning rates.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Grid size impact on two-stage clustering:</strong> Tested all grid sizes with subsequent K-Means clustering. Smaller grids (10x10, 20x20) produced worse Silhouette and Calinski-Harabasz scores in the final clustering, while larger grids (60x60) converged toward standalone K-Means results. The 40x40 grid achieves best clustering metrics while still providing meaningful SOM-based data compression.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Best configuration:</strong> 40x40 grid with lr=0.5 and σ=1.0 achieved lowest QE (0.258) while maintaining acceptable TE (0.389).</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Two-stage clustering:</strong> 40x40 with 1,600 neurons provides meaningful data compression (approximately 8 customers per neuron). Note that results are similar to standalone K-Means, indicating the behavioral feature space is well-suited for centroid-based clustering.</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Decision:</strong> Use 40x40 with lr=0.5 and σ=1.0 for SOM visualization and two-stage clustering.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04afa6c5",
   "metadata": {},
   "source": [
    "### **8.5.2 SOM Visualizations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf006bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SOM with selected best parameters\n",
    "som_best_behav = MiniSom(\n",
    "    x=selected_grid_size_behav,\n",
    "    y=selected_grid_size_behav,\n",
    "    input_len=len(behavioral_feats),\n",
    "    sigma=selected_sigma_behav,\n",
    "    learning_rate=selected_lr_behav,\n",
    "    neighborhood_function='gaussian',\n",
    "    topology='hexagonal',\n",
    "    activation_distance='euclidean',\n",
    "    random_seed=1\n",
    ")\n",
    "\n",
    "# Initialize and train\n",
    "som_best_behav.random_weights_init(som_data_behav)\n",
    "som_best_behav.train_batch(som_data_behav, num_iteration=500 * selected_grid_size_behav * selected_grid_size_behav, verbose=False)\n",
    "\n",
    "print(f\"SOM trained with parameters: Grid={selected_grid_size_behav}x{selected_grid_size_behav}, LR={selected_lr_behav}, σ={selected_sigma_behav}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Component Planes\n",
    "n_cols = 3\n",
    "n_rows = int(np.ceil(len(behavioral_feats) / n_cols))\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, n_rows * 4.5))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(behavioral_feats):\n",
    "    weights = som_best_behav.get_weights()[:, :, idx]\n",
    "    visualize_som_grid(som_best_behav, weights, feature, ax=axes[idx])\n",
    "\n",
    "for idx in range(len(behavioral_feats), len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Component Planes: Feature Distribution across SOM Grid', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01e2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# U-Matrix - Unified Distance Matrix\n",
    "visualize_som_grid(som_best_behav, som_best_behav.distance_map(), 'U-Matrix: Unified Distance Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33311f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hit Map - Customer distribution across SOM grid\n",
    "hitsmatrix_behav = som_best_behav.activation_response(som_data_behav)\n",
    "visualize_som_grid(som_best_behav, hitsmatrix_behav, 'Hit Map: Customer Distribution')\n",
    "\n",
    "display(pd.DataFrame({\n",
    "    'Metric': ['Total Customers', 'Avg per Unit', 'Max in Single Unit', 'Min in Single Unit'],\n",
    "    'Value': [f\"{hitsmatrix_behav.sum():.0f}\", f\"{hitsmatrix_behav.mean():.2f}\", f\"{hitsmatrix_behav.max():.0f}\", f\"{hitsmatrix_behav.min():.0f}\"]\n",
    "}))\n",
    "\n",
    "# Count units with 0 hits\n",
    "n_zero_hit_units_behav = np.sum(hitsmatrix_behav == 0)\n",
    "print(\"Number of units with 0 hits:\", n_zero_hit_units_behav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dee905",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM Visualization Summary</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Visualized the trained 40x40 SOM using three complementary outputs: Component Planes (feature distributions), U-Matrix (cluster boundaries), and Hit Map (customer density). Each visualization serves a distinct purpose in understanding the SOM's learned structure.\n",
    "  </p>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\">Understand how behavioral features are distributed across the SOM grid, identify natural cluster boundaries, and assess whether customer mappings are evenly distributed or concentrated in specific regions</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Component Planes:</strong> Display each feature's weight distribution across the SOM grid. <span style=\"color:#b2182b;\">Red</span> shades indicate higher Z-scores, <span style=\"color:#2166ac;\">blue</span> shades indicate lower Z-scores</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>U-Matrix:</strong> Shows average distance between each SOM unit and its neighbors. <span style=\"color:#b2182b;\">Red</span> regions indicate high distances (cluster boundaries), <span style=\"color:#2166ac;\">blue</span> regions indicate homogeneous areas (cluster centers)</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Hit Map:</strong> Displays how many customers are mapped to each SOM unit. <span style=\"color:#b2182b;\">Red</span> units have more customers, <span style=\"color:#2166ac;\">blue</span> units have fewer</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Distance variability shows left-side concentration:</strong> <span style=\"color:#b2182b;\">Red</span> regions (high variability, Z=+2 to +3) concentrate in both the upper-left and lower-left corners, while <span style=\"color:#2166ac;\">blue</span> regions (low variability) form a diagonal band from lower-right toward the center. No simple gradient pattern</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Companion flight ratio shows central clustering:</strong> <span style=\"color:#b2182b;\">Red</span> cluster (social travelers, Z=+2) distinctly concentrated in the center of the grid with additional spots at the bottom edge, while <span style=\"color:#2166ac;\">blue</span> regions (solo travelers, Z=-2) are located on the left side and upper areas. <span style=\"color:#fddbc7;\">Yellow</span> transition zones between red and blue</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Flight regularity shows scattered hotspots:</strong> Four concentrated <span style=\"color:#b2182b;\">red</span> hotspots (irregular flights, Z=+2 to +3) distributed across the grid, with <span style=\"color:#2166ac;\">blue</span> regions (regular patterns) appearing sparsely between the red and yellow sections. <span style=\"color:#fddbc7;\">Yellow</span> (moderate) dominates most of the grid</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Redemption frequency shows sparse high-value islands:</strong> One larger <span style=\"color:#fddbc7;\">yellow</span>/<span style=\"color:#b2182b;\">red</span> cluster (high redemption, Z=+1 to +3) in the center plus three smaller islands, while <span style=\"color:#2166ac;\">blue</span> (passive redeemers, Z=-1) dominates the majority of the grid including upper-left, right side, and bottom areas. This confirms most customers are passive redeemers with engaged users forming distinct minority clusters</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>U-Matrix reveals diffuse cluster boundaries:</strong> Predominantly <span style=\"color:#92c5de;\">light blue</span> regions (low inter-neuron distances, 0.2-0.5) with scattered <span style=\"color:#f4a582;\">orange</span>/<span style=\"color:#b2182b;\">red</span> spots (higher distances, 0.7-1.0) distributed throughout, suggesting gradual transitions between behavioral segments rather than sharp cluster boundaries</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Hit Map shows relatively uniform coverage:</strong> With 12,885 customers mapped (avg 8.05 per unit), the distribution ranges from 0 to 20 customers per unit. Only 20 units (1.25%) remain empty, indicating excellent grid utilization.</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb4d84",
   "metadata": {},
   "source": [
    "### **8.5.3 Emergent SOM Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303aa8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emergent SOM for Two-Stage Clustering\n",
    "# Using selected grid - optimal balance between granularity and neuron coverage\n",
    "\n",
    "som_emergent_behav = som_best_behav  # Use already trained SOM from Grid Search\n",
    "emergent_grid_size_behav = selected_grid_size_behav\n",
    "\n",
    "qe_emergent_behav = som_emergent_behav.quantization_error(som_data_behav)\n",
    "te_emergent_behav = som_emergent_behav.topographic_error(som_data_behav)\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Metric': ['Grid Size', 'Total Units', 'Avg Customers/Unit', 'Quantization Error', 'Topographic Error'],\n",
    "    'Value': [f'{emergent_grid_size_behav}x{emergent_grid_size_behav}', f'{emergent_grid_size_behav**2:,}', f'{len(som_data_behav)/emergent_grid_size_behav**2:.2f}', f'{qe_emergent_behav:.4f}', f'{te_emergent_behav:.4f}']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39910828",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "  <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM for Two-Stage Clustering</h3>\n",
    "\n",
    "  <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "    Using the 40x40 SOM (1,600 units) trained with optimal Grid Search parameters (lr=0.5, σ=1.0) as foundation for two-stage clustering. K-Means will cluster the neuron weight vectors to identify final customer segments.\n",
    "  </p>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Why 40x40 Grid for Two-Stage Clustering:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Meaningful data compression:</strong> With 12,885 customers across 1,600 neurons, each neuron represents 8.05 customers on average. This compression ratio ensures the SOM provides genuine noise reduction and prototype learning, differentiating two-stage clustering from direct K-Means on raw data</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Robust neuron coverage:</strong> The moderate density of 8 customers per neuron ensures weight vectors are stable representations of local customer profiles, not dominated by individual outliers</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Balanced granularity:</strong> 1,600 neurons provide sufficient resolution for K-Means to identify natural cluster boundaries while avoiding the over-granularity problem where very large grids would produce results nearly identical to direct clustering</li>\n",
    "  </ul>\n",
    "\n",
    "  <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Quality Metrics:</h4>\n",
    "  <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "    <li style=\"margin-right: 20px;\"><strong>Quantization Error: 0.258</strong> - low error indicating neurons accurately represent the underlying data distribution while providing meaningful compression</li>\n",
    "    <li style=\"margin-right: 20px;\"><strong>Topographic Error: 0.389</strong> - 38.9% of data points have non-adjacent 1st and 2nd BMUs. Higher than demographic SOM due to the 4-dimensional behavioral space, but still acceptable for interpretable clustering results</li>\n",
    "  </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a39965",
   "metadata": {},
   "source": [
    "### **8.5.4 Defining the number of clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c88e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten SOM weights for K-Means clustering\n",
    "som_weights_flat_behav = som_emergent_behav.get_weights().reshape(-1, len(behavioral_feats))\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Description': ['SOM weights shape', 'Total neurons', 'Features per neuron'],\n",
    "    'Value': [str(som_weights_flat_behav.shape), emergent_grid_size_behav**2, len(behavioral_feats)]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fbd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate K-Means on SOM weights across range of k values\n",
    "som_km_k_range_behav = range(2, 14)\n",
    "som_km_metrics_behav = {\n",
    "    'k': [],\n",
    "    'Inertia': [],\n",
    "    'Silhouette': [],\n",
    "    'Calinski-Harabasz': [],\n",
    "    'Davies-Bouldin': []\n",
    "}\n",
    "\n",
    "# Store fitted neuron labels for later use\n",
    "som_km_fitted_neuron_labels_behav = {}\n",
    "\n",
    "# Helper function to map neuron labels to customers via BMU\n",
    "def get_customer_labels_behav(som, data, neuron_labels, grid_size):\n",
    "    customer_labels = []\n",
    "    for sample in data:\n",
    "        bmu = som.winner(sample)\n",
    "        neuron_idx = bmu[0] * grid_size + bmu[1]\n",
    "        customer_labels.append(neuron_labels[neuron_idx])\n",
    "    return np.array(customer_labels)\n",
    "\n",
    "for k in som_km_k_range_behav:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init=15, random_state=1, max_iter=300)\n",
    "    km_neuron_labels = kmeans.fit_predict(som_weights_flat_behav)\n",
    "    \n",
    "    # Store neuron labels\n",
    "    som_km_fitted_neuron_labels_behav[k] = km_neuron_labels\n",
    "    \n",
    "    # Map neuron labels to customers via BMU\n",
    "    customer_labels = get_customer_labels_behav(som_emergent_behav, som_data_behav, km_neuron_labels, emergent_grid_size_behav)\n",
    "    \n",
    "    # Metrics on customer data\n",
    "    metrics = evaluate_clustering_metrics(som_data_behav, customer_labels)\n",
    "    \n",
    "    som_km_metrics_behav['k'].append(k)\n",
    "    som_km_metrics_behav['Inertia'].append(kmeans.inertia_)\n",
    "    som_km_metrics_behav['Silhouette'].append(metrics['Silhouette Score'])\n",
    "    som_km_metrics_behav['Calinski-Harabasz'].append(metrics['Calinski-Harabasz Index'])\n",
    "    som_km_metrics_behav['Davies-Bouldin'].append(metrics['Davies-Bouldin Index'])\n",
    "\n",
    "som_km_metrics_df_behav = pd.DataFrame(som_km_metrics_behav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ccc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "plot_elbow_method(\n",
    "    som_km_k_range_behav,\n",
    "    som_km_metrics_df_behav['Inertia'].tolist(),\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means: Elbow Method for Optimal k'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d839e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clustering metrics comparison\n",
    "plot_clustering_metrics(\n",
    "    som_km_metrics_df_behav,\n",
    "    som_km_k_range_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means: Clustering Metrics Evaluation'\n",
    ")\n",
    "\n",
    "som_km_metrics_df_behav"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241cf768",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Optimal k Selection Summary (SOM + K-Means)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Evaluated k=2 to k=13 on the 1,600 SOM neuron weight vectors using multiple validation indices to identify the optimal number of clusters for two-stage clustering.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\">Determine k that balances cluster quality metrics with business interpretability for actionable customer segmentation</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Elbow Method (Inertia):</strong> Total within-cluster sum of squared distances on SOM weights - look for \"elbow\" where adding clusters yields diminishing returns</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Silhouette Score:</strong> Measures cluster cohesion and separation (range -1 to 1, higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Calinski-Harabasz Index:</strong> Ratio of between-cluster to within-cluster variance (higher is better)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Davies-Bouldin Index:</strong> Average similarity between clusters (lower is better)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>k=4 - Elbow Point:</strong> Clear elbow in the inertia curve where additional clusters yield diminishing returns. Good Silhouette (0.180), solid CH (2,900), and improved DBI (1.47) compared to lower k values</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>k=5 - Best Cluster Separation:</strong> Achieves best Davies-Bouldin Index (1.38) among k=2-6, indicating optimal cluster separation. Comparable Silhouette (0.178) to k=4 with slightly lower CH (2,791)</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Trade-off Analysis:</strong> k=2-3 have higher Silhouette and CH but provide insufficient granularity. k=6+ shows declining Silhouette (drops to 0.163) without meaningful DBI improvement. k=4 and k=5 represent the optimal balance between separation quality and interpretability</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Decision for next step:</strong> Compare k=4 vs k=5 to determine the final clustering configuration based on cluster profiles and business interpretability</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4dd6ac",
   "metadata": {},
   "source": [
    "### **8.5.5 Comparison of k Solutions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two candidate solutions based on clustering metrics analysis\n",
    "# k=4\n",
    "# k=5\n",
    "\n",
    "som_km_k_candidate_1_behav = 4\n",
    "som_km_k_candidate_2_behav = 5\n",
    "\n",
    "# Use pre-fitted neuron labels from 8.5.4\n",
    "som_km_labels_k1_behav = som_km_fitted_neuron_labels_behav[som_km_k_candidate_1_behav]\n",
    "som_km_labels_k2_behav = som_km_fitted_neuron_labels_behav[som_km_k_candidate_2_behav]\n",
    "\n",
    "# Map to customers \n",
    "customer_labels_k1_behav = get_customer_labels_behav(som_emergent_behav, som_data_behav, som_km_labels_k1_behav, emergent_grid_size_behav)\n",
    "customer_labels_k2_behav = get_customer_labels_behav(som_emergent_behav, som_data_behav, som_km_labels_k2_behav, emergent_grid_size_behav)\n",
    "\n",
    "# Create temporary DataFrames with cluster labels\n",
    "df_temp_k1_behav = pd.DataFrame(som_data_behav, columns=behavioral_feats)\n",
    "df_temp_k1_behav['Cluster'] = customer_labels_k1_behav\n",
    "\n",
    "df_temp_k2_behav = pd.DataFrame(som_data_behav, columns=behavioral_feats)\n",
    "df_temp_k2_behav['Cluster'] = customer_labels_k2_behav\n",
    "\n",
    "# Calculate cluster profiles (mean values per cluster)\n",
    "cluster_profiles_k1_behav = df_temp_k1_behav.groupby('Cluster').mean()\n",
    "cluster_profiles_k2_behav = df_temp_k2_behav.groupby('Cluster').mean()\n",
    "\n",
    "# Display both profiles for comparison\n",
    "print(f\"Cluster Profiles for k={som_km_k_candidate_1_behav}:\")\n",
    "display(cluster_profiles_k1_behav.round(3))\n",
    "\n",
    "print(f\"\\nCluster Profiles for k={som_km_k_candidate_2_behav}:\")\n",
    "display(cluster_profiles_k2_behav.round(3))\n",
    "\n",
    "# Visualize cluster size comparison\n",
    "plot_cluster_size_comparison(\n",
    "    labels_dict={som_km_k_candidate_1_behav: customer_labels_k1_behav, som_km_k_candidate_2_behav: customer_labels_k2_behav},\n",
    "    palette=CUSTOM_HEX,\n",
    "    title='SOM + K-Means: Cluster Size Comparison'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ddc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k=candidate_1\n",
    "cluster_grid_k1_behav = som_km_labels_k1_behav.reshape((emergent_grid_size_behav, emergent_grid_size_behav))\n",
    "visualize_som_grid(som_emergent_behav, cluster_grid_k1_behav.astype(float), f'SOM + K-Means Clustering (k={som_km_k_candidate_1_behav})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99a52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize k=candidate_2\n",
    "cluster_grid_k2_behav = som_km_labels_k2_behav.reshape((emergent_grid_size_behav, emergent_grid_size_behav))\n",
    "visualize_som_grid(som_emergent_behav, cluster_grid_k2_behav.astype(float), f'SOM + K-Means Clustering (k={som_km_k_candidate_2_behav})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b508722d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Comparison of Clustering Solutions Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Decision: k=5 selected</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K=5 achieves better cluster separation (DBI 1.38 vs 1.47) while providing more actionable marketing segments. The SOM grid visualization shows k=5 produces clearer spatial regions, particularly the yellow cluster (high redemption, Z=+1.39) forming a distinct contiguous area in the center-right. K=5 separates strategically different customer types: Explorers (variable destinations) vs Sporadic Flyers (irregular schedule) require different marketing approaches. The high-redemption segment is more clearly isolated in k=5 (redemption Z=+1.39) compared to k=4 (Z=+1.15). Balanced cluster sizes (17.1% to 23.4%) ensure all segments are substantial enough for targeted campaigns.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db72d9",
   "metadata": {},
   "source": [
    "### **8.5.6 Final SOM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c18c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select k=5 and use pre-fitted labels\n",
    "selected_k_som_behav = som_km_k_candidate_2_behav\n",
    "df_behavioral_clean['Cluster_SOM_KMeans'] = customer_labels_k2_behav\n",
    "\n",
    "# Calculate final metrics\n",
    "som_labels_final_behav = df_behavioral_clean['Cluster_SOM_KMeans'].values\n",
    "som_final_metrics_behav = evaluate_clustering_metrics(df_behavioral_clean[behavioral_feats], som_labels_final_behav)\n",
    "\n",
    "# Store for final comparison (Section 9)\n",
    "behavioral_clustering_results['SOM + K-Means'] = {\n",
    "    'k': selected_k_som_behav,\n",
    "    'Silhouette': som_final_metrics_behav['Silhouette Score'],\n",
    "    'Calinski-Harabasz': som_final_metrics_behav['Calinski-Harabasz Index'],\n",
    "    'Davies-Bouldin': som_final_metrics_behav['Davies-Bouldin Index'],\n",
    "    'R2': get_rsq(df_behavioral_clean[behavioral_feats + ['Cluster_SOM_KMeans']], behavioral_feats, 'Cluster_SOM_KMeans'),\n",
    "    'labels': som_labels_final_behav\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d53479",
   "metadata": {},
   "source": [
    "### **8.5.7 SOM Cluster Profiling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4471999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Profile Heatmap - Z-scores of behavioral features per cluster (SOM)\n",
    "# Reuse cluster_profiles\n",
    "cluster_profiles_som_behav = cluster_profiles_k2_behav\n",
    "som_population_mean_behav = df_behavioral_clean[behavioral_feats].mean()\n",
    "\n",
    "plot_cluster_profiles_heatmap(\n",
    "    cluster_profiles_som_behav,\n",
    "    som_population_mean_behav,\n",
    "    GROUP80_palette_continuous,\n",
    "    title='SOM + K-Means Clustering: Behavioral Profiles (k=5)\\nStandardized Z-Scores per Cluster'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45033c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Cluster Size Distribution\n",
    "plot_cluster_sizes(\n",
    "    df_behavioral_clean['Cluster_SOM_KMeans'].values,\n",
    "    selected_k_som_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means Clustering - Final Cluster Sizes'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c81aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Feature Importance Analysis - Variance across clusters\n",
    "som_feature_variance_behav = cluster_profiles_som_behav.var(axis=0).sort_values(ascending=False)\n",
    "\n",
    "plot_feature_importance(\n",
    "    som_feature_variance_behav,\n",
    "    CUSTOM_HEX,\n",
    "    title='SOM + K-Means Clustering: Feature Importance Analysis\\nWhich Behaviors Differentiate Clusters?'\n",
    ")\n",
    "\n",
    "# Display feature importance as DataFrame\n",
    "som_feature_importance_df_behav = pd.DataFrame({\n",
    "    'Feature': som_feature_variance_behav.index,\n",
    "    'Variance': som_feature_variance_behav.values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "display(som_feature_importance_df_behav)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fdf545",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">SOM + K-Means Clustering Profiling Summary (k=5)</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Analyzed behavioral characteristics of the final 5 SOM + K-Means clusters. The two-stage approach produces segmentation highly consistent with standalone K-Means, validating that the 40x40 SOM preserves the essential cluster structure while providing topological visualization benefits.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; padding-right: 0; color: #000;\">\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 0 (23.4%, n=3,018) - Explorers:</strong> High distance variability (Z=+1.11), solo traveler (companion Z=-0.38). Visits diverse destinations. Equivalent to K-Means Cluster 3.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 1 (21.3%, n=2,744) - Business Commuters:</strong> High flight regularity (regularity Z=+0.87), fixed routes (distance Z=-0.65). Predictable booking behavior. Equivalent to K-Means Cluster 1.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 2 (17.1%, n=2,200) - Family Travelers:</strong> High companion ratio (Z=+1.16), below-average regularity (regularity Z=-0.69). Travels with family on leisure patterns. Equivalent to K-Means Cluster 0.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 3 (19.6%, n=2,525) - Engaged Loyalists:</strong> Very high redemption (Z=+1.39), moderate companion (Z=+0.38), somewhat regular (Z=+0.37). Most valuable loyalty segment. Equivalent to K-Means Cluster 4.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Cluster 4 (18.6%, n=2,398) - Disengaged Solo:</strong> All features negative: fixed routes (distance Z=-0.51), solo (companion Z=-0.83), low regularity (regularity Z=-0.82), passive redeemer (Z=-0.59). Re-engagement target. Equivalent to K-Means Cluster 2.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Primary Driver - Redemption Frequency (Variance: 0.62):</strong> Redemption behavior creates strongest cluster separation, isolating the high-value Engaged segment.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>Secondary Drivers - Companion (0.59) and Regularity (0.51):</strong> Differentiate Family from Solo segments and Commuters from irregular travelers.</li>\n",
    "        <li style=\"margin-right: 20px;\"><strong>SOM vs K-Means Consistency:</strong> Both methods produce identical segment archetypes with similar cluster sizes, confirming robust behavioral patterns. SOM adds topological visualization for segment proximity analysis.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fdcc59",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89494725",
   "metadata": {},
   "source": [
    "# <a class='anchor' id='9'></a>\n",
    "<br>\n",
    "\n",
    "<div style=\"background: linear-gradient(to right, #00411E, #00622D, #00823C, #45AF28, #82BA72); \n",
    "            padding: 10px; color: white; text-align: center;  max-width: 97%;\">\n",
    "    <center><h1 style=\"margin-top: 10px; margin-bottom: 4px; color: white;\n",
    "                       font-size: 32px; font-family: 'Roboto', sans-serif;\">\n",
    "        <b>9. Final Clustering</b></h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a71c5",
   "metadata": {},
   "source": [
    "## **9.1 Select Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dataframe for final comparison\n",
    "demo_clustering_results_df = pd.DataFrame.from_dict(demo_clustering_results, orient='index')\n",
    "behavioral_clustering_results_df = pd.DataFrame.from_dict(behavioral_clustering_results, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c05e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table with Silhouette and Davies-Bouldin\n",
    "comparison_data = []\n",
    "\n",
    "methods = ['Hierarchical', 'K-Means', 'SOM + K-Means', 'Mean Shift', 'GMM']\n",
    "\n",
    "for method in methods:\n",
    "    demo = demo_clustering_results_df.loc[method]\n",
    "    behav = behavioral_clustering_results_df.loc[method]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Clustering Method': method,\n",
    "        'Demo Silhouette': f\"{demo['Silhouette']:.3f} ({int(demo['k'])})\",\n",
    "        'Demo DBI': f\"{demo['Davies-Bouldin']:.3f}\",\n",
    "        'Behav Silhouette': f\"{behav['Silhouette']:.3f} ({int(behav['k'])})\",\n",
    "        'Behav DBI': f\"{behav['Davies-Bouldin']:.3f}\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.set_index('Clustering Method', inplace=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba2305",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Final Model Selection Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Evaluation Metrics: Silhouette Score and Davies-Bouldin Index</strong>\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Silhouette measures cluster cohesion and separation (higher is better), while Davies-Bouldin captures cluster compactness relative to inter-cluster distance (lower is better). R² was excluded because it inherently increases with more clusters, making cross-method comparison misleading when cluster counts differ. Silhouette and DBI provide complementary, cluster-count-independent quality assessment.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Demographic Perspective: K-Means (k=3)</strong><br>\n",
    "        Achieves top Silhouette (0.203) and best DBI (1.736), outperforming all other methods including SOM + K-Means (DBI 1.740). The lean 3-cluster solution balances statistical quality with business interpretability.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Behavioral Perspective: K-Means (k=5)</strong><br>\n",
    "        Clearly outperforms all methods with highest Silhouette (0.186) and lowest DBI (1.360). The 5-cluster solution provides more actionable marketing segments than k=4.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e6f7cb",
   "metadata": {},
   "source": [
    "## **9.2 Merge Clustering Perspectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d766c3b",
   "metadata": {},
   "source": [
    "### Step 1: Combine Clustering Labels from both Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af631396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the final K-Means cluster labels to each dataframe\n",
    "# Demographic: k=3 clusters | Behavioral: k=5 clusters\n",
    "\n",
    "df_demographic_a_scaled['km_cluster'] = demo_clustering_results_df.loc['K-Means', 'labels']\n",
    "df_behavioral_clean['km_cluster'] = behavioral_clustering_results_df.loc['K-Means', 'labels']\n",
    "\n",
    "# Create crosstab: Count customers in each (demo, behav) cluster combination\n",
    "# Rows = Demographic clusters (0-2), Columns = Behavioral clusters (0-4)\n",
    "crosstab = pd.crosstab(\n",
    "    df_demographic_a_scaled['km_cluster'],\n",
    "    df_behavioral_clean['km_cluster'],\n",
    "    margins=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9c2c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725ebed",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#00411E\", \"#00823C\", \"#82BA72\", \"#D5E6D0\", \"#FFFFFF\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    crosstab.iloc[:-1, :-1],\n",
    "    annot=True, \n",
    "    fmt='d', \n",
    "    cmap=LinearSegmentedColormap.from_list('gw', colors[::-1]),  # reversed\n",
    "    cbar_kws={'label': 'Number of Customers'},\n",
    "    linewidths=0.5,\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Cluster Overlap: Demographic vs Behavioral Perspective\\n(Customer Count per Combination)', \n",
    "             fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Behavioral Cluster (k=5)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Demographic Cluster (k=3)', fontweight='bold', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268ea301",
   "metadata": {},
   "source": [
    "### Step 2: Calculate Centroids for each Cluster Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb4edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Centroids for Each Cluster Combination\n",
    "# Align indices (behavioral has fewer rows due to outlier removal)\n",
    "common_idx = df_demographic_a_scaled.index.intersection(df_behavioral_clean.index)\n",
    "\n",
    "# Define feature lists (exclude all cluster label columns)\n",
    "exclude_cols = ['km_cluster', 'ms_cluster', 'gmm_cluster', 'Cluster_SOM_KMeans', 'Cluster']\n",
    "demographic_feats = [col for col in df_demographic_a_scaled.columns if col not in exclude_cols]\n",
    "all_features = demographic_feats + behavioral_feats\n",
    "\n",
    "# Combine perspectives: each customer gets demo + behav features and both cluster labels\n",
    "df_merged = pd.DataFrame({\n",
    "    'demo_cluster': df_demographic_a_scaled.loc[common_idx, 'km_cluster'].values,\n",
    "    'behav_cluster': df_behavioral_clean.loc[common_idx, 'km_cluster'].values\n",
    "})\n",
    "df_merged[demographic_feats] = df_demographic_a_scaled.loc[common_idx, demographic_feats].values\n",
    "df_merged[behavioral_feats] = df_behavioral_clean.loc[common_idx, behavioral_feats].values\n",
    "\n",
    "# Calculate centroids: mean feature values for each (demo, behav) cluster combination\n",
    "df_centroids = df_merged.groupby(['demo_cluster', 'behav_cluster'])[all_features].mean()\n",
    "\n",
    "print(f\"Features: {len(all_features)} | Combinations: {len(df_centroids)}\")\n",
    "df_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6fea3c",
   "metadata": {},
   "source": [
    "### Step 3: Hierarchical Clustering on Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767f6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linkage matrix using Ward method\n",
    "linkage_matrix = linkage(df_centroids[all_features], method='ward', metric='euclidean')\n",
    "\n",
    "y_threshold_first_option = 5\n",
    "y_threshold_second_option = 2.3\n",
    "\n",
    "# Plot dendrogram to decide final k\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "dendrogram(\n",
    "    linkage_matrix,\n",
    "    ax=ax,\n",
    "    labels=[f\"D{i[0]}_B{i[1]}\" for i in df_centroids.index],\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.7 * max(linkage_matrix[:, 2])\n",
    ")\n",
    "plt.axhline(y=y_threshold_first_option, color='r', linestyle='--', linewidth=2, label=f'First option at {y_threshold_first_option}')\n",
    "plt.axhline(y=y_threshold_second_option, color='b', linestyle='--', linewidth=2, label=f'Second option at {y_threshold_second_option}')\n",
    "ax.set_title('Dendrogram: Hierarchical Clustering of Combined Centroids', fontweight='bold', fontsize=14)\n",
    "ax.set_xlabel('Cluster Combination (Demo_Behav)', fontweight='bold')\n",
    "ax.set_ylabel('Distance (Ward)', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8be528",
   "metadata": {},
   "source": [
    "### Step 4: Apply Final HC with chosen k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Compare both k options\n",
    "k_option_1 = 3  # cut at distance ~5\n",
    "k_option_2 = 6  # cut at distance ~2.3\n",
    "\n",
    "# Fit both options\n",
    "hc_k3 = AgglomerativeClustering(linkage='ward', n_clusters=k_option_1)\n",
    "hc_k6 = AgglomerativeClustering(linkage='ward', n_clusters=k_option_2)\n",
    "\n",
    "df_centroids['merged_k3'] = hc_k3.fit_predict(df_centroids[all_features])\n",
    "df_centroids['merged_k6'] = hc_k6.fit_predict(df_centroids[all_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c39c01",
   "metadata": {},
   "source": [
    "### Step 5: Map merged labels back to all customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f37e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: k=3\n",
    "mapper_k3 = df_centroids['merged_k3'].to_dict()\n",
    "df_merged['merged_k3'] = df_merged.apply(\n",
    "    lambda row: mapper_k3[(row['demo_cluster'], row['behav_cluster'])], axis=1\n",
    ")\n",
    "\n",
    "# Option 2: k=6\n",
    "mapper_k6 = df_centroids['merged_k6'].to_dict()\n",
    "df_merged['merged_k6'] = df_merged.apply(\n",
    "    lambda row: mapper_k6[(row['demo_cluster'], row['behav_cluster'])], axis=1\n",
    ")\n",
    "\n",
    "# Compare cluster sizes\n",
    "print(\"Option 1 (k=3) cluster sizes:\")\n",
    "print(df_merged['merged_k3'].value_counts().sort_index())\n",
    "print(f\"\\nOption 2 (k=6) cluster sizes:\")\n",
    "print(df_merged['merged_k6'].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77275c14",
   "metadata": {},
   "source": [
    "### Step 6: Compare metrics for both options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d839b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_merged = df_merged[all_features].values\n",
    "\n",
    "results = []\n",
    "for k, col in [(3, 'merged_k3'), (6, 'merged_k6')]:\n",
    "    labels = df_merged[col].values\n",
    "    results.append({\n",
    "        'k': k,\n",
    "        'Silhouette': silhouette_score(X_merged, labels),\n",
    "        'Davies-Bouldin': davies_bouldin_score(X_merged, labels),\n",
    "        'Calinski-Harabasz': calinski_harabasz_score(X_merged, labels),\n",
    "        'R2': get_rsq(df_merged, all_features, col)\n",
    "    })\n",
    "\n",
    "metrics_comparison = pd.DataFrame(results).set_index('k')\n",
    "metrics_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f1c2a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Multi-Perspective Merging Analysis</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Goal</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The goal of multi-perspective merging is to combine demographic (K-Means k=3) and behavioral (K-Means k=5) clustering perspectives into a unified segmentation solution. This approach attempts to create customer segments that simultaneously capture both \"who customers are\" (demographics: income, education, location, marital status) and \"how they behave\" (flight patterns, companion preferences, redemption behavior). The hypothesis is that combining both perspectives yields richer, more actionable segments than either perspective alone.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The merging process follows an established multi-perspective segmentation workflow:\n",
    "    </p>\n",
    "    <ul style=\"margin: 5px 0; margin-right: 40px; color: #000;\">\n",
    "        <li><strong>Step 1 - Index Alignment:</strong> Aligned customer indices between demographic (n=13,038) and behavioral (n=12,885) datasets. The difference of 153 customers results from DBSCAN outlier removal in behavioral clustering. Analysis proceeds with 12,885 common customers.</li>\n",
    "        <li><strong>Step 2 - Cluster Overlap Analysis:</strong> Created crosstab showing customer distribution across all 15 possible combinations (3 demographic x 5 behavioral clusters). Heatmap visualization reveals how perspectives interact and whether certain demographic-behavioral pairings are over- or under-represented.</li>\n",
    "        <li><strong>Step 3 - Centroid Calculation:</strong> For each of the 15 cluster combinations, calculated centroids representing the mean feature values across all 13 features (9 demographic + 4 behavioral). Each centroid captures the \"average customer profile\" for customers belonging to that specific (demographic, behavioral) combination.</li>\n",
    "        <li><strong>Step 4 - Hierarchical Clustering on Centroids:</strong> Applied Ward linkage hierarchical clustering to the 15 centroids. Ward linkage minimizes within-cluster variance, making it suitable for identifying compact, well-separated groups. The dendrogram visualization enables visual inspection of the hierarchical structure and potential cut points.</li>\n",
    "        <li><strong>Step 5 - Cut Point Evaluation:</strong> Evaluated two cut options based on dendrogram structure:\n",
    "            <ul>\n",
    "                <li>Option 1: Cut at distance 5.0 --> k=3 merged clusters</li>\n",
    "                <li>Option 2: Cut at distance 2.3 --> k=6 merged clusters</li>\n",
    "            </ul>\n",
    "        </li>\n",
    "        <li><strong>Step 6 - Label Mapping & Evaluation:</strong> Mapped merged cluster labels from centroids back to all 12,885 customers using the (demo_cluster, behav_cluster) --> merged_cluster mapping. Calculated Silhouette Score, Davies-Bouldin Index, Calinski-Harabasz Index, and R² for both options.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Findings</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Quantitative Evaluation:</strong> Merged clustering shows significant quality degradation compared to standalone K-Means Behavioral clustering:\n",
    "    </p>\n",
    "    <table style=\"margin: 10px 0; border-collapse: collapse; font-size: 14px; color: #000;\">\n",
    "        <tr style=\"background-color: #82BA72;\">\n",
    "            <th style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">Solution</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">k</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">Silhouette</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">Davies-Bouldin</th>\n",
    "            <th style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">R²</th>\n",
    "        </tr>\n",
    "        <tr style=\"background-color: #D5E6D0;\">\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">Merged (cut at 5.0)</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">3</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">0.129</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">2.30</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">0.212</td>\n",
    "        </tr>\n",
    "        <tr style=\"background-color: #e8f3e5;\">\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">Merged (cut at 2.3)</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">6</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">0.065</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">3.42</td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #000;\">0.268</td>\n",
    "        </tr>\n",
    "        <tr style=\"background-color: #00823C;\">\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #fff;\"><strong>K-Means Behavioral</strong></td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #fff;\"><strong>5</strong></td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #fff;\"><strong>0.186</strong></td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #fff;\"><strong>1.36</strong></td>\n",
    "            <td style=\"padding: 8px; border: 1px solid #00622D; color: #fff;\"><strong>0.471</strong></td>\n",
    "        </tr>\n",
    "    </table>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Key Observations:</strong>\n",
    "    </p>\n",
    "    <ul style=\"margin: 5px 0; margin-right: 40px; color: #000;\">\n",
    "        <li><strong>Silhouette Degradation:</strong> Merged k=3 achieves only 69% of behavioral-only Silhouette (0.129 vs 0.186). Merged k=6 drops to 35% (0.065), indicating poor cluster cohesion and separation.</li>\n",
    "        <li><strong>Davies-Bouldin Increase:</strong> DBI increases from 1.36 (behavioral) to 2.30 (merged k=3) and 3.42 (merged k=6), representing 69% and 151% worse cluster compactness respectively.</li>\n",
    "        <li><strong>R² Trade-off:</strong> While merged solutions explain variance across all 13 features, the behavioral-only solution explains 47% of variance in the 4 behavioral features that actually drive segment differentiation.</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Dendrogram Structure Insight:</strong> The hierarchical clustering dendrogram reveals a critical pattern: merged clusters group almost exclusively by demographic perspective. All D0_* combinations (regardless of behavioral cluster B0-B4) cluster together, all D1_* together, and all D2_* together. This indicates that when demographic and behavioral features are combined in the same feature space, the demographic features dominate the distance calculations, effectively \"drowning out\" the behavioral signal. The behavioral perspective provides minimal additional differentiation beyond what demographics already capture.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c49b16",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #fce8e8ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #8B0000, #A52A2A, #CD5C5C, #F08080) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #8B0000; font-weight: bold;\">Critical Decision: Multi-Perspective Merging rejected</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Final Decision</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>The multi-perspective merging approach is rejected.</strong> The final customer segmentation will be based exclusively on behavioral clustering (K-Means k=5). Demographic attributes will serve only as descriptive profiling variables for the behavioral segments, not as inputs to segment creation. Value-based features (Frequency, Monetary) will be used for a separate prioritization layer.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Rationale: Why Merging fails</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>1. Significant Statistical Quality Degradation</strong></p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The merged solutions show unacceptable degradation in clustering quality metrics. The Silhouette score drops by 30-65% compared to behavioral-only clustering, while Davies-Bouldin increases by 70-150%. A Silhouette score of 0.065 (merged k=6) falls well below a threshold considered minimum for meaningful cluster structure. This indicates that the merged clusters lack internal cohesion and external separation.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>2. Demographic Features Issues</strong></p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The demographic features in this dataset suffer from systematic data quality problems that fundamentally undermine their value for clustering:\n",
    "    </p>\n",
    "    <ul style=\"margin: 5px 0; margin-right: 40px; color: #000;\">\n",
    "        <li><strong>Income Artifact:</strong> All customers with college education show Income=0, creating an artificial pattern where education and income are confounded. This systematic error means income-based segments reflect data collection issues rather than true economic differences.</li>\n",
    "        <li><strong>Geographic Concentration:</strong> Location features (Province, City, FSA) show heavy concentration in a few regions, with sparse representation elsewhere. This limits the discriminative power of geographic segmentation.</li>\n",
    "        <li><strong>Limited Variability:</strong> Features like Gender or Marital status provide simply limited clustering value.</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>3. Limited Strategic Actionability of Demographics</strong></p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Even if demographic data quality were perfect, demographic-based segments offer limited actionability for marketing strategy. Knowing that a customer is male, married, aged 35-44, or lives in Ontario does not directly inform:\n",
    "    </p>\n",
    "    <ul style=\"margin: 5px 0; margin-right: 40px; color: #000;\">\n",
    "        <li>What offer or promotion to send</li>\n",
    "        <li>Which communication channel to prioritize</li>\n",
    "        <li>When to engage (timing, frequency)</li>\n",
    "        <li>What messaging or creative to use</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        In contrast, behavioral segments directly suggest differentiated strategies: \"Explorers\" (high distance variability) may respond to destination discovery campaigns, \"Regular Families\" (high companion ratio) to family package offers, \"Engaged Actives\" (high redemption) to loyalty program communications.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>4. Value-Based Features Reserved for Prioritization</strong></p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        RFM-style value features (Frequency, Monetary) are intentionally excluded from clustering. Including value in cluster creation would conflate \"what customers do\" with \"how valuable they are,\" making it impossible to identify high-potential customers in low-value segments. Instead, value indicators will create a separate prioritization layer, enabling strategic resource allocation: invest heavily in high-value customers across all behavioral segments, while identifying growth opportunities in high-potential, currently-low-value customers.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Strategic Framework: Three-Layer Segmentation</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The final segmentation strategy adopts a three-layer approach that leverages each data perspective for its appropriate purpose:\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>Layer 1: Behavioral Segmentation (5 Clusters)</strong></p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        K-Means clustering on behavioral features (distance_variability, companion_flight_ratio, flight_regularity, redemption_frequency) creates 5 actionable customer segments. These segments answer \"How do customers engage with our service?\" and directly inform differentiated marketing strategies:\n",
    "    </p>\n",
    "    <ul style=\"margin: 5px 0; margin-right: 40px; color: #000;\">\n",
    "        <li><strong>Explorers:</strong> High distance variability, destination-seekers</li>\n",
    "        <li><strong>Sporadic Flyers:</strong> Irregular patterns, re-engagement targets</li>\n",
    "        <li><strong>Regular Families:</strong> High companion ratio, family travelers</li>\n",
    "        <li><strong>Engaged Actives:</strong> High redemption, loyalty program enthusiasts</li>\n",
    "        <li><strong>Passive Solos:</strong> Low engagement across features, activation candidates</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>Layer 2: Value-Based Prioritization (4 Tiers per Cluster)</strong></p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Each behavioral segment will be mapped onto a Frequency x Monetary matrix, creating 4 priority tiers:\n",
    "    </p>\n",
    "    <ul style=\"margin: 5px 0; margin-right: 40px; color: #000;\">\n",
    "        <li><strong>Champions (High F, High M):</strong> Protect and reward</li>\n",
    "        <li><strong>Potential Loyalists (High F, Low M):</strong> Upsell opportunities</li>\n",
    "        <li><strong>Big Spenders (Low F, High M):</strong> Increase engagement frequency</li>\n",
    "        <li><strong>At Risk (Low F, Low M):</strong> Re-activation or deprioritize</li>\n",
    "    </ul>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        This yields 20 strategic combinations (5 behavioral clusters x 4 value tiers), enabling precise resource allocation and tailored strategies.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\"><strong>Layer 3: Demographic Profiling</strong></p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Demographic attributes (income, education, location, gender, marital status) will be used exclusively for descriptive profiling of the 20 strategic combinations. This provides \"persona\" characteristics (\"Who are our Engaged Active Champions?\") for communication personalization and channel selection, without influencing segment membership or prioritization.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #A52A2A; font-weight: bold;\">Next Steps</h4>\n",
    "    <ul style=\"margin: 5px 0; margin-right: 40px; color: #000;\">\n",
    "        <li><strong>Section 9.3 - Final Profiling:</strong> Comprehensive profiling of the 5 behavioral segments including cluster heatmaps, parallel coordinates, t-SNE/UMAP visualizations, feature importance analysis, and demographic distributions.</li>\n",
    "        <li><strong>Section 9.4 - Value Mapping:</strong> Implementation of the Frequency x Monetary prioritization matrix, creating 4 tiers per behavioral cluster and visualizing the 20 strategic combinations.</li>\n",
    "        <li><strong>Section 9.5 - Strategic Recommendations:</strong> Actionable marketing strategies for each behavioral segment and value tier combination, including recommended channels, offer types, and engagement timing.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f20b6",
   "metadata": {},
   "source": [
    "## **9.3 Final Profiling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e5cfd",
   "metadata": {},
   "source": [
    "### Step 1: Cluster Sizes & Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7ccd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setup final clustering data\n",
    "final_labels = behavioral_clustering_results_df.loc['K-Means', 'labels']\n",
    "df_final = df_behavioral_clean.copy()\n",
    "df_final['Cluster'] = final_labels\n",
    "k_final = 5\n",
    "\n",
    "# Cluster sizes (reuse existing function)\n",
    "plot_cluster_sizes(final_labels, k_final, CUSTOM_HEX, title='Final Segmentation: Cluster Size Distribution (K-Means k=5)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756b47e",
   "metadata": {},
   "source": [
    "### Step 2: Final Profiles (Z-Scores + Original Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28bab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Behavioral profiles - Z-Scores vs Original Values\n",
    "cluster_profiles = df_final.groupby('Cluster')[behavioral_feats].mean()\n",
    "\n",
    "# Convert to original scale\n",
    "original_stats = df_behavioral_a[behavioral_feats].agg(['mean', 'std'])\n",
    "cluster_profiles_original = cluster_profiles.copy()\n",
    "for feat in behavioral_feats:\n",
    "    cluster_profiles_original[feat] = (cluster_profiles[feat] * original_stats.loc['std', feat]) + original_stats.loc['mean', feat]\n",
    "\n",
    "# Normalize per row (feature) for better visualization\n",
    "cluster_profiles_normalized = cluster_profiles_original.T.copy()\n",
    "for feat in cluster_profiles_normalized.index:\n",
    "    row_min = cluster_profiles_normalized.loc[feat].min()\n",
    "    row_max = cluster_profiles_normalized.loc[feat].max()\n",
    "    cluster_profiles_normalized.loc[feat] = (cluster_profiles_normalized.loc[feat] - row_min) / (row_max - row_min)\n",
    "\n",
    "# Color palette\n",
    "colors = [\"#00411E\", \"#00823C\", \"#82BA72\", \"#D5E6D0\", \"#FFFFFF\"]\n",
    "green_white_cmap = LinearSegmentedColormap.from_list('gw', colors[::-1])\n",
    "\n",
    "# Side-by-side comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Left: Z-Scores (global scale)\n",
    "sns.heatmap(cluster_profiles.T, annot=True, fmt='.2f', cmap=GROUP80_palette_continuous, \n",
    "            center=0, ax=axes[0], cbar_kws={'label': 'Z-Score'}, linewidths=0.5)\n",
    "axes[0].set_title('Final Profiles: Z-Scores', fontweight='bold')\n",
    "axes[0].set_xlabel('Cluster', fontweight='bold')\n",
    "axes[0].set_ylabel('Feature', fontweight='bold')\n",
    "\n",
    "# Right: Original Values (normalized per row, no colorbar)\n",
    "sns.heatmap(cluster_profiles_normalized, annot=cluster_profiles_original.T.values, fmt='.2f', \n",
    "            cmap=green_white_cmap, ax=axes[1], cbar=False, linewidths=0.5)\n",
    "axes[1].set_title('Final Profiles: Original Scale\\n(Colors normalized per feature: dark=high, light=low)', fontweight='bold')\n",
    "axes[1].set_xlabel('Cluster', fontweight='bold')\n",
    "axes[1].set_ylabel('Feature', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af281b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Key Findings: Five Distinct Final Segments</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The K-Means clustering (k=5) reveals <strong>five behaviorally distinct customer segments</strong> based on travel patterns and loyalty program engagement. Each cluster exhibits a unique combination of characteristics that will inform targeted marketing strategies in Section 9.5.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Cluster Profiles:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 0 - Family Travelers:</strong> 36% of flights with companions (highest), moderate route diversity, below-average monthly consistency. Families or friends traveling together for leisure.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 1 - Business Commuters:</strong> 55% flight regularity score (highest), consistent routes with low distance variation, predominantly solo (23% companion flights). Predictable business travelers with fixed schedules.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 2 - Disengaged Solo:</strong> Only 18% companion flights, 46% regularity (lowest), redeems in just 4% of available months (lowest). Infrequent, disengaged travelers. Re-engagement target.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 3 - Explorers:</strong> Distance CV of 1.09 (highest) meaning flight distances vary as much as their average. Adventure seekers visiting diverse destinations near and far.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 4 - Engaged Loyalists:</strong> Redeems points in 16% of available months (highest, 4x more than Cluster 2), above-average regularity (53%) and companions (28%). Most engaged with the loyalty program.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Feature Interpretation:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>companion_flight_ratio:</strong> Percentage of flights taken with at least one companion (0.36 = 36% of flights).</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>flight_regularity:</strong> Consistency of monthly flight activity (0.55 = moderate-high consistency, 1.0 = perfectly regular).</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>distance_variability:</strong> Coefficient of Variation of flight distances (1.09 = high diversity in destinations).</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>redemption_frequency:</strong> Proportion of months with point redemptions (0.16 = redeems in 16% of months).</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Strategic Implications:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        These behavioral segments provide actionable foundations for differentiated marketing approaches. The distinguishing features directly translate to marketing levers: group packages, subscription models, re-engagement campaigns, destination promotions, and loyalty rewards. Detailed strategies will be developed in <strong>Section 9.5</strong>.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038070ee",
   "metadata": {},
   "source": [
    "### Step 3: Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fb0fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Importance - Variance across cluster centroids\n",
    "feature_variance = cluster_profiles.var().sort_values(ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(feature_variance.index, feature_variance.values, color=CUSTOM_HEX[1])\n",
    "\n",
    "# Add values inside bars\n",
    "for bar, val in zip(bars, feature_variance.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height()/2, \n",
    "            f'{val:.3f}', ha='center', va='center', fontweight='bold', fontsize=11, color='white')\n",
    "\n",
    "ax.set_xlabel('Feature', fontweight='bold')\n",
    "ax.set_ylabel('Variance', fontweight='bold')\n",
    "ax.set_title('Feature Importance: Variance Across Cluster Centroids', fontweight='bold', fontsize=13, pad=15)\n",
    "ax.set_xticklabels(feature_variance.index, rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f777d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Feature Importance Summary</h3>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Redemption frequency</strong> (0.64) is the strongest cluster differentiator, clearly separating Engaged Loyalists from other segments. <strong>Companion flight ratio</strong> (0.60) ranks second, distinguishing Family Travelers from solo segments. <strong>Flight regularity</strong> (0.53) and <strong>distance variability</strong> (0.48) contribute moderately, identifying Business Commuters and Explorers respectively. All four features show substantial variance (0.48-0.64), confirming each contributes meaningfully to the segmentation.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a482ee66",
   "metadata": {},
   "source": [
    "### Step 4: PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a1e804",
   "metadata": {},
   "outputs": [],
   "source": [
    "behavioral_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e635c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: PCA - 2D Visualization\n",
    "\n",
    "# Fit PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(df_final[behavioral_feats])\n",
    "\n",
    "# Add to dataframe\n",
    "df_final['pca_1'] = X_pca[:, 0]\n",
    "df_final['pca_2'] = X_pca[:, 1]\n",
    "\n",
    "# Variance explained\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "print(f\"PCA Variance Explained:\")\n",
    "print(f\"PC1: {var_explained[0]:.1%}\")\n",
    "print(f\"PC2: {var_explained[1]:.1%}\")\n",
    "print(f\"Total: {var_explained.sum():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a78909",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cb4462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Scatter Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Colorful palette\n",
    "colors = cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# Plot each cluster separately\n",
    "for label in sorted(df_final['Cluster'].unique()):\n",
    "    cluster_data = df_final[df_final['Cluster'] == label]\n",
    "    ax.scatter(\n",
    "        cluster_data['pca_1'],\n",
    "        cluster_data['pca_2'],\n",
    "        c=[colors[label]],\n",
    "        alpha=0.6,\n",
    "        s=30,\n",
    "        edgecolors='none',\n",
    "        label=f'Cluster {label}'\n",
    "    )\n",
    "\n",
    "# Add cluster centroids\n",
    "for label in sorted(df_final['Cluster'].unique()):\n",
    "    cluster_data = df_final[df_final['Cluster'] == label]\n",
    "    centroid_x = cluster_data['pca_1'].mean()\n",
    "    centroid_y = cluster_data['pca_2'].mean()\n",
    "    ax.scatter(centroid_x, centroid_y, c='black', marker='X', s=200, \n",
    "               edgecolors=colors[label], linewidths=2, zorder=10)\n",
    "    ax.annotate(f'C{label}', (centroid_x + 0.1, centroid_y + 0.1), \n",
    "                fontsize=12, fontweight='bold', ha='center', va='center', zorder=20)\n",
    "\n",
    "ax.legend(loc='best', framealpha=1, title='Cluster', fontsize=10, markerscale=1.5)\n",
    "ax.set_xlabel(f'PC1 ({var_explained[0]:.1%} variance)', fontweight='bold')\n",
    "ax.set_ylabel(f'PC2 ({var_explained[1]:.1%} variance)', fontweight='bold')\n",
    "ax.set_title('PCA: 2D Projection of 5 Behavioral Segments', fontweight='bold', fontsize=13, pad=15)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a71f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Loadings Heatmap\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,\n",
    "    columns=['PC1', 'PC2'],\n",
    "    index=behavioral_feats\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "sns.heatmap(loadings, annot=True, cmap='PiYG', center=0, fmt='.3f', ax=ax, linewidths=0.5)\n",
    "ax.set_title('PCA Component Loadings', fontweight='bold', fontsize=13, pad=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1e1bc",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">PCA Visualization Summary</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Principal Component Analysis (PCA) reduces the 4 behavioral features to 2 dimensions while preserving maximum variance. PC1 and PC2 together explain <strong>58.1%</strong> of total variance (32.3% + 25.8%), providing a reasonable 2D representation of the cluster structure.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Loading Interpretation:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>PC1 (Engagement Axis):</strong> Driven by redemption_frequency (0.74) and companion_flight_ratio (0.56). High PC1 values indicate engaged customers who redeem points frequently and travel with companions. Low PC1 values indicate disengaged solo travelers.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>PC2 (Travel Style Axis):</strong> Driven by distance_variability (0.76) and flight_regularity (0.46), with negative loading on companion_flight_ratio (-0.45). High PC2 values indicate solo explorers visiting diverse destinations. Low PC2 values indicate family travelers with consistent routes.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Four distinct corner clusters:</strong> Clusters 0, 2, 3, and 4 occupy separate regions of the PCA space, confirming behavioral differentiation.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 4 (Engaged Loyalists):</strong> Far right (high PC1) - highest redemption and companion activity.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 2 (Disengaged Solo):</strong> Far left (low PC1) - lowest engagement across all metrics.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 3 (Explorers):</strong> Top (high PC2) - highest distance variability, diverse destinations.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 0 (Family Travelers):</strong> Bottom (low PC2) - highest companion ratio, consistent travel patterns.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 1 (Business Commuters):</strong> Centered in the middle, overlapping with all other clusters. This segment has <strong>average values across most features</strong> except high flight regularity. Their central position reflects their balanced, predictable behavior without extreme characteristics in any direction.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Interpretation:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The PCA projection validates the K-Means segmentation: four segments show clear spatial separation representing distinct behavioral profiles, while Business Commuters form a general baseline cluster. The overlap of Cluster 1 with others suggests these customers could potentially migrate to other segments with targeted marketing interventions.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3cde12",
   "metadata": {},
   "source": [
    "### Step 5: t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: t-SNE with different perplexity values\n",
    "\n",
    "perplexity_values = [5, 20, 40]\n",
    "tsne_results = {}\n",
    "\n",
    "for perp in perplexity_values:\n",
    "    print(f\"Computing t-SNE with perplexity={perp}...\")\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perp,\n",
    "        random_state=42,\n",
    "        max_iter=1000,\n",
    "        learning_rate='auto',\n",
    "        init='pca'\n",
    "    )\n",
    "    tsne_results[perp] = tsne.fit_transform(df_final[behavioral_feats])\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa47be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE perplexity comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for idx, perp in enumerate(perplexity_values):\n",
    "    ax = axes[idx]\n",
    "    X_tsne = tsne_results[perp]\n",
    "    \n",
    "    for label in sorted(df_final['Cluster'].unique()):\n",
    "        mask = df_final['Cluster'] == label\n",
    "        ax.scatter(\n",
    "            X_tsne[mask, 0],\n",
    "            X_tsne[mask, 1],\n",
    "            c=[colors[label]],\n",
    "            alpha=0.6,\n",
    "            s=20,\n",
    "            edgecolors='none',\n",
    "            label=f'Cluster {label}' if idx == 0 else ''\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f't-SNE (perplexity={perp})', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('t-SNE 1', fontweight='bold')\n",
    "    ax.set_ylabel('t-SNE 2', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legend to first subplot\n",
    "axes[0].legend(loc='best', framealpha=1, title='Cluster', fontsize=9, markerscale=1.5)\n",
    "\n",
    "plt.suptitle('t-SNE: Effect of Perplexity on Cluster Visualization', fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca11d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE Stability Test - multiple runs with same perplexity\n",
    "perp_test = 30\n",
    "random_seeds = [42, 123, 456]\n",
    "stability_results = {}\n",
    "\n",
    "print(f\"Testing t-SNE stability with perplexity={perp_test}\\n\")\n",
    "for seed in random_seeds:\n",
    "    print(f\"Run with random_state={seed}...\")\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=perp_test,\n",
    "        random_state=seed,\n",
    "        max_iter=1000,\n",
    "        learning_rate='auto',\n",
    "        init='pca'\n",
    "    )\n",
    "    stability_results[seed] = tsne.fit_transform(df_final[behavioral_feats])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387674d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize stability comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for idx, seed in enumerate(random_seeds):\n",
    "    ax = axes[idx]\n",
    "    X_tsne = stability_results[seed]\n",
    "    \n",
    "    for label in sorted(df_final['Cluster'].unique()):\n",
    "        mask = df_final['Cluster'] == label\n",
    "        ax.scatter(\n",
    "            X_tsne[mask, 0],\n",
    "            X_tsne[mask, 1],\n",
    "            c=[colors[label]],\n",
    "            alpha=0.6,\n",
    "            s=20,\n",
    "            edgecolors='none',\n",
    "            label=f'Cluster {label}' if idx == 0 else ''\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f't-SNE Run {idx+1} (seed={seed})', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('t-SNE 1', fontweight='bold')\n",
    "    ax.set_ylabel('t-SNE 2', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].legend(loc='best', framealpha=1, title='Cluster', fontsize=9, markerscale=1.5)\n",
    "\n",
    "plt.suptitle(f't-SNE Stability Test (perplexity={perp_test})', fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d4c03",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">t-SNE Visualization Summary</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology - How t-SNE Differs from PCA:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        t-SNE (t-Distributed Stochastic Neighbor Embedding) is a <strong>non-linear</strong> dimensionality reduction technique that preserves <strong>local neighborhood structure</strong>. Unlike PCA which finds linear combinations that maximize variance, t-SNE focuses on keeping similar points close together in the low-dimensional space. The axes in t-SNE have <strong>no interpretable meaning</strong> - only the relative distances between points matter.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Perplexity Parameter (5, 20, 40):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Perplexity=5 (too local):</strong> Clusters are heavily fragmented and mixed. The algorithm focuses on too few neighbors, creating artificial micro-clusters and losing all global structure. Worst separation.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Perplexity=20 (balanced):</strong> Clusters begin to separate into distinct regions. Sub-cluster structure becomes visible within segments, revealing potential heterogeneity (e.g., Cluster 1 and 3 show internal groupings).</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Perplexity=40 (more global):</strong> Best overall cluster separation. Clusters form cohesive groups with clearer boundaries. Cluster 4 (purple) and Cluster 0 (blue) are well-separated; Cluster 1 (orange) remains distributed across the space.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Stability Test (perplexity=30, different seeds):</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The three runs with different random seeds reveal a <strong>critical limitation</strong>: cluster positions change dramatically between runs. In Run 1, Cluster 4 (purple) appears on the right; in Run 2, it moves to the bottom; in Run 3, it shifts again. However, <strong>local groupings within clusters remain consistent</strong> - the same customers stay together, only their global position changes. This confirms t-SNE preserves local structure but not global distances.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Limitations:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Non-reproducible:</strong> Different random seeds produce different visualizations.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>No interpretable axes:</strong> Cannot explain what \"t-SNE 1\" or \"t-SNE 2\" represent.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Global distances unreliable:</strong> Distance between clusters does not reflect true similarity.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Computationally expensive:</strong> Slower than PCA, especially for large datasets.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Cannot project new data:</strong> Requires full recomputation for new customers.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Insight:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        t-SNE confirms that clusters have meaningful <strong>local cohesion</strong> - customers within each segment are genuinely similar to each other. The consistent finding across all perplexity values that <strong>Cluster 1 (Business Commuters) remains distributed</strong> throughout the space reinforces the PCA finding: this segment represents a general baseline without extreme behavioral characteristics.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9679282a",
   "metadata": {},
   "source": [
    "### Step 6: UMAP Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c17aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: UMAP with different n_neighbors values\n",
    "\n",
    "n_neighbors_values = [5, 15, 50]\n",
    "umap_results = {}\n",
    "\n",
    "print(\"Running UMAP with different n_neighbors values...\\n\")\n",
    "\n",
    "for n_neigh in n_neighbors_values:\n",
    "    print(f\"Computing UMAP with n_neighbors={n_neigh}...\")\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=n_neigh,\n",
    "        min_dist=0.1,\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    umap_results[n_neigh] = reducer.fit_transform(df_final[behavioral_feats])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize UMAP n_neighbors comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for idx, n_neigh in enumerate(n_neighbors_values):\n",
    "    ax = axes[idx]\n",
    "    X_umap = umap_results[n_neigh]\n",
    "    \n",
    "    for label in sorted(df_final['Cluster'].unique()):\n",
    "        mask = df_final['Cluster'] == label\n",
    "        ax.scatter(\n",
    "            X_umap[mask, 0],\n",
    "            X_umap[mask, 1],\n",
    "            c=[colors[label]],\n",
    "            alpha=0.6,\n",
    "            s=20,\n",
    "            edgecolors='none',\n",
    "            label=f'Cluster {label}' if idx == 0 else ''\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f'UMAP (n_neighbors={n_neigh})', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('UMAP 1', fontweight='bold')\n",
    "    ax.set_ylabel('UMAP 2', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].legend(loc='best', framealpha=1, title='Cluster', fontsize=9, markerscale=1.5)\n",
    "\n",
    "plt.suptitle('UMAP: Effect of n_neighbors on Cluster Visualization', fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8727c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP with different min_dist values\n",
    "min_dist_values = [0.0, 0.1, 0.5]\n",
    "umap_mindist_results = {}\n",
    "\n",
    "print(\"Running UMAP with different min_dist values...\\n\")\n",
    "\n",
    "for min_d in min_dist_values:\n",
    "    print(f\"Computing UMAP with min_dist={min_d}...\")\n",
    "    reducer = umap.UMAP(\n",
    "        n_components=2,\n",
    "        n_neighbors=15,\n",
    "        min_dist=min_d,\n",
    "        random_state=42,\n",
    "        n_jobs=1\n",
    "    )\n",
    "    umap_mindist_results[min_d] = reducer.fit_transform(df_final[behavioral_feats])\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize min_dist comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "for idx, min_d in enumerate(min_dist_values):\n",
    "    ax = axes[idx]\n",
    "    X_umap = umap_mindist_results[min_d]\n",
    "    \n",
    "    for label in sorted(df_final['Cluster'].unique()):\n",
    "        mask = df_final['Cluster'] == label\n",
    "        ax.scatter(\n",
    "            X_umap[mask, 0],\n",
    "            X_umap[mask, 1],\n",
    "            c=[colors[label]],\n",
    "            alpha=0.6,\n",
    "            s=20,\n",
    "            edgecolors='none',\n",
    "            label=f'Cluster {label}' if idx == 0 else ''\n",
    "        )\n",
    "    \n",
    "    ax.set_title(f'UMAP (min_dist={min_d})', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('UMAP 1', fontweight='bold')\n",
    "    ax.set_ylabel('UMAP 2', fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].legend(loc='best', framealpha=1, title='Cluster', fontsize=9, markerscale=1.5)\n",
    "\n",
    "plt.suptitle('UMAP: Effect of min_dist on Cluster Visualization', fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd19c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">UMAP Visualization Summary</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Methodology:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        UMAP (Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that balances local and global structure. Unlike t-SNE, UMAP is deterministic (reproducible) and faster.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">n_neighbors Parameter (5, 15, 50):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>n_neighbors=5:</strong> Clusters form horizontal band-like structures stretched across the space.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>n_neighbors=15:</strong> Clusters form distinct, separated blob-like groups with clearer boundaries.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>n_neighbors=50:</strong> Clusters become round, cohesive blobs that connect to each other in a chain-like pattern.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">min_dist Parameter (0.0, 0.1, 0.5):</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>min_dist=0.0:</strong> Tightest clustering - points pack closely together within each cluster.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>min_dist=0.1:</strong> Similar to 0.0, clusters remain compact with good separation.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>min_dist=0.5:</strong> Clusters spread out significantly and begin to overlap, boundaries less clear.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Key Findings:</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 0 (Family Travelers, blue):</strong> Consistently positioned in the center, surrounded by other clusters. This central position indicates behavioral overlap with multiple segments - families share characteristics with other traveler types.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><strong>Cluster 4 (Engaged Loyalists, purple):</strong> The only cluster that forms its own isolated group across all parameter settings. This confirms Engaged Loyalists are behaviorally the most distinct segment with unique characteristics (high redemption frequency) that separate them from all other customers.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5728d965",
   "metadata": {},
   "source": [
    "### Step 7: Multi-Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c403afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison: PCA, t-SNE, UMAP\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "colors = cm.tab10(np.linspace(0, 1, 10))\n",
    "\n",
    "# PCA\n",
    "for label in sorted(df_final['Cluster'].unique()):\n",
    "    mask = df_final['Cluster'] == label\n",
    "    axes[0].scatter(\n",
    "        df_final.loc[mask, 'pca_1'],\n",
    "        df_final.loc[mask, 'pca_2'],\n",
    "        c=[colors[label]],\n",
    "        alpha=0.6,\n",
    "        s=20,\n",
    "        edgecolors='none',\n",
    "        label=f'Cluster {label}'\n",
    "    )\n",
    "axes[0].set_title('PCA: Global Structure', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel(f'PC1 ({var_explained[0]:.1%})', fontweight='bold')\n",
    "axes[0].set_ylabel(f'PC2 ({var_explained[1]:.1%})', fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE (perplexity=30)\n",
    "X_tsne_best = tsne_results[20]  # oder 30 je nachdem welcher besser war\n",
    "for label in sorted(df_final['Cluster'].unique()):\n",
    "    mask = df_final['Cluster'] == label\n",
    "    axes[1].scatter(\n",
    "        X_tsne_best[mask, 0],\n",
    "        X_tsne_best[mask, 1],\n",
    "        c=[colors[label]],\n",
    "        alpha=0.6,\n",
    "        s=20,\n",
    "        edgecolors='none'\n",
    "    )\n",
    "axes[1].set_title('t-SNE: Local Structure (perplexity=20)', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('t-SNE 1', fontweight='bold')\n",
    "axes[1].set_ylabel('t-SNE 2', fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# UMAP (n_neighbors=15, min_dist=0.1)\n",
    "X_umap_best = umap_results[15]\n",
    "for label in sorted(df_final['Cluster'].unique()):\n",
    "    mask = df_final['Cluster'] == label\n",
    "    axes[2].scatter(\n",
    "        X_umap_best[mask, 0],\n",
    "        X_umap_best[mask, 1],\n",
    "        c=[colors[label]],\n",
    "        alpha=0.6,\n",
    "        s=20,\n",
    "        edgecolors='none'\n",
    "    )\n",
    "axes[2].set_title('UMAP: Balanced View (n_neighbors=15)', fontweight='bold', fontsize=12)\n",
    "axes[2].set_xlabel('UMAP 1', fontweight='bold')\n",
    "axes[2].set_ylabel('UMAP 2', fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "# Legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='center right', bbox_to_anchor=(0.98, 0.5),\n",
    "           framealpha=1, title='Cluster', fontsize=10, markerscale=2)\n",
    "\n",
    "plt.suptitle('Method Comparison: Same 5 Segments, Different Views', fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.88)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973c48c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e8f3e5ff; padding: 15px; margin-right: 30px; border-left: 5px solid; border-image: linear-gradient(to bottom, #00411E, #00622D, #00823C, #45AF28, #82BA72) 1; border-radius: 5px; max-width: 95%;\">\n",
    "    <h3 style=\"margin-top: 0; margin-bottom: 10px; color: #00411E; font-weight: bold;\">Multi-Method Comparison Summary</h3>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Where do the methods agree? And disagree?</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Agreement:</strong> All three methods show Cluster 4 (Engaged Loyalists, purple) as the most concentrated cluster, and Cluster 1 (Business Commuters, orange) consistently in the center overlapping with other segments.<br><br>\n",
    "        <strong>Disagreement:</strong> PCA shows heavy overlap in the center. t-SNE fragments everything into scattered sub-groups. UMAP forms three distinct blobs but with mixed cluster membership.\n",
    "    </p>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Where can you find overlap? Sub-clusters? Isolated clusters?</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Overlap:</strong> In UMAP, Clusters 0, 1, 2, and 3 appear in all three blobs. In PCA, all clusters overlap in the center.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Sub-clusters:</strong> t-SNE fragments all clusters into multiple scattered groups, revealing internal heterogeneity.</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Concentrated cluster:</strong> Cluster 4 (purple) is the only cluster that appears in just one UMAP blob.</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">What areas represent specific behaviors? (PCA)</h4>\n",
    "    <ul style=\"margin: 10px 40px 10px 20px; padding-left: 20px; color: #000;\">\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>High PC1 (right):</strong> High redemption + high companions --> Cluster 4 (Engaged Loyalists)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>High PC2 (top):</strong> High distance variability --> Cluster 3 (Explorers)</li>\n",
    "        <li style=\"margin-bottom: 5px;\"><strong>Low PC2 (bottom):</strong> High companion ratio --> Cluster 0 (Family Travelers)</li>\n",
    "    </ul>\n",
    "    <h4 style=\"margin-top: 15px; margin-bottom: 8px; color: #00622D; font-weight: bold;\">Cross-Selling and Upselling Opportunities:</h4>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        The visualizations provide insights into which customers are most likely to respond to cross-selling and upselling campaigns. The key principle: customers who are positioned close to another cluster or in overlapping regions are behaviorally similar to that cluster and therefore more likely to adopt those behaviors with targeted marketing.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        Clusters that appear in the center (like Cluster 1 in PCA) or spread across multiple blobs (like Clusters 0, 1, 2, 3 in UMAP) indicate customers who share characteristics with multiple segments. These are prime targets for behavioral migration campaigns. Conversely, isolated clusters (like Cluster 4) represent customers with unique, distinct behaviors who are less likely to shift but more valuable to retain.\n",
    "    </p>\n",
    "    <p style=\"margin: 10px 0; margin-right: 40px; color: #000;\">\n",
    "        <strong>Detailed segment-specific strategies will be developed in Section 9.5.</strong>\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb00a4",
   "metadata": {},
   "source": [
    "### Step 9: Demographic Profiling (8 kommt noch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dda2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Add behavioral and demographic cluster labels to df_Customer\n",
    "# Die Indices von df_behavioral_clean und df_demographic_a_scaled matchen df_Customer's Index\n",
    "\n",
    "# Behavioral cluster labels\n",
    "df_profiling = df_Customer.copy()\n",
    "df_profiling['Behavioral_Cluster'] = df_behavioral_clean['km_cluster'].reindex(df_profiling.index)\n",
    "\n",
    "# Demographic cluster labels\n",
    "df_profiling['Demographic_Cluster'] = df_demographic_a_scaled['km_cluster'].reindex(df_profiling.index)\n",
    "\n",
    "# Drop rows without behavioral cluster (outliers removed by DBSCAN)\n",
    "df_profiling = df_profiling.dropna(subset=['Behavioral_Cluster'])\n",
    "df_profiling['Behavioral_Cluster'] = df_profiling['Behavioral_Cluster'].astype(int)\n",
    "df_profiling['Demographic_Cluster'] = df_profiling['Demographic_Cluster'].astype(int)\n",
    "\n",
    "print(f\"Profiling DataFrame shape: {df_profiling.shape}\")\n",
    "print(f\"\\nBehavioral Clusters:\")\n",
    "print(df_profiling['Behavioral_Cluster'].value_counts().sort_index())\n",
    "print(f\"\\nDemographic Clusters:\")\n",
    "print(df_profiling['Demographic_Cluster'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5875ba",
   "metadata": {},
   "source": [
    "Add scaled behavioral columns to df profiling and do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dadfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scaled behavioral features for profiling\n",
    "df_profiling[\"distance_variability_scaled\"] = df_behavioral_clean[\"distance_variability\"].reindex(df_profiling.index)\n",
    "df_profiling[\"companion_flight_ratio_scaled\"] = df_behavioral_clean[\"companion_flight_ratio\"].reindex(df_profiling.index)\n",
    "df_profiling[\"flight_regularity_scaled\"] = df_behavioral_clean[\"flight_regularity\"].reindex(df_profiling.index)\n",
    "df_profiling[\"redemption_frequency_scaled\"] = df_behavioral_clean[\"redemption_frequency\"].reindex(df_profiling.index)\n",
    "\n",
    "behavioral_feats_scaled = [\n",
    "    \"distance_variability_scaled\",\n",
    "    \"companion_flight_ratio_scaled\",\n",
    "    \"flight_regularity_scaled\",\n",
    "    \"redemption_frequency_scaled\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cce785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do pca for 3D visualization\n",
    "pca = PCA(n_components=3, random_state=42)\n",
    "X_pca_final = pca.fit_transform(df_profiling[behavioral_feats_scaled])\n",
    "\n",
    "# Add to dataframe\n",
    "df_profiling['pca_1'] = X_pca_final[:, 0]\n",
    "df_profiling['pca_2'] = X_pca_final[:, 1]\n",
    "df_profiling['pca_3'] = X_pca_final[:, 2]\n",
    "\n",
    "# Save as CSV\n",
    "df_profiling.to_csv('data/clustering_data/customer_segmentation_profiles.csv', index=True)\n",
    "\n",
    "# Drop the scaled columns to clean up\n",
    "df_profiling.drop(columns=behavioral_feats_scaled, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86964a28",
   "metadata": {},
   "source": [
    "## 9.4 Value Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d732da",
   "metadata": {},
   "source": [
    "## 9.5 Strategic Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open to dos but for later so when the final clustering is ready then we are doing more in depth cluster visualization and profiling, and also decision tree classifier for feature importance\n",
    "\n",
    "# for feature importance\n",
    "'''\n",
    "- Train a Decision Tree Classifier model over the cluster labels\n",
    "    - Estimate the respective feature importance (Gini importance)\n",
    "    - Apply the trained model to classify multivariate outliers  \n",
    "'''\n",
    "\n",
    "\n",
    "# for cluster profiling visualizations\n",
    "# Plot single variables to visualizes features across clusters\n",
    "# example from tutorium \n",
    "'''\n",
    "pd.crosstab(df['merged_labels'], df['education'], normalize='index').plot.bar(\n",
    "    stacked=True,\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.title(\"Education Level Distribution by Merged Segment\", fontsize=14)\n",
    "plt.xlabel(\"Merged Segment\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.legend(title=\"Education Level\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc5be07",
   "metadata": {},
   "source": [
    "Cluster 1 (Business Commuters) as Gateway: In PCA, Cluster 1 (orange) sits in the center of the plot, surrounded by and overlapping with all other clusters. In UMAP, orange points appear in all three blobs. This central positioning indicates Business Commuters share behavioral characteristics with every other segment - making them the natural \"gateway\" cluster. Customers likely transition through this segment when changing travel patterns, so cross-selling different behaviors (more companions, more redemptions, diverse destinations) has the highest success probability here.\n",
    "Cluster 4 (Engaged Loyalists) - Protect and Retain: Purple points concentrate in one area (PCA right side, UMAP bottom blob only). This isolation shows Engaged Loyalists have unique behavioral patterns distinct from all other customers. Focus should be on retention rather than cross-selling - these customers already exhibit the desired high-engagement behavior.\n",
    "Cluster 2 (Disengaged Solo) - Re-activation Target: Green points spread across all areas but consistently show low engagement. The overlap with other clusters in the visualizations suggests these customers could exhibit other behaviors but currently don't. Re-activation campaigns should target moving them toward Cluster 1 (Business Commuters) first as the easiest behavioral shift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e0061",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
